{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdswuho7MsCk"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C2/W4/ungraded_lab/C2_W4_Lab_1_multi_class_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/doantronghieu/DEEP-LEARNING/main/helper_DL.py\n",
        "!pip install colorama\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size':15})\n",
        "import seaborn           as sns\n",
        "sns.set()\n",
        "import helper_DL as helper"
      ],
      "metadata": {
        "id": "skOiQNg4MvAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UD-1_xY-h2u"
      },
      "source": [
        "# Ungraded Lab: Multi-class Classifier\n",
        "\n",
        "In this lab, you will look at how to build a model to distinguish between more than two classes. The code will be similar to the ones you've been using before with a few key changes in the model and in the training parameters. Let's dive in!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvwVR5lHA8q_"
      },
      "source": [
        "## Download and Prepare the Dataset\n",
        "\n",
        "You will be using the [Rock-Paper-Scissors dataset](http://www.laurencemoroney.com/rock-paper-scissors-dataset/), a gallery of hands images in Rock, Paper, and Scissors poses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh6SMXvIMsCo"
      },
      "outputs": [],
      "source": [
        "# Download the train set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week4/rps.zip\n",
        "# Download the test set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week4/rps-test-set.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnYP_HhYNVUK"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Extract the archive\n",
        "local_zip = './rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('tmp/rps-train')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = './rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('tmp/rps-test')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3vqjYrpB0hI"
      },
      "source": [
        "As usual, you will assign the directory names into variables and look at the filenames as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrxdR83ANgjS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_dir = 'tmp/rps-train/rps'\n",
        "\n",
        "rock_dir     = os.path.join(base_dir, 'rock')\n",
        "paper_dir    = os.path.join(base_dir, 'paper')\n",
        "scissors_dir = os.path.join(base_dir, 'scissors')\n",
        "\n",
        "rock_files     = os.listdir(rock_dir)\n",
        "paper_files    = os.listdir(paper_dir)\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "\n",
        "print(f'Total training rock images:     {len(os.listdir(rock_dir))}')\n",
        "print(f'Total training paper images:    {len(os.listdir(paper_dir))}')\n",
        "print(f'Total training scissors images: {len(os.listdir(scissors_dir))}\\n')\n",
        "\n",
        "print(rock_files[:10])\n",
        "print(paper_files[:10])\n",
        "print(scissors_files[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t_CNSs6B-8y"
      },
      "source": [
        "You can also inspect some of the images to see the variety in your model inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp9dLel9N9DS"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "\n",
        "next_rock     = [os.path.join(rock_dir, fname)     for fname in rock_files    [pic_index - 2:pic_index]]\n",
        "next_paper    = [os.path.join(paper_dir, fname)    for fname in paper_files   [pic_index - 2:pic_index]]\n",
        "next_scissors = [os.path.join(scissors_dir, fname) for fname in scissors_files[pic_index - 2:pic_index]]\n",
        "\n",
        "for (i, img_path) in enumerate(next_rock + next_paper + next_scissors):\n",
        "    img = mpimg.imread(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('Off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufa0YF5oCpYw"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "You will then build your CNN. You will use 4 convolution layers with 64-64-128-128 filters then append a `Dropout` layer to avoid overfitting and some Dense layers for the classification. The output layer would be a 3-neuron dense layer activated by [Softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax). You've seen this in Course 1 when you were training with Fashion MNIST. It scales your output to a set of probabilities that add up to 1. The order of this 3-neuron output would be `paper`-`rock`-`scissors` (e.g. a `[0.8 0.2 0.0]` output means the model is prediciting 80% probability for paper and 20% probability for rock.\n",
        "\n",
        "You can examine the architecture with `model.summary()` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgvGg2nsCj-0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tfk\n",
        "from tensorflow import nn\n",
        "from tensorflow.keras import layers, optimizers, losses, models, Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    # The input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # The first convolution\n",
        "    layers.Conv2D(64, (3, 3), activation = nn.relu, input_shape = (150, 150, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # The second convolution\n",
        "    layers.Conv2D(64, (3, 3), activation = nn.relu),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # The third convolution\n",
        "    layers.Conv2D(128, (3, 3), activation = nn.relu),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # The fourth convolution\n",
        "    layers.Conv2D(128, (3, 3), activation = nn.relu),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Flatten the results to feed into a DNN\n",
        "    layers.Flatten(),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    # 512 neuron hidden layer\n",
        "    layers.Dense(512, activation = nn.relu),\n",
        "    layers.Dense(3, activation = nn.softmax)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LlOK5OyOPVvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P1iuHGiFrPV"
      },
      "source": [
        "You will then compile the model. The key change here is the `loss` function. Whereas before you were using `binary_crossentropy` for 2 classes, you will change it to [categorical_crossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-function) to extend it to more classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OskuZ2ThFqmg"
      },
      "outputs": [],
      "source": [
        "# Set the training parameters\n",
        "model.compile(loss = losses.categorical_crossentropy,\n",
        "              optimizer = optimizers.RMSprop(),\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ps7kIRaFRIC"
      },
      "source": [
        "## Prepare the ImageDataGenerator\n",
        "\n",
        "You will prepare the generators as before. You will set the training set up for data augmentation so it can mimick other poses that the model needs to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWTisYLQM1aM"
      },
      "outputs": [],
      "source": [
        "TRAINING_DIR   = 'tmp/rps-train/rps'\n",
        "VALIDATION_DIR = 'tmp/rps-test/rps-test-set'\n",
        "\n",
        "training_datagen = ImageDataGenerator(rescale = 1. / 255,\n",
        "                                      rotation_range = 40,\n",
        "                                      width_shift_range = 0.2,\n",
        "                                      height_shift_range = 0.2,\n",
        "                                      shear_range = 0.2,\n",
        "                                      zoom_range = 0.2,\n",
        "                                      horizontal_flip = True,\n",
        "                                      fill_mode = 'nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                       target_size = (150, 150),\n",
        "                                                       class_mode = 'categorical',\n",
        "                                                       batch_size = 126)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              target_size = (150, 150),\n",
        "                                                              class_mode = 'categorical',\n",
        "                                                              batch_size = 126)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orf1QQlGGyOe"
      },
      "source": [
        "## Train the model and evaluate the results\n",
        "\n",
        "You will train for 25 epochs and evaludate the results afterwards. Observe how both the training and validation accuracy are trending upwards. This is a good indication that the model is not overfitting to only your training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mHX5L7HFXQ7"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(train_generator, epochs = 25, steps_per_epoch = 20,\n",
        "                    validation_data = validation_generator, validation_steps = 3,\n",
        "                    verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeTRVCr6aosw"
      },
      "outputs": [],
      "source": [
        "helper.plot_history_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ps8Q1tpYMG"
      },
      "source": [
        "# Model Prediction\n",
        "\n",
        "You should be able to upload an image here and have it classified without crashing. This codeblock will only work in Google Colab, however. You can use your own images or use the ones available [here](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-validation.zip)\n",
        "\n",
        "**Important Note:** Due to some compatibility issues, the following code block will result in an error after you select the images(s) to upload if you are running this notebook as a `Colab` on the `Safari` browser. For all other browsers, continue with the next code block and ignore the next one after it.\n",
        "\n",
        "_For Safari users: please comment out or skip the code block below, uncomment the next code block and run it._"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_softmax(target_size, model, class_names):\n",
        "  \"\"\"\n",
        "  Args: \n",
        "    target_size (list)  (int)\n",
        "    class_names (tuple) (str)\n",
        "  \"\"\"\n",
        "  from google.colab import files\n",
        "  from keras.preprocessing import image\n",
        "  \n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "\n",
        "      # Predicting images\n",
        "      path = '/content/' + fn\n",
        "\n",
        "      img = image.load_img(path, target_size=(target_size[0], target_size[1]))\n",
        "      img = image.img_to_array(img)\n",
        "      img = img / 255\n",
        "      img = np.expand_dims(img, axis=0)\n",
        "\n",
        "      images = np.vstack([img])\n",
        "      classes = model.predict(images, batch_size=10)\n",
        "      \n",
        "      print(fn)\n",
        "      print(classes)"
      ],
      "metadata": {
        "id": "1VjMwb4AS_lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_softmax(target_size=(150, 150), model=model, class_names=['rock', 'paper', 'scissors'])"
      ],
      "metadata": {
        "id": "yj-vLfKETDNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHRufhQYJJLU"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "That concludes this short exercise on the multi-class classifiers. You saw that with just a few changes, you were able to convert your binary classifiers to predict more classes. You used the same techniques for data and model preparation and were able to get relatively good results in just 25 epochs. For practice, you can search for other datasets (e.g. [here](https://archive.ics.uci.edu/ml/datasets.php) with more classes and revise the model to accomodate it. Try to experiment with different layers and data augmentation techniques to improve your metrics."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C2_W4_Lab_1_multi_class_classifier.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}