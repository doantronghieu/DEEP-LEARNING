{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doantronghieu/DEEP-LEARNING/blob/main/CSR_TF_Dev_Pro_Cert/C1/C1_W4_Lab_2_image_generator_with_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv5-EW-5jxGr"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C1/W4/ungraded_labs/C1_W4_Lab_2_image_generator_with_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB2cQUShkXNm"
      },
      "source": [
        "# Ungraded Lab: ImageDataGenerator with a Validation Set\n",
        "\n",
        "In this lab, you will continue using the `ImageDataGenerator` class to prepare the `Horses or Humans` dataset. This time, you will add a validation set so you can also measure how well the model performs on data it hasn't seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsO-u_3fySMd"
      },
      "source": [
        "**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running it on your local machine might result in some of the code blocks throwing errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5FfBGV5yUjb"
      },
      "source": [
        "Run the code blocks below to download the datasets `horse-or-human.zip` and `validation-horse-or-human.zip` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZT2UsyIVe_"
      },
      "outputs": [],
      "source": [
        "# Download the training set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n",
        "# Download the validation set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brUxyTpYZHy"
      },
      "source": [
        "Then unzip both archives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip training set\n",
        "local_zip = 'horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "\n",
        "\n",
        "# Unzip validation set\n",
        "local_zip = './validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./validation-horse-or-human')\n",
        "\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "Similar to the previous lab, you will define the directories containing your images. This time, you will include those with validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR_M9nWN-K8B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory with the training horse pictures\n",
        "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "\n",
        "# Directory with the training human pictures\n",
        "train_human_dir = os.path.join('./horse-or-human/humans')\n",
        "\n",
        "# Directory with the validation horse pictures\n",
        "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with the validation human pictures\n",
        "validation_human_dir = os.path.join('./validation-horse-or-human/humans')\n",
        "\n",
        "print(train_horse_dir)\n",
        "print(train_human_dir)\n",
        "print(validation_horse_dir)\n",
        "print(validation_human_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBYtA_Zd8_T"
      },
      "source": [
        "Now see what the filenames look like in these directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PIP1rkmeAYS"
      },
      "outputs": [],
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "validation_horse_names = os.listdir(validation_horse_dir)\n",
        "validation_human_names = os.listdir(validation_human_dir)\n",
        "\n",
        "print(f'TRAIN SET HORSES: {train_horse_names[:10]}\\n')\n",
        "print(f'TRAIN SET HUMANS: {train_human_names[:10]}\\n')\n",
        "print(f'VAL SET HORSES: {validation_horse_names[:10]}\\n')\n",
        "print(f'VAL SET HUMANS: {validation_human_names[:10]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlqN5KbafhLI"
      },
      "source": [
        "You can find out the total number of horse and human images in the directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4XHh2xSfgie"
      },
      "outputs": [],
      "source": [
        "print(f'Total training horse images:   {len(os.listdir(train_horse_dir))}')\n",
        "print(f'Total training human images:   {len(os.listdir(train_human_dir))}')\n",
        "print(f'Total validation horse images: {len(os.listdir(validation_horse_dir))}')\n",
        "print(f'Total validation human images: {len(os.listdir(validation_human_dir))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WZABE9eX-8"
      },
      "source": [
        "Now take a look at a few pictures to get a better sense of what they look like. First, configure the `matplotlib` parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2_Q0-_5UAv-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvHzGCxXkqp"
      },
      "source": [
        "Now, display a batch of 8 horse and 8 human pictures. You can rerun the cell to see a fresh batch each time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpr8GxjOU8in"
      },
      "outputs": [],
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname)\n",
        "                    for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname)\n",
        "                    for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "print(len(next_horse_pix), '-', next_horse_pix)\n",
        "print(len(next_human_pix), '-', next_human_pix)\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix + next_human_pix):\n",
        "    # Set up subplot; subplot indices start at 1\n",
        "    sp = plt.subplot(nrows, ncols, i + 1)\n",
        "    sp.axis('Off') # No show axes\n",
        "\n",
        "    img = mpimg.imread(img_path)\n",
        "    plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "## Building a Small Model from Scratch\n",
        "\n",
        "You will define the same model architecture as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvfZg3LQbD-5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tfk\n",
        "from tensorflow.keras import layers, optimizers, models, losses\n",
        "from tensorflow import nn\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Input shape: Desired size of the image 300x300 with 3 bytes color\n",
        "\n",
        "    # First convolution\n",
        "    layers.Conv2D(16, (3, 3), activation=nn.relu, input_shape=(300, 300, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second convolution\n",
        "    layers.Conv2D(32, (3, 3), activation=nn.relu),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Third convolution\n",
        "    layers.Conv2D(64, (3, 3), activation=nn.relu),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Fourth convolution\n",
        "    layers.Conv2D(64, (3, 3), activation=nn.relu),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # Fifth convolution\n",
        "    layers.Conv2D(64, (3, 3), activation=nn.relu),\n",
        "    layers.MaxPooling2D(2, 2),       \n",
        "\n",
        "    # Flatten the results to feed into a DNN\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation=nn.relu),  # 512 neuron hidden layer\n",
        "    # Only 1 output neuron\n",
        "    # It will contains a value from 0-1 where 0 for 1 class ('horses') and 1\n",
        "    #  for the other ('humans')\n",
        "    layers.Dense(1, activation=nn.sigmoid)                    \n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EaFDP5srBa"
      },
      "source": [
        "You can review the network architecture and the output shapes with `model.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZKj8392nbgP"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEkKSpZlvJXA"
      },
      "source": [
        "You will also use the same compile settings as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DHWhFP_uhq3"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.binary_crossentropy,\n",
        "              optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Now you will setup the data generators. It will mostly be the same as last time but notice the additional code to also prepare the validation data. It will need to be instantiated separately and also scaled to have `[0,1]` range of pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClebU9NJg99G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# All images will be rescaled by 1. / 255\n",
        "train_datagen      = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    './horse-or-human/', # The source directory for training images\n",
        "    target_size=(300, 300), # All images will be resized to 300x300\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary') # Since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "# Flow validation images in batches of 128 using validation_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    './validation-horse-or-human/', # The source directory for validation images\n",
        "    target_size=(300, 300), # All images will be resized to 300x300\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(len(train_generator), '-', len(validation_generator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu3Jdwkjwax4"
      },
      "source": [
        "### Training\n",
        "Now train the model for 15 epochs. Here, you will pass parameters for `validation_data` and `validation_steps`. With these, you will notice additional outputs in the print statements: `val_loss` and `val_accuracy`. Notice that as you train with more epochs, your training accuracy might go up but your validation accuracy goes down. This can be a sign of overfitting and you need to prevent your model from reaching this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb1_lgobv81m"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_generator, steps_per_epoch=8, epochs=15, verbose=1,\n",
        "                    validation_data=validation_generator, validation_steps=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH"
      },
      "source": [
        "### Model Prediction\n",
        "\n",
        "Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human.\n",
        "\n",
        "**Important Note:** Due to some compatibility issues, the following code block will result in an error after you select the images(s) to upload if you are running this notebook as a `Colab` on the `Safari` browser. For all other browsers, continue with the next code block and ignore the next one after it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoWp43WxJDNT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "    # Predicting images\n",
        "    path = '/content/' + fn\n",
        "\n",
        "    img = image.load_img(path, target_size=(300, 300))\n",
        "    img = image.img_to_array(img)\n",
        "    img = img / 255\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    images = np.vstack([img])\n",
        "    classes = model.predict(images, batch_size=10)\n",
        "    \n",
        "    if (classes[0] > 0.5):\n",
        "        print(f'There is a human in image_{i+1}')\n",
        "    else:\n",
        "        print(f'There is a horse in image_{i+1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8EHQyWGDvWz"
      },
      "source": [
        "### Visualizing Intermediate Representations\n",
        "\n",
        "As before, you can plot how the features are transformed as it goes through each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5tES8rXFjux"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "#   Define a new model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "visualization_model = models.Model(inputs=model.input,\n",
        "                                   outputs=successive_outputs)\n",
        "\n",
        "# Prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, fn) for fn in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, fn) for fn in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300)) # This is a PIL image\n",
        "x = img_to_array(img)         # Numpy array with shape (300, 300, 3)\n",
        "x = x.reshape((1,) + x.shape) # Numpy array with shape (1, 300, 300, 3)\n",
        "x = x / 255 # Scale by 1 / 255\n",
        "\n",
        "#   Run the image through the network, thus obtaining all intermediate\n",
        "# representations for this image\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so we can have them as part of the plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "    # Just do this for the conv/maxpool layers, not the fully-connected layers\n",
        "    if (len(feature_map.shape) == 4):\n",
        "\n",
        "        # Number of features in feature map\n",
        "        # The feature map has shape (1, size, size, n_features)\n",
        "        n_features = feature_map.shape[-1] \n",
        "        size       = feature_map.shape[1]\n",
        "\n",
        "        # Tile the images in this matrix\n",
        "        display_grid = np.zeros((size, size * n_features))\n",
        "        for i in range(n_features):\n",
        "            x = feature_map[0, :, :, i]\n",
        "            x -= x.mean()\n",
        "            x /= x.std()\n",
        "            x *= 64\n",
        "            x += 128\n",
        "            x = np.clip(x, 0, 255).astype('uint8')\n",
        "\n",
        "            # Tile each filter into this big horizontal grid\n",
        "            display_grid[:, i * size : (i + 1) * size] = x\n",
        "\n",
        "        # Display the grid\n",
        "        scale = 20. / n_features\n",
        "        plt.figure(figsize=(scale * n_features, scale))\n",
        "        plt.title(layer_name)\n",
        "        plt.grid(False)\n",
        "        plt.imshow(display_grid, aspect='auto', cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "outputs": [],
      "source": [
        "# import os, signal\n",
        "# os.kill(os.getpid(), signal.SIGKILL)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C1_W4_Lab_2_image_generator_with_validation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}