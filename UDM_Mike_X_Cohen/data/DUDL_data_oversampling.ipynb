{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"YeuAheYyhdZw"},"outputs":[],"source":["# import libraries\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader,TensorDataset\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"OhLQ2YSvpiGj"},"source":["# A function that returns a dataset with a specified size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_tZ1ymVp0Sf"},"outputs":[],"source":["data_full = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n","\n","# Now for the function\n","def make_the_dataset(N, double_the_data=False):\n","\n","    # Extract labels (number IDs) and remove from data\n","    labels = data_full[:N, 0]\n","    data   = data_full[:N, 1:]\n","\n","    # Normalize the data to a range of [0 1]\n","    data_norm = data / np.max(data)\n","\n","    # Make an exact copy of ALL the data\n","    if double_the_data:\n","        data_norm = np.concatenate((data_norm, data_norm), axis=0)\n","        labels    = np.concatenate((labels, labels), axis=0)\n","\n","    # Convert to tensor\n","    data_tensor   = torch.tensor(data_norm).float()\n","    labels_tensor = torch.tensor(labels).long()\n","\n","    # Use scikitlearn to split the data\n","    train_data, test_data, train_labels, test_labels = train_test_split(data_tensor, labels_tensor, train_size=0.9)\n","\n","    # # Make an exact copy of the TRAIN data\n","    # if doubleTheData:\n","    #   train_data   = torch.cat((train_data,train_data),axis=0)\n","    #   train_labels = torch.cat((train_labels,train_labels),axis=0)\n","    \n","    # Convert into PyTorch Datasets\n","    train_data_set = TensorDataset(train_data, train_labels)\n","    test_data_set  = TensorDataset(test_data, test_labels)\n","\n","    # Translate into Dataloader objects\n","    batch_size   = 20\n","    train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n","                            shuffle=True, drop_last=True)\n","    test_loader  = DataLoader(dataset=test_data_set, \n","                            batch_size=test_data_set.tensors[0].shape[0])\n","\n","    return train_loader, test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQQAEnLDBeNm"},"outputs":[],"source":["# Check the sizes\n","r, t = make_the_dataset(N=200, double_the_data=False)\n","print(r.dataset.tensors[0].shape)\n","print(t.dataset.tensors[0].shape)\n","\n","r, t = make_the_dataset(N=200, double_the_data=True)\n","print(r.dataset.tensors[0].shape)\n","print(t.dataset.tensors[0].shape)"]},{"cell_type":"markdown","metadata":{"id":"OK8Opkhgp0bO"},"source":["# Create the DL model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JK3OO3tAtZkA"},"outputs":[],"source":["def create_the_MNIST_net():\n","    \"\"\"\"\n","    FFN_SCRAMBLEDMNIST | FFN_SHIFTEDMNIST | DATA_ DATA_OVERSAMPLING\n","    \"\"\"\n","    class mnist_net(nn.Module):\n","        def __init__(self):\n","            super().__init__()\n","\n","            # Input layer\n","            self.input = nn.Linear(784, 64)\n","\n","            # Hidden layer\n","            self.fc1 = nn.Linear(64, 32)\n","            self.fc2 = nn.Linear(32, 32)\n","\n","            # Output layer\n","            self.output = nn.Linear(32, 10)\n","\n","        # Forward pass\n","        def forward(self, x):\n","            x = F.relu(self.input(x))\n","            x = F.relu(self.fc1(x))\n","            x = F.relu(self.fc2(x))\n","\n","            return self.output(x)\n","        \n","    # Create the model instance\n","    net = mnist_net()\n","\n","    # Loss Function\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    # Optimizer\n","    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n","\n","    return net, loss_func, optimizer\n"]},{"cell_type":"markdown","metadata":{"id":"dvfGQIRGp0ht"},"source":["# Create a function that trains the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IblJo1NCp0kl"},"outputs":[],"source":["def train_the_model():\n","    \"\"\"\n","    FFN_SCRAMBLEDMNIST | DATA_ DATA_OVERSAMPLING\n","    \"\"\"\n","    \n","    num_epochs = 50\n","\n","    # Create a new model\n","    net, loss_func, optimizer = create_the_MNIST_net()\n","\n","    # Initialize\n","    losses    = torch.zeros(num_epochs)\n","    train_acc = []\n","    test_acc  = []\n","\n","    # Loop over epochs\n","    for epoch_i in range(num_epochs):\n","\n","        # Loop over training data batches\n","        batch_acc  = []\n","        batch_loss = [] \n","\n","        for X, y in train_loader:\n","            # Forward pass and loss\n","            y_hat = net(X)\n","            loss  = loss_func(y_hat,y)\n","\n","            # Backprop\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Loss from this batch\n","            batch_loss.append(loss.item())\n","\n","            # Compute accuracy\n","            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n","            matches_numeric = matches.float()                # Convert to numbers (1/0)\n","            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n","            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n","        # End of batch loop.\n","\n","        # Get the average training accuracy of the batches\n","        train_acc.append(np.mean(batch_acc))\n","\n","        # The average losses accross the batches\n","        losses[epoch_i] = np.mean(batch_loss)\n","\n","        # Test accuracy\n","        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n","        with torch.no_grad():\n","          y_hat = net(X)\n","        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n","\n","    # End epochs\n","\n","    return train_acc, test_acc, losses, net"]},{"cell_type":"markdown","metadata":{"id":"rkI7gcZH26m2"},"source":["# Run the model once to confirm that it works"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHzKOZjnp0qn"},"outputs":[],"source":["# Generate a dataset\n","train_loader, test_loader = make_the_dataset(N=5000)\n","\n","# Test it\n","train_acc, test_acc, losses, net = train_the_model()\n","\n","\n","# Plot the results\n","fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n","\n","ax[0].plot(losses)\n","ax[0].set_xlabel('Epochs')\n","ax[0].set_ylabel('Loss')\n","ax[0].set_ylim([0, 3])\n","ax[0].set_title('Model Loss')\n","\n","ax[1].plot(train_acc, label='Train')\n","ax[1].plot(test_acc,  label='Test')\n","ax[1].set_xlabel('Epochs')\n","ax[1].set_ylabel('Accuracy (%)')\n","ax[1].set_ylim([10, 100])\n","ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}')\n","ax[1].legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XpGm9xdQ27Ob"},"source":["# Run an experiment showing better performance with increased N\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9pCC1R2p0nu"},"outputs":[],"source":["# List of data sample sizes\n","sample_sizes = np.arange(start=500, stop=4001, step=500)\n","\n","# Initialize results matrix\n","results_single = np.zeros(shape=(len(sample_sizes), 3))\n","results_double = np.zeros(shape=(len(sample_sizes), 3))\n","\n","for sample_size_idx, sample_size_i in enumerate(sample_sizes):\n","\n","    # Without doubling the data!\n","    # Generate a dataset and train the model\n","    train_loader, test_loader        = make_the_dataset(N=sample_size_i, double_the_data=False)\n","    train_acc, test_acc, losses, net = train_the_model()\n","\n","    # Grab the results\n","    results_single[sample_size_idx, 0] = np.mean(train_acc[-5:])\n","    results_single[sample_size_idx, 1] = np.mean(test_acc[-5:])\n","    results_single[sample_size_idx, 2] = torch.mean(loss[-5:]).item()\n","\n","    # With doubling the data!\n","    # Generate a dataset and train the model\n","    train_loader, test_loader        = make_the_dataset(N=sample_size_i, double_the_data=True)\n","    train_acc, test_acc, losses, net = train_the_model()\n","\n","    # Grab the results\n","    results_double[sample_size_idx, 0] = np.mean(train_acc[-5:])\n","    results_double[sample_size_idx, 1] = np.mean(test_acc[-5:])\n","    results_double[sample_size_idx, 2] = torch.mean(loss[-5:]).item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SnUUHPm7xQE"},"outputs":[],"source":["fig, ax = plt.subplot(1, 3, figsize=(15, 5))\n","\n","# Axis and title labels\n","titles      = ['Train', 'Devset', 'Losses']\n","y_ax_labels = ['Accuracy', 'Accuracy', 'Losses']\n","\n","# Common features\n","for i in range(3):\n","\n","    # Plot the lines\n","    ax[i].plot(sample_sizes, results_single[:, i], 's-', label='Original')\n","    ax[i].plot(sample_sizes, results_double[:, i], 's-', label='Doubled')\n","\n","    # Make it look nicer\n","    ax[i].set_ylabel(y_ax_labels[i])\n","    ax[i].set_title(titles[i])\n","    ax[i].legend()\n","    ax[i].set_xlabel('Unique sample size')\n","    ax[i].grid('on')\n","\n","    if (i < 2):\n","        ax[i].set_ylim([20, 102])\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1E6m2P4I_MRP"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Uh28k_l29urR"},"source":["# Additional explorations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ib3uQtfv9wE2"},"outputs":[],"source":["# 1) Notice that we're using the \"test_dataset\" multiple times, which really means that it's the devset,\n","#    aka hold-out set, and not a true TEST set. A real test set gets evaluated only once. Modify the code\n","#    to create a test set, using images in dataFull that are not in dataNorm. Note that you don't need \n","#    to re-run the entire experiment; you only need to train two models (and save their 'net' outputs), so that\n","#    you can run the test data through (make sure to normalize the test data!). Then you can evaluate the test\n","#    performance relative to train and devset from those two models.\n","# \n","# 2) We've previously discovered that Adam can outperform SGD on the MNIST dataset. I used SGD here on purpose --\n","#    to make performance worse (!) so we could test for effects of oversampling. Re-run the experiment using\n","#    Adam to see whether you still get the same effects. \n","# "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMfIbRLNAjprsUyyiYQOHY5","collapsed_sections":[],"name":"DUDL_data_oversampling.ipynb","provenance":[{"file_id":"1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM","timestamp":1618343530003},{"file_id":"1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW","timestamp":1617803880910},{"file_id":"15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4","timestamp":1617737766196},{"file_id":"1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ","timestamp":1617734878578},{"file_id":"1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j","timestamp":1617196833019},{"file_id":"1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H","timestamp":1617124341706},{"file_id":"1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn","timestamp":1616697516760},{"file_id":"1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg","timestamp":1616615469755},{"file_id":"1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK","timestamp":1616608248670}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
