{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tronghieu2810/DEEP-LEARNING/blob/main/UDEMY/%5BMike_X_Cohen%5D_Deep_understanding/DUDL_total.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk8OkAQng6wD"
      },
      "source": [
        "# LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIkuGnHOg6wF"
      },
      "outputs": [],
      "source": [
        "# For DL modeling\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary # Getting summary info on models\n",
        "\n",
        "# For number-crunching\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# For dataset management\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')\n",
        "import seaborn as sns\n",
        "\n",
        "# For timing computations\n",
        "import time\n",
        "\n",
        "import copy\n",
        "\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "import sys\n",
        "\n",
        "# For doing PCA on the model output\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torchvision # For importing data\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFamHfcjj4d"
      },
      "source": [
        "# IMPORT DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJQ9EM9Njj4d"
      },
      "source": [
        "## TORCHVISION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kihicNEqjj4e"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "my_data = torchvision.datasets.CIFAR10(root='cifar10', download=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xX77M3ojj4e"
      },
      "source": [
        "## WEB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_27ltGOZjj4g"
      },
      "outputs": [],
      "source": [
        "data_url = 'https://www.cdc.gov/nchs/data/dvs/state-data-rates-90-95-99-19.xlsx'\n",
        "\n",
        "# Import directly into pandas\n",
        "data = pd.read_excel(data_url,header=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ8HCv7ajj4h"
      },
      "source": [
        "## HARD DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imPkrLM_jj4h"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLf_YTFnjj4i"
      },
      "source": [
        "## GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFotN7Cujj4j"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aotyZMjPg6wH"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouTc-BI0g6wH"
      },
      "source": [
        "## REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znLdc3bRg6wH"
      },
      "outputs": [],
      "source": [
        "N = 30\n",
        "x = torch.randn(N, 1) # 30 random numbers\n",
        "y = x + torch.randn(N, 1) / 2\n",
        "\n",
        "plt.plot(x, y, 's')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy3hJupvg6wI"
      },
      "source": [
        "## QWERTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSuhahqGg6wI"
      },
      "source": [
        "## QWETY 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSQuPF6EF8oV"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_per_clust = 300\n",
        "blur        = 1\n",
        "A           = [1, 1]\n",
        "B           = [5, 1]\n",
        "C           = [4, 3]\n",
        "\n",
        "# Generate data\n",
        "a = [A[0] + np.random.randn(n_per_clust) * blur, A[1] + np.random.randn(n_per_clust) * blur]\n",
        "b = [B[0] + np.random.randn(n_per_clust) * blur, B[1] + np.random.randn(n_per_clust) * blur]\n",
        "c = [C[0] + np.random.randn(n_per_clust) * blur, C[1] + np.random.randn(n_per_clust) * blur]\n",
        "\n",
        "# Concatanate into a matrix\n",
        "data_np   = np.hstack(tup=(a, b, c)).T\n",
        "\n",
        "# True labels\n",
        "labels_np = np.hstack(tup=(np.zeros(shape=(n_per_clust)), \n",
        "                           np.ones(shape=(n_per_clust)),\n",
        "                           1 + np.ones(shape=(n_per_clust))))\n",
        "\n",
        "# Convert to a pytorch tensor\n",
        "data   = torch.tensor(data_np).float()\n",
        "labels = torch.tensor(labels_np).long() # note: \"long\" format for CCE\n",
        "\n",
        "# Show the data\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.plot(data[np.where(labels == 0)[0], 0], data[np.where(labels == 0)[0], 1], 'bs', alpha=0.5)\n",
        "plt.plot(data[np.where(labels == 1)[0], 0], data[np.where(labels == 1)[0], 1], 'ko', alpha=0.5)\n",
        "plt.plot(data[np.where(labels == 2)[0], 0], data[np.where(labels == 2)[0], 1], 'r^', alpha=0.5)\n",
        "\n",
        "plt.title('The qwerties!')\n",
        "plt.xlabel('qwerty dimension 1')\n",
        "plt.ylabel('qwerty dimension 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V_9_iD_hP98"
      },
      "source": [
        "### CREATING FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gakM257ehP99"
      },
      "outputs": [],
      "source": [
        "def create_some_data(n_per_clust):\n",
        "    \"\"\"\n",
        "    qwerty 2\n",
        "    \"\"\"\n",
        "    \n",
        "    A = [1, 1]\n",
        "    B = [5, 1]\n",
        "    C = [4, 4]\n",
        "\n",
        "    # Generate data\n",
        "    a = [A[0] + np.random.randn(n_per_clust), A[1] + np.random.randn(n_per_clust)]\n",
        "    b = [B[0] + np.random.randn(n_per_clust), B[1] + np.random.randn(n_per_clust)]\n",
        "    c = [C[0] + np.random.randn(n_per_clust), C[1] + np.random.randn(n_per_clust)]\n",
        "\n",
        "    # Concatanate into a matrix\n",
        "    data_np   = np.hstack(tup=(a, b, c)).T\n",
        "\n",
        "    # True labels\n",
        "    labels_np = np.hstack(tup=(np.zeros(shape=(n_per_clust)), \n",
        "                            np.ones(shape=(n_per_clust)),\n",
        "                            1 + np.ones(shape=(n_per_clust))))\n",
        "    \n",
        "    # Pul all outputs into a Dict\n",
        "    output = {}\n",
        "    output['data']   = torch.tensor(data_np).float()\n",
        "    output['labels'] = torch.tensor(labels_np).long()\n",
        "\n",
        "    # Use scikitlearn to split the data\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(output['data'], output['labels'],\n",
        "                                                       train_size=0.9)\n",
        "    \n",
        "    ## Step 3: Convert into PyTorch Datasets\n",
        "    train_data_set = TensorDataset(train_data, train_labels)\n",
        "    test_data_set  = TensorDataset(test_data, test_labels)\n",
        "\n",
        "    # Step 4: Translate into Dataloader objects\n",
        "    batch_size   = 8\n",
        "    output['train_data'] = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                            shuffle=True, drop_last=True)\n",
        "    output['test_data']  = DataLoader(dataset=test_data_set, \n",
        "                            batch_size=test_data_set.tensors[0].shape[0])\n",
        "    \n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwhMbKx2jj4n"
      },
      "source": [
        "## QWETY 2 - 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Ws3ypVjj4o"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_per_clust = 300\n",
        "blur        = 1\n",
        "A           = [1, 1]\n",
        "B           = [5, 1]\n",
        "C           = [4, 3]\n",
        "\n",
        "# Generate data\n",
        "a = [A[0] + np.random.randn(n_per_clust) * blur, A[1] + np.random.randn(n_per_clust) * blur]\n",
        "b = [B[0] + np.random.randn(n_per_clust) * blur, B[1] + np.random.randn(n_per_clust) * blur]\n",
        "c = [C[0] + np.random.randn(n_per_clust) * blur, C[1] + np.random.randn(n_per_clust) * blur]\n",
        "\n",
        "# Concatanate into a matrix\n",
        "data_np   = np.hstack(tup=(a, b, c)).T\n",
        "\n",
        "# True labels\n",
        "labels_np = np.hstack(tup=(np.zeros(shape=(n_per_clust)), \n",
        "                           np.ones(shape=(n_per_clust)),\n",
        "                           1 + np.ones(shape=(n_per_clust))))\n",
        "\n",
        "# Convert to a pytorch tensor\n",
        "data   = torch.tensor(data_np).float()\n",
        "labels = torch.tensor(labels_np).long() # note: \"long\" format for CCE\n",
        "\n",
        "# Show the data\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Draw distance to origin\n",
        "color = 'bkr'\n",
        "for i in range(len(data)):\n",
        "    plt.plot([0, data[i, 0]], [0, data[i, 1]], color=color[labels[i]], alpha=0.2)\n",
        "\n",
        "plt.plot(data[np.where(labels == 0)[0], 0], data[np.where(labels == 0)[0], 1], 'bs', alpha=0.5)\n",
        "plt.plot(data[np.where(labels == 1)[0], 0], data[np.where(labels == 1)[0], 1], 'ko', alpha=0.5)\n",
        "plt.plot(data[np.where(labels == 2)[0], 0], data[np.where(labels == 2)[0], 1], 'r^', alpha=0.5)\n",
        "\n",
        "plt.grid(color=[0.9, 0.9, 0.9])\n",
        "plt.title('The qwerties!')\n",
        "plt.xlabel('qwerty dimension 1')\n",
        "plt.ylabel('qwerty dimension 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsP9HXhMjj4p"
      },
      "outputs": [],
      "source": [
        "# Compute Euclidean distance to the origin\n",
        "dist_2_orig = torch.sqrt((data[:, 0] ** 2) + (data[:, 1] ** 2))\n",
        "\n",
        "plt.plot(labels + torch.randn(900) / 10, dist_2_orig, 'o')\n",
        "plt.xticks([0, 1, 2], labels=['Blue', 'Black', 'Red'])\n",
        "plt.ylabel('Euclidean Distance (a.u.)')\n",
        "plt.title('Distance to Origin')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB8a-hPRjj4p"
      },
      "outputs": [],
      "source": [
        "# Add that to the data matrix\n",
        "data_aug = torch.cat((data, dist_2_orig.view(len(data), 1)), axis=1)\n",
        "\n",
        "# Check data sizes\n",
        "print(data.shape)\n",
        "print(data_aug.shape)\n",
        "print('')\n",
        "\n",
        "# Look at some of the data\n",
        "print(data_aug)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G3vetqpg6wJ"
      },
      "source": [
        "## QWETY DOUGNUTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyWZvJ811z5a"
      },
      "source": [
        "## IRIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJjUQq3F112P"
      },
      "outputs": [],
      "source": [
        "# Import dataset (comes with seaborn)\n",
        "import seaborn as sns\n",
        "iris = sns.load_dataset('iris')\n",
        "\n",
        "# Convert from pandas dataframe to tensor | Final column is the outcome variable\n",
        "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
        "\n",
        "# Transform species to number\n",
        "labels = torch.zeros(len(data), dtype=torch.long)\n",
        "\n",
        "# Labels[iris.species=='setosa'] = 0 # don't need!\n",
        "labels[iris.species == 'versicolor'] = 1\n",
        "labels[iris.species == 'virginica'] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjAvYLcG17x9"
      },
      "outputs": [],
      "source": [
        "# Check out the first few lines of data\n",
        "iris.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe3v0AGNg6wJ"
      },
      "source": [
        "## WINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEV9txg-hP-C"
      },
      "outputs": [],
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "data = pd.read_csv(url, sep=';')\n",
        "# Remove rows with outliers\n",
        "data = data[data['total sulfur dioxide']<200]\n",
        "\n",
        "# Z-SCORE ALL VARIABLES EXCEPT FOR QUALITY\n",
        "# Find the columns we want to normalize (all except quality)\n",
        "cols_2_zscore = data.keys()\n",
        "cols_2_zscore = cols_2_zscore.drop('quality')\n",
        "data[cols_2_zscore] = data[cols_2_zscore].apply(stats.zscore)\n",
        "\n",
        "qual_threshold = 5\n",
        "# Create a new column for binarized (boolean) quality\n",
        "temp = (data['quality'] > qual_threshold).astype(int)\n",
        "data['boolQuality'] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5pZdFLqhP-C"
      },
      "outputs": [],
      "source": [
        "# Convert from pandas dataframe to tensor\n",
        "dataT  = torch.tensor(data=data[cols_2_zscore].values).float()\n",
        "labels = torch.tensor(data=data['boolQuality'].values).float()\n",
        "labels = labels[:, None] # Transform to matrix. We'll actually need the labels to be a \"tensor\"\n",
        "\n",
        "#  Use scikitlearn to split the data\n",
        "train_data, test_data, train_labels, test_labels = \\\n",
        "    train_test_split(dataT, labels, test_size=0.1)\n",
        "\n",
        "# Convert into PyTorch Datasets\n",
        "train_data_set = TensorDataset(train_data, train_labels)\n",
        "test_data_set  = TensorDataset(test_data, test_labels)\n",
        "\n",
        "# Translate into Dataloader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(dataset=test_data_set, \n",
        "                          batch_size=test_data_set.tensors[0].shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUHvrImYhP-D"
      },
      "source": [
        "### PROCESS THE DATA BY QUALITY METRIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnmSe8r7hP-D"
      },
      "outputs": [],
      "source": [
        "def create_a_dataset(qual_threshold):\n",
        "\n",
        "    # Create a new column for binarized (boolean) quality\n",
        "    data['boolQuality'] = 0\n",
        "\n",
        "    temp = (data['quality'] > qual_threshold).astype(int)\n",
        "    data['boolQuality'] = temp\n",
        "\n",
        "    # Convert from pandas dataframe to tensor\n",
        "    dataT  = torch.tensor(data=data[cols_2_zscore].values).float()\n",
        "    labels = torch.tensor(data=data['boolQuality'].values).float()\n",
        "    labels = labels[:, None] # Transform to matrix. We'll actually need the labels to be a \"tensor\"\n",
        "\n",
        "    # Use scikitlearn to split the data\n",
        "    train_data, test_data, train_labels, test_labels = \\\n",
        "        train_test_split(dataT, labels, test_size=0.1)\n",
        "\n",
        "    # Convert into PyTorch Datasets\n",
        "    train_data_set = TensorDataset(train_data, train_labels)\n",
        "    test_data_set  = TensorDataset(test_data, test_labels)\n",
        "\n",
        "    # Translate into Dataloader objects\n",
        "    batch_size   = 8\n",
        "    train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                            shuffle=True, drop_last=True)\n",
        "    test_loader  = DataLoader(dataset=test_data_set, \n",
        "                          batch_size=test_data_set.tensors[0].shape[0])\n",
        "    \n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz-tIPU8hP-D"
      },
      "outputs": [],
      "source": [
        "# Test the dataset and the number of samples\n",
        "\n",
        "# Note that the data are roughly balanced with thresh=5, not 4 or 6\n",
        "train_loader, test_loader = create_a_dataset(qual_threshold=6)\n",
        "\n",
        "# Get number of high/low quality wines\n",
        "quality_ratings = train_loader.dataset.tensors[1].detach()\n",
        "print(f'{torch.sum(quality_ratings == 0).item()} low-rated wines')\n",
        "print(f'{torch.sum(quality_ratings == 1).item()} high-rated wines')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xp4LDVag6wJ"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu9Fq0s6pV3J"
      },
      "outputs": [],
      "source": [
        "# Import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "labels = data[: ,0]\n",
        "data   = data[:, 1:]\n",
        "\n",
        "# normalize the data to a range of [0 1]\n",
        "data_norm = data / np.max(data)\n",
        "\n",
        "# Reshape to 2D (CNN)\n",
        "data_norm = data_norm.reshape(data_norm.shape[0], 1, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6tjBvB_hyyV"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax[0].hist(data.flatten(), 50)\n",
        "ax[0].set_xlabel('Pixel intensity values')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_title('Histogram of original data')\n",
        "ax[0].set_yscale('log')\n",
        "\n",
        "ax[1].hist(data_norm.flatten(), 50)\n",
        "ax[1].set_xlabel('Pixel intensity values')\n",
        "ax[1].set_ylabel('Count')\n",
        "ax[1].set_title('Histogram of normalized data')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbx2CK7D3Xkp"
      },
      "outputs": [],
      "source": [
        "# Normalize the data to 0 or 1\n",
        "data_norm = (data > 0).astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax[0].hist(data.flatten(), 50)\n",
        "ax[0].set_xlabel('Pixel intensity values')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_title('Histogram of original data')\n",
        "ax[0].set_yscale('log')\n",
        "\n",
        "ax[1].hist(data_norm.flatten(), 50)\n",
        "ax[1].set_xlabel('Pixel intensity values')\n",
        "ax[1].set_ylabel('Count')\n",
        "ax[1].set_title('Histogram of normalized data')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Confirm that the data have limited values\n",
        "print(np.unique(data))\n",
        "print('')\n",
        "print(np.unique(data_norm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrsSlPLZMvW2"
      },
      "source": [
        "### RANDOMLY SCRAMBLE THE DATA\n",
        "PRESERVING THE RE-ORDERING FOR EACH IMAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv2FtOeDM4Ve"
      },
      "outputs": [],
      "source": [
        "eggs      = np.random.permutation(data.shape[1]) # 784 pixels\n",
        "scrambled = data_norm[:, eggs] # Resort the pixels order\n",
        "\n",
        "# Show a few random digits\n",
        "fig, axs = plt.subplots(3, 4, figsize=(10, 6))\n",
        "\n",
        "for ax in axs.flatten():\n",
        "  # Pick a random image\n",
        "  rand_img_2_show = np.random.randint(0, high=data.shape[0])\n",
        "\n",
        "  # Create the image\n",
        "  img = np.reshape(scrambled[rand_img_2_show, :], (28, 28))\n",
        "  ax.imshow(img, cmap='gray')\n",
        "\n",
        "  # Title\n",
        "  ax.set_title(f'The number {labels[rand_img_2_show]}')\n",
        "\n",
        "plt.suptitle('The scrambled data', fontsize=20)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KMn5_uiTt2o"
      },
      "source": [
        "### SHIFT THE TEST IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ebjdOD_TvB0"
      },
      "outputs": [],
      "source": [
        "# First let's see how to shift a vectorized image\n",
        "# print(test_loader.dataset.tensors[0].shape)\n",
        "# Grab one image data\n",
        "temp = test_loader.dataset.tensors[0][0, :]\n",
        "# Reshape to 2D image\n",
        "temp = temp.reshape(28, 28)\n",
        "\n",
        "# Shift the image (pytorch calls it \"rolling\")\n",
        "temp_shift = torch.roll(temp, shifts=8, dims=1)\n",
        "\n",
        "# Now show them both\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
        "ax[0].imshow(temp, cmap='gray')\n",
        "ax[0].set_title('Original')\n",
        "\n",
        "ax[1].imshow(temp_shift, cmap='gray')\n",
        "ax[1].set_title('Shifted (Rolled)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjXWrQavU5u6"
      },
      "outputs": [],
      "source": [
        "# Now repeat for all images in the test set\n",
        "\n",
        "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
        "\n",
        "  # Get the image\n",
        "  img = test_loader.dataset.tensors[0][i, :]\n",
        "\n",
        "  # Reshape and roll by max. 10 pixels\n",
        "  rand_roll = np.random.randint(-10, 11)\n",
        "  img       = torch.roll(img.reshape(28, 28), shifts=rand_roll, dims=1)\n",
        "\n",
        "  # Re-vectorize and put back into the matrix \n",
        "  test_loader.dataset.tensors[0][i, :] = img.reshape(1, -1)\n",
        "\n",
        "# Note: now run the previous cell again to confirm the shifting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmPbJCieF_pq"
      },
      "outputs": [],
      "source": [
        "# Now repeat for all images in the test set\n",
        "# CNN \n",
        "for i in range(train_loader.dataset.tensors[0].shape[0]):\n",
        "\n",
        "  # Get the image\n",
        "  img = train_loader.dataset.tensors[0][i, :, :]\n",
        "\n",
        "  # Reshape and roll by max. 10 pixels\n",
        "  rand_roll = np.random.randint(-10, 11)\n",
        "  img       = torch.roll(img, shifts=rand_roll, dims=1)\n",
        "\n",
        "  # Re-vectorize and put back into the matrix \n",
        "  train_loader.dataset.tensors[0][i, :, :] = img\n",
        "\n",
        "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
        "\n",
        "  # Get the image\n",
        "  img = test_loader.dataset.tensors[0][i, :, :]\n",
        "\n",
        "  # Reshape and roll by max. 10 pixels\n",
        "  rand_roll = np.random.randint(-10, 11)\n",
        "  img       = torch.roll(img, shifts=rand_roll, dims=1)\n",
        "\n",
        "  # Re-vectorize and put back into the matrix \n",
        "  test_loader.dataset.tensors[0][i, :, :] = img\n",
        "\n",
        "# Note: now run the previous cell again to confirm the shifting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT8iXtR3aoKh"
      },
      "source": [
        "### NO7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q8GZujbapHV"
      },
      "outputs": [],
      "source": [
        "# Step 1: Convert to tensor\n",
        "data_tensor   = torch.tensor(data_norm).float()\n",
        "labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "# Boolean vector with the 7's\n",
        "where7 = (labels == 7)\n",
        "\n",
        "# Separate data into tensors with, and without 7's\n",
        "data_no7   = data_tensor[~where7, :]\n",
        "labels_no7 = labels_tensor[~where7]\n",
        "data7      = data_tensor[where7, :]\n",
        "\n",
        "# Step 3: Convert into PyTorch Datasets\n",
        "train_data_set = TensorDataset(data_no7, labels_no7)\n",
        "test_data_set  = TensorDataset(data7)\n",
        "\n",
        "# Step 4: Translate into Dataloader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(dataset=test_data_set, \n",
        "                          batch_size=test_data_set.tensors[0].shape[0])\n",
        "\n",
        "# Confirm that the separation is accurate\n",
        "print(np.unique(labels_no7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nSZI0t8KMMG"
      },
      "source": [
        "### AUTO_ENCODER_OCCLUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDbZhKS5KNPG"
      },
      "outputs": [],
      "source": [
        "# Reconstruct a sample as an image\n",
        "\n",
        "img = data_tensor[12345, :].view(28, 28)\n",
        "\n",
        "occluded = copy.deepcopy(img)\n",
        "occluded[10:13, :] = 1\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
        "\n",
        "ax[0].imshow(img, cmap='gray')\n",
        "ax[0].set_title('Original image')\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(occluded, cmap='gray')\n",
        "ax[1].set_title('Occluded image')\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (F)MNIST"
      ],
      "metadata": {
        "id": "cIGyuTeenBQL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU7rvmWuhjud"
      },
      "source": [
        "# MNIST DATA\n",
        "# Import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Extract labels, normalize, reshape\n",
        "labels_T    = torch.tensor(data[:, 0]).long()\n",
        "data        =              data[:, 1:]\n",
        "data_norm   = data / np.max(data)\n",
        "data_norm_T = torch.tensor(data_norm.reshape(data_norm.shape[0], 1, 28, 28)).float()\n",
        "\n",
        "# Split the data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data_norm_T, labels_T, test_size=0.1)\n",
        "\n",
        "# Convert into PyTorch Datasets\n",
        "train_data_set = TensorDataset(train_data, train_labels)\n",
        "test_data_set  = TensorDataset(test_data,  test_labels)\n",
        "\n",
        "# Translate into Dataloader objects\n",
        "batch_size   = 32\n",
        "numbers_train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                        shuffle=True, drop_last=True)\n",
        "numbers_test_loader  = DataLoader(dataset=test_data_set, \n",
        "                        batch_size=test_data_set.tensors[0].shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FMNIST data\n",
        "# Transformation\n",
        "transform = T.Compose([T.ToTensor(), T.Normalize(0.5, 0.5)])\n",
        "\n",
        "# Import the data and simutaneously apply the transform\n",
        "train_data_set = torchvision.datasets.FashionMNIST(root='./data', train=True,   download=True, transform=transform)\n",
        "test_data_set  = torchvision.datasets.FashionMNIST(root='./data', train=False,  download=True, transform=transform)\n",
        "\n",
        "# Transform to dataloaders\n",
        "batch_size           = 32\n",
        "fashion_train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                        shuffle=True, drop_last=True)\n",
        "fashion_test_loader  = DataLoader(dataset=test_data_set, \n",
        "                        batch_size=len(test_data_set))"
      ],
      "metadata": {
        "id": "vJcsrgdzoY2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform = T.Compose([T.ToTensor(),\n",
        "                       T.RandomHorizontalFlip(p=0.5),\n",
        "                       T.Normalize(0.5, 0.5)])\n",
        "\n",
        "# Import the data and simultaneously apply the transform\n",
        "train_set = torchvision.datasets.FashionMNIST(root='./data', train=True,   download=True, transform=transform)\n",
        "dev_test  = torchvision.datasets.FashionMNIST(root='./data', train=False,  download=True, transform=transform)\n",
        "\n",
        "# Split the devtest into two separate sets\n",
        "rand_idx = np.random.permutation(10000) # Random permutation of indices\n",
        "dev_set  = Subset(dev_test, rand_idx[:6000])\n",
        "test_set = Subset(dev_test, rand_idx[6000:])\n",
        "\n",
        "# Translate into DataLoader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "dev_loader   = DataLoader(dataset=dev_set,   batch_size=len(dev_set))\n",
        "test_loader  = DataLoader(dataset=test_set,  batch_size=len(test_set))\n"
      ],
      "metadata": {
        "id": "-U3plyduic9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN"
      ],
      "metadata": {
        "id": "x6X83_m1fgzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset (comes with colab!)\n",
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# extract labels (number IDs) and remove from data\n",
        "data   = data[:, 1:]\n",
        "\n",
        "# normalize the data to a range of [-1 1]\n",
        "data_norm = data / np.max(data)\n",
        "data_norm = 2 * data_norm - 1\n",
        "\n",
        "data_T = torch.tensor(data_norm).float()\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "kKQV3QX9fhvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EMNIST"
      ],
      "metadata": {
        "id": "881-xrrgh3LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "cdata = torchvision.datasets.EMNIST(root='emnist', split='letters', download=True)\n",
        "\n",
        "# Transform to 4D tensor for conv layer (and transform from int8 to float)\n",
        "images = cdata.data.view([124800, 1, 28, 28]).float()\n",
        "\n",
        "# Eliminate the N/A and subtract 1 from the original\n",
        "# Remove the first class category\n",
        "letter_categories = cdata.classes[1:]\n",
        "# Relabel labels to start at 0\n",
        "labels = copy.deepcopy(cdata.targets) - 1\n",
        "\n",
        "# Normalize the images\n",
        "images /= torch.max(images)"
      ],
      "metadata": {
        "id": "6NcIvcDbh6dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REVIEW"
      ],
      "metadata": {
        "id": "5EzVa_ELktfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSPECT THE DATA\n",
        "\n",
        "# The categories\n",
        "print(cdata.classes)\n",
        "print(f'{len(cdata.classes)} classes')\n",
        "print(f'\\nData size: {cdata.data.shape}')\n",
        "\n",
        "# Transform to 4D tensor for conv layer (and transform from int8 to float)\n",
        "images = cdata.data.view([124800, 1, 28, 28]).float()\n",
        "print(f'Tensor data: {images.shape}')"
      ],
      "metadata": {
        "id": "IvYakrI9iUjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminate the N/A and subtract 1 from the original\n",
        "# Remove the first class category\n",
        "letter_categories = cdata.classes[1:]\n",
        "\n",
        "# Relabel labels to start at 0\n",
        "labels = copy.deepcopy(cdata.targets) - 1\n",
        "print(labels.shape)\n",
        "\n",
        "print(torch.sum(labels == 0))\n",
        "print(torch.unique(labels))"
      ],
      "metadata": {
        "id": "oEsrbGQxj69Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next issue: do we need to normalize the images?\n",
        "plt.hist(images[:10, :, :, :].view(1, -1).detach(), 40);\n",
        "plt.title('Raw values')\n",
        "plt.show()\n",
        "\n",
        "# yarp.\n",
        "images /= torch.max(images)\n",
        "\n",
        "plt.hist(images[:10, :, :, :].view(1, -1).detach(), 40);\n",
        "plt.title('After normalization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hSCzjZnKksJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some images\n",
        "fig, axs = plt.subplots(3, 7, figsize=(13, 6))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "\n",
        "    # Pick a random pic\n",
        "    which_pic = np.random.randint(images.shape[0])\n",
        "\n",
        "    # Extract the image and its target letter\n",
        "    I      = np.squeeze(images[which_pic, :, :])\n",
        "    letter = letter_categories[labels[which_pic]]\n",
        "    \n",
        "    # Visualize\n",
        "    ax.imshow(I.T, cmap='gray')\n",
        "    ax.set_title(f'The letter {letter}')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yz1FqhTDlVGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTrXfA_Nb3cq"
      },
      "source": [
        "## CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2818LenFb3cq"
      },
      "outputs": [],
      "source": [
        "c_data = torchvision.datasets.CIFAR10(root='cifar10', download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yEM2e3_C6RFt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqN4H_qDfJwi"
      },
      "source": [
        "## GAUSS\n",
        "Create Gaussian blurs with different widths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrWEB_kpfJwi"
      },
      "outputs": [],
      "source": [
        "n_per_class = 1000 # Total 2000 images\n",
        "img_size    = 91\n",
        "\n",
        "x    = np.linspace(start=-4, stop=4, num=img_size)\n",
        "X, Y = np.meshgrid(x, x)\n",
        "\n",
        "# The two widths (a.u.) sigma\n",
        "widths = [1.8, 2.4]\n",
        "\n",
        "# Initialize tensors containing images and labels\n",
        "images = torch.zeros(2 * n_per_class, 1, img_size, img_size)\n",
        "labels = torch.zeros(2 * n_per_class)\n",
        "\n",
        "for i in range(2 * n_per_class):\n",
        "    \n",
        "    # Create the gaussian with random centers\n",
        "    # Ro = random offset\n",
        "    # `i % 2`: Even image belong to label 0\n",
        "    ro = 2 * np.random.randn(2)\n",
        "    G  = np.exp(-((X - ro[0]) ** 2 + (Y - ro[1]) ** 2) / (2 * widths[i % 2] ** 2))\n",
        "    \n",
        "    # And add noise\n",
        "    G += np.random.randn(img_size, img_size) / 5\n",
        "    \n",
        "    # Add to the tensor\n",
        "    # `.view`: Convert from numpy matrix to PTorch Tensor\n",
        "    images[i, :, :, :] = torch.tensor(G).view(1, img_size, img_size)\n",
        "    labels[i]          = i % 2\n",
        "\n",
        "labels = labels[:, None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H33OUrT6fJwi"
      },
      "outputs": [],
      "source": [
        "# Visualize some images\n",
        "fig, axs = plt.subplots(3, 7, figsize=(13, 6))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    which_pic = np.random.randint(2 * n_per_class)\n",
        "    G         = np.squeeze(images[which_pic, :, :])\n",
        "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
        "    ax.set_title(f'Class {int(labels[which_pic].item())}')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTidNPxrhvab"
      },
      "source": [
        "### VARYING WIDTHS AND OCCLUDING BARS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHI7Qdmnhvac"
      },
      "outputs": [],
      "source": [
        "n_gauss     = 1000 # Total 2000 images\n",
        "img_size    = 91\n",
        "\n",
        "x    = np.linspace(start=-4, stop=4, num=img_size)\n",
        "X, Y = np.meshgrid(x, x)\n",
        "\n",
        "# Vary the weight smoothly\n",
        "widths = np.linspace(start=2, stop=20, num=n_gauss)\n",
        "\n",
        "# Initialize tensors containing images\n",
        "images = torch.zeros(n_gauss, 1, img_size, img_size)\n",
        "\n",
        "for i in range(n_gauss):\n",
        "    \n",
        "    # Create the gaussian with random centers\n",
        "    # Ro = random offset\n",
        "    # `i % 2`: Even image belong to label 0\n",
        "    ro = 1.5 * np.random.randn(2)\n",
        "    G  = np.exp(-((X - ro[0]) ** 2 + (Y - ro[1]) ** 2) / widths[i])\n",
        "    \n",
        "    # And add noise\n",
        "    G += np.random.randn(img_size, img_size) / 5\n",
        "    \n",
        "    # Add a random bar randomly\n",
        "    i1 = np.random.choice(np.arange(2, 28))\n",
        "    i2 = np.random.choice(np.arange(2, 6))\n",
        "    \n",
        "    if np.random.randn() > 0:\n",
        "        G[i1:i1 + i2, :] = 1\n",
        "    else:\n",
        "        G[:, i1:i1 + i2] = 1\n",
        "    \n",
        "    # Add to the tensor\n",
        "    # `.view`: Convert from numpy matrix to PTorch Tensor\n",
        "    images[i, :, :, :] = torch.tensor(G).view(1, img_size, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW1WZAcGhvac"
      },
      "outputs": [],
      "source": [
        "# Visualize some images\n",
        "fig, axs = plt.subplots(3, 7, figsize=(13, 6))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    which_pic = np.random.randint(n_gauss)\n",
        "    G         = np.squeeze(images[which_pic, :, :])\n",
        "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kjybuQwhvac"
      },
      "source": [
        "### OCC AND NON-OCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdaahoeuhvac"
      },
      "outputs": [],
      "source": [
        "n_gauss     = 1000 # Total 2000 images\n",
        "img_size    = 91\n",
        "\n",
        "x    = np.linspace(start=-4, stop=4, num=img_size)\n",
        "X, Y = np.meshgrid(x, x)\n",
        "\n",
        "# Vary the weight smoothly\n",
        "widths = np.linspace(start=2, stop=20, num=n_gauss)\n",
        "\n",
        "# Initialize 02 tensors containing images\n",
        "images_occ    = torch.zeros(n_gauss, 1, img_size, img_size)\n",
        "images_no_occ = torch.zeros(n_gauss, 1, img_size, img_size)\n",
        "\n",
        "\n",
        "for i in range(n_gauss):\n",
        "    \n",
        "    # Create the gaussian with random centers\n",
        "    # Ro = random offset\n",
        "    # `i % 2`: Even image belong to label 0\n",
        "    ro = 1.5 * np.random.randn(2)\n",
        "    G  = np.exp(-((X - ro[0]) ** 2 + (Y - ro[1]) ** 2) / widths[i])\n",
        "    G += np.random.randn(img_size, img_size) / 5 # And add noise\n",
        "    \n",
        "    # Add the original to the no_occ\n",
        "    images_no_occ[i, :, :, :] = torch.tensor(G).view(1, img_size, img_size)\n",
        "    \n",
        "    # Add a random bar randomly\n",
        "    i1 = np.random.choice(np.arange(10, img_size - 10))\n",
        "    i2 = np.random.choice(np.arange(2, 6))\n",
        "    \n",
        "    if np.random.randn() > 0:   G[i1:i1 + i2, :] = 1\n",
        "    else:                       G[:, i1:i1 + i2] = 1\n",
        "    \n",
        "    # Add to the tensor with occlusion\n",
        "    # `.view`: Convert from numpy matrix to PTorch Tensor\n",
        "    images_occ[i, :, :, :] = torch.tensor(G).view(1, img_size, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTzNA6MHhvac"
      },
      "outputs": [],
      "source": [
        "# Visualize some images\n",
        "fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
        "\n",
        "for i in range(10):\n",
        "    which_pic = np.random.randint(n_gauss)\n",
        "    \n",
        "    axs[0, i].imshow(np.squeeze(images_no_occ[which_pic, :, :]), vmin=-1, vmax=1, cmap='jet')\n",
        "    axs[0, i].set_xticks([]), axs[0, i].set_yticks([])\n",
        "    \n",
        "    axs[1, i].imshow(np.squeeze(images_occ   [which_pic, :, :]), vmin=-1, vmax=1, cmap='jet')\n",
        "    axs[1, i].set_xticks([]), axs[1, i].set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhukLj8Yhvad"
      },
      "source": [
        "### FIND GAUSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFGGAKIghvad"
      },
      "outputs": [],
      "source": [
        "n_gauss  = 1000\n",
        "img_size = 91\n",
        "\n",
        "x    = np.linspace(start=-4, stop=4, num=img_size)\n",
        "X, Y = np.meshgrid(x, x)\n",
        "\n",
        "# Initialize tensors containing images and labels\n",
        "images = torch.zeros(n_gauss, 1, img_size, img_size)\n",
        "labels = torch.zeros(n_gauss, 3)\n",
        "\n",
        "for i in range(n_gauss):\n",
        "    # Location and width parameters\n",
        "    loc = np.max(x) / 2 + np.random.randn(2) # Center coordinate\n",
        "    wid = np.random.rand() * 10 + 5          # Width of Gaussian\n",
        "    \n",
        "    # Create the Gaussian with random centers\n",
        "    G =  np.exp(-((X - loc[0]) ** 2 + (Y - loc[1]) ** 2) / wid)\n",
        "    G += np.random.randn(img_size, img_size) / 10\n",
        "    \n",
        "    # Add to the tensor\n",
        "    images[i, :, :, :] = torch.tensor(G).view(1, img_size, img_size)\n",
        "    labels[i, :]       = torch.tensor([loc[0], loc[1], wid])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrDZVh1rhvad"
      },
      "outputs": [],
      "source": [
        "# Visualize some images\n",
        "fig, axs = plt.subplots(3, 7, figsize=(15, 7))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    which_pic = np.random.randint(n_gauss)\n",
        "    G         = np.squeeze(images[which_pic, :, :])\n",
        "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet', extent=[-4, 4, -4, 4], origin='upper')\n",
        "    ax.set_title(f'XY=({labels[which_pic, 0]:.0f}, {labels[which_pic, 1]:.0f}, W={labels[which_pic, 2]:.0f})')\n",
        "    ax.plot([-4, 4], [0, 0], 'w--')\n",
        "    ax.plot([0, 0], [-4, 4], 'w--')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN"
      ],
      "metadata": {
        "id": "RzAnKX_Zvnem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = 3000\n",
        "img_size = 64\n",
        "\n",
        "x    = np.linspace(start=-4, stop=4, num=img_size)\n",
        "X, Y = np.meshgrid(x, x)\n",
        "\n",
        "# Initialize tensors containing images and labels\n",
        "images = torch.zeros(n_images, 1, img_size, img_size)\n",
        "\n",
        "for i in range(n_images):\n",
        "    \n",
        "    # Create the gaussian with random centers\n",
        "    # Ro = random offset\n",
        "    # `i % 2`: Even image belong to label 0\n",
        "    ro    = 2 * np.random.randn(2)\n",
        "    width = np.random.rand() / 0.6 + 1.8 # Random width\n",
        "    G     = np.exp(-((X - ro[0]) ** 2 + (Y - ro[1]) ** 2) / (2 * width ** 2))\n",
        "    \n",
        "    # And add noise\n",
        "    G += np.random.randn(img_size, img_size) / 5\n",
        "    \n",
        "    # Add to the tensor\n",
        "    # `.view`: Convert from numpy matrix to PTorch Tensor\n",
        "    images[i, :, :, :] = torch.tensor(G).view(1, img_size, img_size)"
      ],
      "metadata": {
        "id": "K4FH4t82vpFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some images\n",
        "fig, axs = plt.subplots(3, 7, figsize=(13, 6))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    which_pic = np.random.randint(n_images)\n",
        "    G         = np.squeeze(images[which_pic, :, :])\n",
        "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BFIFwqtqwgtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STL10"
      ],
      "metadata": {
        "id": "sj2kuXWSUP3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform = T.Compose([T.ToTensor(),    # Normalizes to range [0, 1]\n",
        "                       T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) # Further normalization\n",
        "\n",
        "# Import the data and simultaneously apply the transform\n",
        "train_data_set = torchvision.datasets.STL10(root='./data', download=True, split='train', transform=transform)\n",
        "test_data_set  = torchvision.datasets.STL10(root='./data', download=True, split='test',  transform=transform)\n",
        "\n",
        "# Translate into Dataloader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                        shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(dataset=test_data_set, \n",
        "                        batch_size=256)"
      ],
      "metadata": {
        "id": "a86y9AXrUR8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REVIEW"
      ],
      "metadata": {
        "id": "C15FbXVEWfAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the datasets\n",
        "print(f'Data shapes (train/test):\\n {train_data_set.data.shape}\\n {test_data_set.data.shape}\\n')\n",
        "# Range of pixel intensity values\n",
        "print(f'Data value range: {np.min(train_data_set.data)} -> {np.max(train_data_set.data)}\\n')\n",
        "# The unique categories\n",
        "print(f'Data categories: {train_data_set.classes}')"
      ],
      "metadata": {
        "id": "TTM_qUCdWhWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of the data\n",
        "plt.hist(X.data.numpy().flatten(), 100);"
      ],
      "metadata": {
        "id": "ezVIbD-jXBBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INSPECT A FEW RANDOM IMAGES\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "for (i, ax) in enumerate(axs.flatten()):\n",
        "    # Extract that image (need to transpose it back to 32*32*3)\n",
        "    pic = X.data[i].numpy().transpose((1, 2, 0))\n",
        "    pic = pic - np.min(pic)\n",
        "    pit = pic / np.max(pic)\n",
        "\n",
        "    label = train_data_set.classes[y[i]]\n",
        "\n",
        "    ax.imshow(pic)\n",
        "    ax.text(0, 0, label, ha='left', va='top', fontweight='bold', color='k', backgroundcolor='y')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zdhcLqc7YGgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxXxFF7g6wK"
      },
      "source": [
        "# PREPROCESS DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UOYkTPui2O1"
      },
      "source": [
        "## CREATE TRAIN/TEST GROUPS USING DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wOxwT2qi3Az"
      },
      "outputs": [],
      "source": [
        "# Convert to tensor\n",
        "data_tensor   = torch.tensor(data_norm).float()\n",
        "labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "#  Use scikitlearn to split the data\n",
        "train_data, test_data, train_labels, test_labels = \\\n",
        "    train_test_split(data_tensor, labels_tensor, test_size=0.1)\n",
        "\n",
        "# Convert into PyTorch Datasets\n",
        "train_data_set = TensorDataset(train_data, train_labels)\n",
        "test_data_set  = TensorDataset(test_data, test_labels)\n",
        "\n",
        "# Translate into Dataloader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                        shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(dataset=test_data_set, \n",
        "                        batch_size=test_data_set.tensors[0].shape[0])\n",
        "\n",
        "# How many batches are there?\n",
        "# print(f'There are {len(train_loader)} batches, each with { batch_size} samples')\n",
        "\n",
        "# Sizes of each batch\n",
        "# for data, label in data_loader:\n",
        "#     print(f'BATCH INFO: {data.size()} | {label.size()}\\n')\n",
        "\n",
        "# print(train_loader.dataset.tensors) # All the images + All the labels\n",
        "# print(test_loader.dataset.tensors[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Elu37EfJwi"
      },
      "outputs": [],
      "source": [
        "# Use scikitlearn to split the data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(images, labels, test_size=0.1)\n",
        "\n",
        "# Convert into PyTorch Datasets\n",
        "train_data_set = TensorDataset(train_data, train_labels)\n",
        "test_data_set  = TensorDataset(test_data,  test_labels)\n",
        "\n",
        "# Translate into Dataloader objects\n",
        "batch_size   = 32\n",
        "train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                        shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(dataset=test_data_set, \n",
        "                        batch_size=test_data_set.tensors[0].shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u31ILz_Ejj4z"
      },
      "source": [
        "##  TRAIN/DEV/TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSh9hMlxjj40"
      },
      "outputs": [],
      "source": [
        "# Specify sizes of the partitions\n",
        "# Order is train, devset, test\n",
        "partitions = [3 * n_per_clust - 400, 200, 200]\n",
        "\n",
        "# Split the data\n",
        "train_data, dev_test_data, train_labels, dev_test_labels = train_test_split(data, labels, train_size=partitions[0])\n",
        "\n",
        "# Now split the devtest data\n",
        "dev_data, test_data, dev_labels, test_labels             = train_test_split(dev_test_data, dev_test_labels, train_size=partitions[1])\n",
        "\n",
        "# Print out the sizes\n",
        "print(f'   Total data size: {data.shape}')\n",
        "print('--------------------------------------------')\n",
        "print(f'Training data size: {train_data.shape}')\n",
        "print(f'  Devset data size: {dev_data.shape}')\n",
        "print(f'    Test data size: {test_data.shape}')\n",
        "\n",
        "# Convert them into PyTorch Datasets\n",
        "train_data_set = TensorDataset(train_data, train_labels)\n",
        "dev_data_set   = TensorDataset(dev_data,   dev_labels)\n",
        "test_data_set  = TensorDataset(test_data,  test_labels)\n",
        "\n",
        "# Translate into DataLoader objects\n",
        "batch_size   = 30\n",
        "train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "dev_loader   = DataLoader(dataset=dev_data_set,   batch_size=dev_data.tensors[0].shape[0])\n",
        "test_loader  = DataLoader(dataset=test_data_set,  batch_size=test_data.tensors[0].shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZnWvUeVfxIP"
      },
      "source": [
        "## CUSTOM DATASET CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOMXM1zjfwe2"
      },
      "outputs": [],
      "source": [
        "class custom_dataset(Dataset):\n",
        "    def __init__(self, tensors, transform=None):\n",
        "        \"\"\"\n",
        "        Check that sizes of data and labels match\n",
        "        All of the images must have corresponding labels\n",
        "        \"\"\"\n",
        "        assert all(tensors[0].size(0) == t.size(0) for t in tensors), \"Size mismatch between tensors\"\n",
        "\n",
        "        # Assign inputs\n",
        "        self.tensors   = tensors\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        What to do when someone wants and item from the dataset\n",
        "        Return transformed version of x if there are transforms\n",
        "        \"\"\"\n",
        "        if self.transform:\n",
        "            x = self.transform(self.tensors[0][index])\n",
        "        else:\n",
        "            x = self.tensors[0][index]\n",
        "\n",
        "        # And return label\n",
        "        y = self.tensors[1][index]\n",
        "\n",
        "        # Return the (data,label) tuple\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        # Total number of images in the dataset\n",
        "        return self.tensors[0].size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwBJ-m8Mhvae"
      },
      "source": [
        "# LOSS FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg6oBpe0hvae"
      },
      "source": [
        "## CNN_CUSTOM_LOSS_FUNC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cKD5qENhvae"
      },
      "outputs": [],
      "source": [
        "# L1 Loss Function\n",
        "class my_L1_Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, y_hat, y):\n",
        "        loss = torch.mean(torch.abs(y_hat - y))\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rpyX69khvaf"
      },
      "outputs": [],
      "source": [
        "# L2 + Average Loss Function\n",
        "class my_L2_Avg_Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, y_hat, y):\n",
        "        # MSE part\n",
        "        mse = torch.mean((y_hat - y) ** 2)\n",
        "        \n",
        "        # Average part\n",
        "        avg = torch.abs(torch.mean(y_hat))\n",
        "        \n",
        "        # Sum together\n",
        "        return mse + avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uCCWbsUhvaf"
      },
      "outputs": [],
      "source": [
        "# Correlation Loss Function\n",
        "class my_Corr_Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, y_hat, y):\n",
        "        mean_x = torch.mean(y_hat)\n",
        "        mean_y = torch.mean(y)\n",
        "        num    = torch.sum((y_hat - mean_x) * (y - mean_y))\n",
        "        den    = (torch.numel(y) - 1) * torch.std(y_hat) * torch.std(y)\n",
        "        \n",
        "        return -num / den"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bkshPl_g6wK"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0eYAmOAjj41"
      },
      "source": [
        "## LOAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN16fz4Kjj41"
      },
      "outputs": [],
      "source": [
        "# Create arbitary model\n",
        "net = create_the_MNIST_net()[0]\n",
        "# Replace that model with the one trained yet\n",
        "net.load_state_dict(torch.load('trainedModel.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeujhYlHg6wK"
      },
      "source": [
        "## CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZEBuiJIg6wK"
      },
      "source": [
        "### REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCoOdla0g6wL"
      },
      "outputs": [],
      "source": [
        "ANN_reg = nn.Sequential(    # Object containing the model\n",
        "    nn.Linear(1, 1),        # Input layer|Number of input, number of output\n",
        "    nn.ReLU(),              # Non-linear Activation Function\n",
        "    nn.Linear(1, 1)         # Output layer|Prediction of model\n",
        ")\n",
        "\n",
        "ANN_reg\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Loss function | The object that implements the MSE loss function\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "# Optimizer (the flavor of gradient descent to implement) | Stochastic GD\n",
        "optimizer = torch.optim.SGD(params=ANN_reg.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCuxVRd8hP-J"
      },
      "source": [
        "### META_PARAM_RELUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1LkENwLhP-J"
      },
      "outputs": [],
      "source": [
        "class ANN_wine(nn.Module):\n",
        "  \"\"\"\n",
        "  META_PARAM_RELUS\n",
        "  \"\"\"\n",
        "  def __init__(self, act_func):\n",
        "    super().__init__()\n",
        "\n",
        "    # LAYERS\n",
        "    # Input layer\n",
        "    self.input = nn.Linear(11, 16)\n",
        "\n",
        "    # Hidden layer(s). 'fc' = fully connected\n",
        "    self.fc1 = nn.Linear(16, 32)\n",
        "    self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "    # Output layer\n",
        "    self.output = nn.Linear(32, 1)\n",
        "\n",
        "    # Activation funcion to pass through\n",
        "    self.act_func = act_func\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward(self, x):\n",
        "    # Get activation function type, this code replaces torch.relu with torch.<self.actfun>\n",
        "    act_func = getattr(torch.nn, self.act_func)\n",
        "    x = act_func()(self.input(x))\n",
        "    x = act_func()(self.fc1(x))\n",
        "    x = act_func()(self.fc2(x))\n",
        "    x = self.output(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC620oVshP-K"
      },
      "source": [
        "### DATA_ UNBALANCED DATA | WEIGHTS_ XAVIER_VS._KAIMING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q18lmebDhP-K"
      },
      "outputs": [],
      "source": [
        "class ANN_wine(nn.Module):\n",
        "  \"\"\"\n",
        "  DATA_ UNBALANCED DATA | WEIGHTS_ XAVIER_VS._KAIMING\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # LAYERS\n",
        "    # Input layer\n",
        "    self.input = nn.Linear(11, 16)\n",
        "\n",
        "    # Hidden layer(s). 'fc' = fully connected\n",
        "    self.fc1 = nn.Linear(16, 32)\n",
        "    self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "    # Output layer\n",
        "    self.output = nn.Linear(32, 1)\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.input(x))\n",
        "    x = F.leaky_relu(self.fc1(x))\n",
        "    x = F.leaky_relu(self.fc2(x))\n",
        "\n",
        "    return self.output(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu-i8CYBGsBj"
      },
      "source": [
        "### WEIGHTS_ XAVIER_KAIMING_INITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cQUSe0_GsBk"
      },
      "outputs": [],
      "source": [
        "class the_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input layer\n",
        "        self.input = nn.Linear(100, 100)\n",
        "\n",
        "        # Hidden layer\n",
        "        self.fc1 = nn.Linear(100, 100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, 100)\n",
        "    \n",
        "        # Output layer\n",
        "        self.output = nn.Linear(100, 2)\n",
        "    \n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "\n",
        "        return self.output(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN_MNIST"
      ],
      "metadata": {
        "id": "tori35tefn9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class discriminator_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256    , 256)\n",
        "        self.out = nn.Linear(256    , 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        return torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "mT-5x5bifosF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class generator_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(64,  256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.out = nn.Linear(256, 784)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        return torch.tanh(x)"
      ],
      "metadata": {
        "id": "47upXFVwglCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN_CNN_GAUSS"
      ],
      "metadata": {
        "id": "Q9zeN2wXw1NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class discriminator_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution layers\n",
        "        self.conv1 = nn.Conv2d(1,    64, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64,  128, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv5 = nn.Conv2d(512,   1, kernel_size=4, stride=1, padding=0, bias=False)\n",
        "\n",
        "        # Batchnorm\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=512)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), negative_slope=0.2)\n",
        "        x = self.bn2(x)\n",
        "        x = F.leaky_relu(self.conv3(x), negative_slope=0.2)\n",
        "        x = self.bn3(x)\n",
        "        x = F.leaky_relu(self.conv4(x), negative_slope=0.2)\n",
        "        x = self.bn4(x)\n",
        "\n",
        "        return torch.sigmoid(self.conv5(x)).view(-1, 1)"
      ],
      "metadata": {
        "id": "KgT9nJoxw11G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class generator_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution layers\n",
        "        self.conv1 = nn.ConvTranspose2d(100, 512, kernel_size=4, stride=1, padding=0, bias=False)\n",
        "        self.conv2 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv4 = nn.ConvTranspose2d(128,  64, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.conv5 = nn.ConvTranspose2d(64,   1,  kernel_size=4, stride=2, padding=1, bias=False)\n",
        "\n",
        "        # Batchnorm\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=512)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=64)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = torch.tanh(self.conv5(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "IF3W2RnpyrXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE1tMkphg6wL"
      },
      "source": [
        "## CREATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF8EVLBjjj44"
      },
      "source": [
        "### META_PARAMS_MULTIOUTPUTS | DATA_SAVE_BEST_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ_vmn_Fjj45"
      },
      "outputs": [],
      "source": [
        "def create_the_qwety_net():\n",
        "    \"\"\"\"\n",
        "    META_PARAMS_MULTIOUTPUTS | DATA_SAVE_BEST_MODEL\n",
        "    \"\"\"\n",
        "    class qwerty_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear (2, 8)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(8, 8)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(8, 3)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.output(x)\n",
        "\n",
        "            return x\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = qwerty_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.01)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHoHmbfSGP4g"
      },
      "source": [
        "### META_PARAMS_OPTIMIZERS_QWERTY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96BgcARCGVzO"
      },
      "outputs": [],
      "source": [
        "def create_the_qwety_net(optimizer_algo, learning_rate):\n",
        "    \"\"\"\n",
        "    META_PARAMS_OPTIMIZERS_QWERTY\n",
        "    \"\"\"\n",
        "    class qwerty_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear (2, 8)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(8, 8)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(8, 3)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.output(x)\n",
        "\n",
        "            return x\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = qwerty_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer_func = getattr(torch.optim, optimizer_algo)\n",
        "    optimizer      = optimizer_func(net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    return net, loss_func, optimizer\n",
        "\n",
        "# # Test the model with optimizer type as input\n",
        "\n",
        "# # Try 'SGD', 'RMSprop', and 'Adam'\n",
        "# optim = create_the_qwety_net('RMSprop')[2]\n",
        "# optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AFBPexAkj0I"
      },
      "source": [
        "### FFN_NONMNIST, FFN_BINARIZED MNIST, FFN_NO7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWRtUp6gkqdE"
      },
      "outputs": [],
      "source": [
        "def create_the_MNIST_net():\n",
        "    \"\"\"\"\n",
        "    FFN_NONMNIST | FFN_BINARIZED MNIST | FFN_NO7\n",
        "    \"\"\"\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 64)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(64, 32)\n",
        "            self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(32, 10)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "\n",
        "            return torch.log_softmax(self.output(x), axis=1)\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net()\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.NLLLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYH1XrZJoqRV"
      },
      "source": [
        "### FFN_WEIGHTHISTOGRAMS, MODEL_PERFORM_APRF_WINE, WEIGHTS_FREEZE_WEIGHTS, WEIGHTS_WEIGHTS_CHANGES "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD7CLQckorLC"
      },
      "outputs": [],
      "source": [
        "def create_the_MNIST_net():\n",
        "    \"\"\"\"\n",
        "    FFN_WEIGHTHISTOGRAMS | MODEL_PERFORM_APRF_WINE | WEIGHTS_FREEZE_WEIGHTS | WEIGHTS_WEIGHTS_CHANGES\n",
        "    \"\"\"\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 64)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(64, 32)\n",
        "            self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(32, 10)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "\n",
        "            return self.output(x)\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net()\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YexnxKWR2qsO"
      },
      "source": [
        "### FFN_ BREADTH VS. DEPTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbiUXl922ruZ"
      },
      "outputs": [],
      "source": [
        "def create_the_MNIST_net(n_units, n_layers):\n",
        "    \"\"\"\"\n",
        "    FFN_ BREADTH VS. DEPTH\n",
        "    \"\"\"\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self, n_units, n_layers):\n",
        "            super().__init__()\n",
        "\n",
        "            # Create dictionary to store the layers\n",
        "            self.layers   = nn.ModuleDict()\n",
        "            self.n_layers = n_layers\n",
        "\n",
        "            # Input layer\n",
        "            self.layers['input'] = nn.Linear(784, n_units)\n",
        "\n",
        "            # Hidden layer\n",
        "            for i in range(n_layers):\n",
        "              self.layers[f'hidden{i}'] = nn.Linear(n_units, n_units)\n",
        "\n",
        "            # Output layer\n",
        "            self.layers['output'] = nn.Linear(n_units, 10)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            # Input layer\n",
        "            x = self.layers['input'](x)\n",
        "\n",
        "            # Hidden layers\n",
        "            for i in range(self.n_layers):\n",
        "              x = F.relu(self.layers[f'hidden{i}'](x))\n",
        "\n",
        "            return self.layers['output'](x)\n",
        "\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net(n_units, n_layers)\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "    return net, loss_func, optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQOhCVbzHCfz"
      },
      "source": [
        "### FFN_OPTIMIZERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9_2z5LAHEMA"
      },
      "outputs": [],
      "source": [
        "def create_the_mnist_net(optimizer_algo, learning_rate):\n",
        "    \"\"\"\n",
        "    FFN_OPTIMIZERS\n",
        "    \"\"\"\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 64)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(64, 32)\n",
        "            self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(32, 10)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "\n",
        "            return self.output(x)\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer_func = getattr(torch.optim, optimizer_algo)\n",
        "    optimizer      = optimizer_func(net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    return net, loss_func, optimizer\n",
        "\n",
        "# # Test the model with optimizer type as input\n",
        "\n",
        "# # Try 'SGD', 'RMSprop', and 'Adam'\n",
        "# optim = create_the_qwety_net('RMSprop')[2]\n",
        "# optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UaNMlpuOzoB"
      },
      "source": [
        "### FFN_SCRAMBLEDMNIST, FFN_SHIFTEDMNIST, DATA_ DATA_OVERSAMPLING, DATA_NOISE_AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT2m0MR4OzoG"
      },
      "outputs": [],
      "source": [
        "def create_the_MNIST_net():\n",
        "    \"\"\"\"\n",
        "    FFN_SCRAMBLEDMNIST | FFN_SHIFTEDMNIST | DATA_ DATA_OVERSAMPLING | DATA_NOISE_AUGMENTATION\n",
        "    \"\"\"\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 64)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(64, 32)\n",
        "            self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(32, 10)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "\n",
        "            return self.output(x)\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net()\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "    return net, loss_func, optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPyp2wLNhP-N"
      },
      "source": [
        "### DATA_DATA_VS_DEPTH_QWERTY2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ougEKWHLhP-O"
      },
      "outputs": [],
      "source": [
        "def create_the_qwerty_net(n_units, n_layers):\n",
        "  \"\"\"\n",
        "  DATA_DATA_VS_DEPTH_QWERTY2\n",
        "  \"\"\"\n",
        "  class qwerty_net(nn.Module):\n",
        "    def __init__(self, n_units, n_layers):\n",
        "      super().__init__()\n",
        "\n",
        "      # Create dictionary to store the layers\n",
        "      self.layers = nn.ModuleDict()\n",
        "      self.n_layers = n_layers\n",
        "\n",
        "      # Input layer\n",
        "      self.layers['input'] = nn.Linear(2, n_units) \n",
        "      \n",
        "      # Didden layers\n",
        "      for i in range(n_layers):\n",
        "        self.layers[f'hidden{i}'] = nn.Linear(n_units, n_units) \n",
        "\n",
        "      # Output layer\n",
        "      self.layers['output'] = nn.Linear(n_units, 3)\n",
        "    \n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "      # Input layer\n",
        "      x = self.layers['input'](x)\n",
        "\n",
        "      # Hidden layers\n",
        "      for i in range(self.n_layers):\n",
        "        x = F.relu(self.layers[f'hidden{i}'](x))\n",
        "      \n",
        "      # Return output layer\n",
        "      x = self.layers['output'](x)\n",
        "      return x\n",
        "  \n",
        "  # Create the model instance\n",
        "  net = qwerty_net(n_units, n_layers)\n",
        "  \n",
        "  # Loss function\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7nlUx9UhP-O"
      },
      "outputs": [],
      "source": [
        "# Test the model with fake input\n",
        "n_units_per_layer = 12\n",
        "n_layers = 4\n",
        "\n",
        "net, loss_func, optimizer = create_the_qwerty_net(n_units_per_layer, n_layers)\n",
        "print(net)\n",
        "\n",
        "# Input is 10 samples\n",
        "input = torch.rand(10, 2)\n",
        "net(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFDxPmNpjj49"
      },
      "source": [
        "### DATA_DATA_FEATURE_AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMf-CEBzjj49"
      },
      "outputs": [],
      "source": [
        "def create_the_qwety_net(use_extra_feature=False):\n",
        "    \"\"\"\"\n",
        "    DATA_DATA_FEATURE_AUGMENTATION\n",
        "    \"\"\"\n",
        "    class qwerty_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            if use_extra_feature:\n",
        "                self.input = nn.Linear(3, 8)\n",
        "            else:\n",
        "                self.input = nn.Linear (2, 8)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(8, 8)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(8, 3)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "\n",
        "            # By requeset, only use XY features\n",
        "            if not use_extra_feature:\n",
        "                x = x[:, :2]\n",
        "\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.output(x)\n",
        "\n",
        "            return x\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = qwerty_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fixtayZ_jj4-"
      },
      "source": [
        "### MODEL_PERFORM_APRF_WINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndeF2R3vjj4-"
      },
      "outputs": [],
      "source": [
        "class ANN_wine(nn.Module):\n",
        "    \"\"\"\"\n",
        "    APRF_WINE\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # LAYERS\n",
        "        # Input layer\n",
        "        self.input = nn.Linear(11, 16)\n",
        "\n",
        "        # Hidden layer(s). 'fc' = fully connected\n",
        "        self.fc1 = nn.Linear(16, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Linear(32, 1)\n",
        "    \n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        x = F.relu(input=self.input(x))\n",
        "        x = F.relu(input=self.fc1(x))\n",
        "        x = F.relu(input=self.fc2(x))\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgAVT_dethXS"
      },
      "source": [
        "### MODEL_PERFORM_MNIST_NO7, MODEL_PERFORM_TIME | WEIGHTS_DEMO_INITS | WEIGHTS_VARIANCE_INITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieN5ShkSthXS"
      },
      "outputs": [],
      "source": [
        "def create_the_MNIST_net():\n",
        "    \"\"\"\n",
        "    MODEL_PERFORM_MNIST_NO7 | MODEL_PERFORM_TIME | WEIGHTS_DEMO_INITS | WEIGHTS_VARIANCE_INITS\n",
        "    \"\"\"\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 64)\n",
        "\n",
        "            # Hidden layer\n",
        "            self.fc1 = nn.Linear(64, 32)\n",
        "            self.fc2 = nn.Linear(32, 32)\n",
        "\n",
        "            # Output layer\n",
        "            self.output = nn.Linear(32, 10)\n",
        "\n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "\n",
        "            return self.output(x)\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net()\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3VUT059io0I"
      },
      "source": [
        "### AUTOENCODER_DENOISING_MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Z69tvDio0I"
      },
      "outputs": [],
      "source": [
        "# Create a class for the model\n",
        "def create_the_MNIST_AE():\n",
        "    \"\"\"\n",
        "    AUTOENCODER_DENOISING_MNIST\n",
        "    \"\"\"\n",
        "    class ae_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 250)\n",
        "\n",
        "            # Encoder layer\n",
        "            self.enc = nn.Linear(250, 50)\n",
        "\n",
        "            # Latent layer\n",
        "            self.lat = nn.Linear(50, 250)\n",
        "\n",
        "            # Decoder layer\n",
        "            self.dec = nn.Linear(250, 784)\n",
        "    \n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.enc(x))\n",
        "            x = F.relu(self.lat(x))\n",
        "            y = torch.sigmoid(self.dec(x))\n",
        "\n",
        "            return y\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = ae_net()\n",
        "\n",
        "    # Loss function\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srg5ABcUJ1v4"
      },
      "source": [
        "### AUTOENCODER_HOW_MANY_UNIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj4WDSMhJ1v4"
      },
      "outputs": [],
      "source": [
        "# Create a class for the model\n",
        "def create_the_MNIST_AE(n_enc, n_bottle):\n",
        "    \"\"\"\n",
        "    AUTOENCODER_HOW_MANY_UNIT\n",
        "    \"\"\"\n",
        "    class ae_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, n_enc)\n",
        "\n",
        "            # Encoder layer\n",
        "            self.encoding = nn.Linear(n_enc, n_bottle)\n",
        "\n",
        "            # Latent layer\n",
        "            self.bottleneck = nn.Linear(n_bottle, n_enc)\n",
        "\n",
        "            # Decoder layer\n",
        "            self.decoding = nn.Linear(n_enc, 784)\n",
        "    \n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.encoding(x))\n",
        "            x = F.relu(self.bottleneck(x))\n",
        "            y = torch.sigmoid(self.decoding(x))\n",
        "\n",
        "            return y\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = ae_net()\n",
        "\n",
        "    # Loss function\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSlEMNUjVBXz"
      },
      "source": [
        "### AUTO_ENCODER_OCCLUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpUX-9AeVBXz"
      },
      "outputs": [],
      "source": [
        "# Create a class for the model\n",
        "def create_the_MNIST_AE():\n",
        "    \"\"\"\n",
        "    AUTO_ENCODER_OCCLUSION\n",
        "    \"\"\"\n",
        "    class ae_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 128)\n",
        "\n",
        "            # Encoder layer\n",
        "            self.enc = nn.Linear(128, 50)\n",
        "\n",
        "            # Latent layer\n",
        "            self.lat = nn.Linear(50, 128)\n",
        "\n",
        "            # Decoder layer\n",
        "            self.dec = nn.Linear(128, 784)\n",
        "    \n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.input(x))\n",
        "            x = F.relu(self.enc(x))\n",
        "            x = F.relu(self.lat(x))\n",
        "            y = torch.sigmoid(self.dec(x))\n",
        "\n",
        "            return y\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = ae_net()\n",
        "\n",
        "    # Loss function\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt1Lk5cz3Z7-"
      },
      "source": [
        "### AUTOENCODER_ LATENT_CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj-4g7993Z7-"
      },
      "outputs": [],
      "source": [
        "# Create a class for the model\n",
        "def create_the_MNIST_AE():\n",
        "    \"\"\"\n",
        "    AUTOENCODER_ LATENT_CODE\n",
        "    \"\"\"\n",
        "    class ae_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Input layer\n",
        "            self.input = nn.Linear(784, 150)\n",
        "\n",
        "            # Encoder layer\n",
        "            self.enc = nn.Linear(150, 15)\n",
        "\n",
        "            # Latent layer\n",
        "            self.lat = nn.Linear(15, 150)\n",
        "\n",
        "            # Decoder layer\n",
        "            self.dec = nn.Linear(150, 784)\n",
        "    \n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            x     = F.relu(self.input(x))\n",
        "            codex = F.relu(self.enc(x)) # Output the hidden-layer activation\n",
        "            x     = F.relu(self.lat(codex))\n",
        "            y     = torch.sigmoid(self.dec(x))\n",
        "\n",
        "            return y, codex\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = ae_net()\n",
        "\n",
        "    # Loss function\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4VwTqCyqcl6"
      },
      "source": [
        "### CNN_MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeSRoSo4qe5u"
      },
      "outputs": [],
      "source": [
        "def create_the_MNIST_net(print_toggle=False):\n",
        "    \"\"\"\n",
        "    CNN_MNIST\n",
        "    \"\"\"\n",
        "\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self, print_toggle):\n",
        "            super().__init__()\n",
        "            # Output Image size(Conv + Pool) = (Floor((Input + 2 * Padding - Filter) / Stride) + 1) / Pool\n",
        "\n",
        "            # Convolution layers. Not put Pooling layer here, do not have parameter.\n",
        "            self.conv1 = nn.Conv2d(1,  10, kernel_size=5, stride=1, padding=1) # Out(Max + Pool) = 13\n",
        "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=1) # Out(Max + Pool) = 5\n",
        "\n",
        "            # Compute the number of units in FClayer\n",
        "            expect_size = np.floor((5 + 2 * 0 - 1) / 1) + 1 # FC1 layer has no padding or kernel, so set to 0/1\n",
        "            expect_size = 20 * int(expect_size ** 2)        # 20 feature maps comes from conv2\n",
        "\n",
        "            # Fully-connected layer\n",
        "            self.fc1 = nn.Linear(expect_size, 50)\n",
        "\n",
        "            # Output layer\n",
        "            self.out = nn.Linear(50, 10)\n",
        "\n",
        "            # Toggle for printing out tensor sizes during forward prop\n",
        "            self.print = print_toggle\n",
        "        \n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            \n",
        "            print(f'Input: {x.shape}') if self.print else None\n",
        "\n",
        "            # Convolution -> Maxpool -> Relu\n",
        "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "            print(f'Layer conv1/pool1: {x.shape}') if self.print else None\n",
        "\n",
        "            # Convolution -> Maxpool -> Relu\n",
        "            x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "            print(f'Layer conv2/pool2: {x.shape}') if self.print else None\n",
        "\n",
        "            # Reshape for Linear layer\n",
        "            n_units = x.shape.numel() / x.shape[0] # Numbers in X / Number of data samples\n",
        "            x       = x.view(-1, int(n_units))     # Number of images / Number of units\n",
        "            if self.print: print(f'Vectorized: {x.shape}')\n",
        "\n",
        "            # Linear layer\n",
        "            x = F.relu(self.fc1(x))\n",
        "            if self.print: print(f'Layer fc1: {x.shape}')\n",
        "            x = self.out(x)\n",
        "            if self.print: print(f'Layer out: {x.shape}')\n",
        "\n",
        "            return x\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net(print_toggle)\n",
        "\n",
        "    # Loss function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5RTkOl_fJwq"
      },
      "source": [
        "### CNN_CLASSIFY_GAUSSIAN_BLURS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBPDk8hpfJwq"
      },
      "outputs": [],
      "source": [
        "def make_the_net():\n",
        "    \"\"\"\n",
        "    CNN_CLASSIFY_GAUSSIAN_BLURS\n",
        "    \"\"\"\n",
        "    class gauss_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            \n",
        "            # All layers in one go using nn.Sequential\n",
        "            self.enc = nn.Sequential(\n",
        "                nn.Conv2d(1, 6, kernel_size=3, padding=1),  # (91 + 2*1 -3)/1 + 1 = 91\n",
        "                nn.ReLU(),                                  # Treated like a \"layer\"\n",
        "                nn.AvgPool2d(2, 2),                         # 91 / 2 = 45\n",
        "                \n",
        "                \n",
        "                nn.Conv2d(6, 4, kernel_size=3, padding=1),  # (45 +2*1 -3)/1 + 1 = 45\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(2, 2),                         # 45 / 2 = 22\n",
        "                \n",
        "                nn.Flatten(),                               # Vectorize conv output\n",
        "                nn.Linear(22 * 22 * 4, 50),                 # 50\n",
        "                nn.Linear(50, 1)                            # 1\n",
        "            )\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.enc(x)\n",
        "        \n",
        "    \n",
        "    # Create the model instance\n",
        "    net = gauss_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    \n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNX68a7EfJwq"
      },
      "source": [
        "### CNN_GAUSS_FEATURE_MAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uMgGD6CfJwq"
      },
      "outputs": [],
      "source": [
        "def make_the_net():\n",
        "    \"\"\"\n",
        "    CNN_GAUSS_FEATURE_MAPS\n",
        "    \"\"\"\n",
        "    class gauss_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            \n",
        "            # Conv1\n",
        "            self.conv1 = nn.Conv2d(1, 6, kernel_size=3, padding=1)\n",
        "                # Out: (91 + 2*1 - 3)/1 + 1 = 91\n",
        "                # Pool: 91 / 2 = 45\n",
        "            \n",
        "            # Conv2\n",
        "            self.conv2 = nn.Conv2d(6, 4, kernel_size=3, padding=1)\n",
        "                # Out: (45 + 2*1 - 3)/1 + 1 = 45\n",
        "                # Pool: 45 / 2 = 22\n",
        "            \n",
        "            # Fc1\n",
        "            self.fc1 = nn.Linear(22*22*4, 50)\n",
        "            \n",
        "            # Fc2 (output)\n",
        "            self.fc2 = nn.Linear(50, 1)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # First conv-pool set\n",
        "            conv1_act = F.relu(self.conv1(x))\n",
        "            x         = F.avg_pool2d(conv1_act, (2, 2))\n",
        "            \n",
        "            # Second conv-pool set\n",
        "            conv2_act = F.relu(self.conv2(x))\n",
        "            x         = F.avg_pool2d(conv2_act, (2, 2))\n",
        "            \n",
        "            # ANN part\n",
        "            x = x.reshape(x.shape[0], -1)\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.fc2(x)\n",
        "            \n",
        "            return x, conv1_act, conv2_act\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = gauss_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    \n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUShb-mGfqJQ"
      },
      "source": [
        "### CNN_SOFTCODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okkD0tvefqJR"
      },
      "outputs": [],
      "source": [
        "def make_the_net():\n",
        "    \"\"\"\n",
        "    CNN_SOFTCODE\n",
        "    \"\"\"\n",
        "    class gauss_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # Soft-code variables\n",
        "            k = (3, 2) # Kernel size\n",
        "            s = (2, 3) # Stride size\n",
        "            \n",
        "            # Conv1\n",
        "            self.conv1 = nn.Conv2d(1, 3, kernel_size=k, stride=s)\n",
        "            \n",
        "            # Output of Conv layer (Second line is for pooling layer)\n",
        "            im_size_W = np.floor((91 + 2 * self.conv1.padding[0] - k[0]) / s[0]) + 1\n",
        "            im_size_W = np.floor(im_size_W / 2)\n",
        "            im_size_H = np.floor((91 + 2 * self.conv1.padding[1] - k[1]) / s[1]) + 1\n",
        "            im_size_H = np.floor(im_size_H / 2)\n",
        "\n",
        "            # Conv2\n",
        "            self.conv2 = nn.Conv2d(3, 6, kernel_size=k, stride=s)\n",
        "\n",
        "            # Requires the img size from the previous conv-pool layer\n",
        "            im_size_W = np.floor((im_size_W + 2 * self.conv2.padding[0] - k[0]) / s[0]) + 1\n",
        "            # Input to nn.Linear must be ints, and int() rounds down\n",
        "            im_size_W = int(im_size_W / 2) \n",
        "            im_size_H = np.floor((im_size_H + 2 * self.conv2.padding[1] - k[1]) / s[1]) + 1\n",
        "            im_size_H = int(im_size_H / 2)\n",
        "            \n",
        "            # Fc1\n",
        "            self.fc1 = nn.Linear(im_size_H * im_size_W * self.conv2.out_channels, 50)\n",
        "            \n",
        "            # Fc2 (output)\n",
        "            self.fc2 = nn.Linear(50, 1)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # First conv-pool set\n",
        "            conv1_act = F.relu(self.conv1(x))\n",
        "            x         = F.avg_pool2d(conv1_act, (2, 2))\n",
        "            \n",
        "            # Second conv-pool set\n",
        "            conv2_act = F.relu(self.conv2(x))\n",
        "            x         = F.avg_pool2d(conv2_act, (2, 2))\n",
        "            \n",
        "            # ANN part\n",
        "            x = x.reshape(x.shape[0], -1)\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.fc2(x)\n",
        "            \n",
        "            return x, conv1_act, conv2_act\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = gauss_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    \n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOtnOKEblV-t"
      },
      "source": [
        "### CNN_LINEAR_UNITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3vQVuGYlV-u"
      },
      "outputs": [],
      "source": [
        "def make_the_net(fc_units):\n",
        "    \"\"\"\n",
        "    CNN_LINEAR_UNITS\n",
        "    \"\"\"\n",
        "    class gauss_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            \n",
        "            # All layers in one go using nn.Sequential\n",
        "            self.enc = nn.Sequential(\n",
        "                nn.Conv2d(1, 6, kernel_size=3, padding=1),  # (91 + 2*1 -3)/1 + 1 = 91\n",
        "                nn.ReLU(),                                  # Treated like a \"layer\"\n",
        "                nn.AvgPool2d(2, 2),                         # 91 / 2 = 45\n",
        "                \n",
        "                \n",
        "                nn.Conv2d(6, 4, kernel_size=3, padding=1),  # (45 +2*1 -3)/1 + 1 = 45\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(2, 2),                         # 45 / 2 = 22\n",
        "                \n",
        "                nn.Flatten(),                               # Vectorize conv output\n",
        "                nn.Linear(22 * 22 * 4, 2 * fc_units),       \n",
        "                nn.Linear(2 * fc_units, fc_units),\n",
        "                nn.Linear(fc_units, 1)                            # 1\n",
        "            )\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.enc(x)\n",
        "        \n",
        "    \n",
        "    # Create the model instance\n",
        "    net = gauss_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    \n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpUEKQOZhvao"
      },
      "source": [
        "### CNN_GAUSS_AUTOENCODER|GAUSS_AE_OCCLUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qovy37IXhvao"
      },
      "outputs": [],
      "source": [
        "def make_the_net():\n",
        "    \"\"\"\n",
        "    CNN_GAUSS_AUTOENCODER|GAUSS_AE_OCCLUSION\n",
        "    \"\"\"\n",
        "    class gauss_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            \n",
        "            # Encoding layer\n",
        "            self.enc = nn.Sequential(\n",
        "                nn.Conv2d(1, 6, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2, 2),\n",
        "                \n",
        "                nn.Conv2d(6, 4, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2, 2)\n",
        "            )\n",
        "            \n",
        "            # Decoding layer\n",
        "            self.dec = nn.Sequential(\n",
        "                nn.ConvTranspose2d(4, 6, 3, 2),\n",
        "                nn.ReLU(),\n",
        "                nn.ConvTranspose2d(6, 1, 3, 2)\n",
        "            )\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.dec(self.enc(x))\n",
        "        \n",
        "    \n",
        "    # Create the model instance\n",
        "    net = gauss_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.MSELoss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    \n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZU3a-sIhvap"
      },
      "source": [
        "### CNN_CUSTOM_LOSS_FUNC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgOnCijehvap"
      },
      "outputs": [],
      "source": [
        "def make_the_net():\n",
        "    \"\"\"\n",
        "    CNN_CUSTOM_LOSS_FUNC\n",
        "    \"\"\"\n",
        "    class gauss_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            \n",
        "            # Encoding layer\n",
        "            self.enc = nn.Sequential(\n",
        "                nn.Conv2d(1, 6, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2, 2),\n",
        "                \n",
        "                nn.Conv2d(6, 4, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2, 2)\n",
        "            )\n",
        "            \n",
        "            # Decoding layer\n",
        "            self.dec = nn.Sequential(\n",
        "                nn.ConvTranspose2d(4, 6, 3, 2),\n",
        "                nn.ReLU(),\n",
        "                nn.ConvTranspose2d(6, 1, 3, 2)\n",
        "            )\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.dec(self.enc(x))\n",
        "        \n",
        "    \n",
        "    # Create the model instance\n",
        "    net = gauss_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = my_L1_Loss()\n",
        "    # loss_func = my_L2_Avg_Loss()\n",
        "    # loss_func = my_Corr_Loss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    \n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze6s3KpMhvap"
      },
      "source": [
        "### CNN_FIND_GAUSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3B0AAB5hvap"
      },
      "outputs": [],
      "source": [
        "def make_the_net():\n",
        "    \"\"\"\n",
        "    CNN_FIND_GAUSS\n",
        "    \"\"\"\n",
        "    class gauss_net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            \n",
        "            # All layers in one go using nn.Sequential\n",
        "            self.enc = nn.Sequential(\n",
        "                nn.Conv2d(1, 6, kernel_size=3, padding=1),  # (91 + 2*1 -3)/1 + 1 = 91\n",
        "                nn.ReLU(),                                  # Treated like a \"layer\"\n",
        "                nn.AvgPool2d(2, 2),                         # 91 / 2 = 45\n",
        "                \n",
        "                \n",
        "                nn.Conv2d(6, 4, kernel_size=3, padding=1),  # (45 +2*1 -3)/1 + 1 = 45\n",
        "                nn.ReLU(),\n",
        "                nn.AvgPool2d(2, 2),                         # 45 / 2 = 22\n",
        "                \n",
        "                nn.Flatten(),                               # Vectorize conv output\n",
        "                nn.Linear(22 * 22 * 4, 50),                 # 50\n",
        "                nn.Linear(50, 3)                            # 3\n",
        "            )\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.enc(x)\n",
        "        \n",
        "    \n",
        "    # Create the model instance\n",
        "    net = gauss_net()\n",
        "    \n",
        "    # Loss Function\n",
        "    loss_func = nn.MSELoss()\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    \n",
        "    return net, loss_func, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN_EMNIST, TRANSFER_LETTER2NUMBER"
      ],
      "metadata": {
        "id": "N0OelLXAm7L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_the_net(print_toggle=False):\n",
        "    \"\"\"\n",
        "    CNN_EMNIST, TRANSFER_LETTER2NUMBER\n",
        "    \"\"\"\n",
        "    class emnist_net(nn.Module):\n",
        "        def __init__(self, print_toggle):\n",
        "            super().__init__()\n",
        "\n",
        "            self.print = print_toggle\n",
        "\n",
        "            ######################## FEATURE MAP LAYERS ########################\n",
        "            self.conv1  = nn.Conv2d(1, 6, kernel_size=3, padding=1)\n",
        "            self.bnorm1 = nn.BatchNorm2d(num_features=6)\n",
        "                # (28 + 2*1 - 3)/1 + 1 = 28/2 = 14\n",
        "            \n",
        "            self.conv2  = nn.Conv2d(6, 6, kernel_size=3, padding=1)\n",
        "            self.bnorm2 = nn.BatchNorm2d(num_features=6)\n",
        "                # (14 + 2*1 - 3)/1 + 1 = 14/2 = 7\n",
        "\n",
        "            ######################## LINEAR DECISION LAYERS ####################\n",
        "            self.fc1 = nn.Linear(7 * 7 * 6, 50)\n",
        "            self.fc2 = nn.Linear(50, 26)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # Convolution -> Maxpool -> Batchnorm -> Relu\n",
        "            if self.print:    print(f'Input:            {list(x.shape)}')\n",
        "\n",
        "            x = F.max_pool2d(self.conv1(x), 2)\n",
        "            x = F.leaky_relu(self.bnorm1(x))\n",
        "            if self.print:    print(f'First CPR Block:  {list(x.shape)}')\n",
        "\n",
        "            x = F.max_pool2d(self.conv2(x), 2)\n",
        "            x = F.leaky_relu(self.bnorm2(x))\n",
        "            if self.print:    print(f'Second CPR Block: {list(x.shape)}')\n",
        "\n",
        "            # Reshape for linear layer\n",
        "            n_units = x.shape.numel() / x.shape[0]\n",
        "            x       = x.view(-1, int(n_units))\n",
        "            if self.print:    print(f'Vectorized:       {list(x.shape)}')\n",
        "\n",
        "            # Linear layer\n",
        "            x = F.leaky_relu(self.fc1(x))\n",
        "            x = self.fc2(x)\n",
        "            if self.print:    print(f'Final output:     {list(x.shape)}')\n",
        "\n",
        "            return x\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = emnist_net(print_toggle)\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ],
      "metadata": {
        "id": "h2gIae8Tm8CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN_ HOW_LOW"
      ],
      "metadata": {
        "id": "HQrjLJNJ38Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_the_net(print_toggle=False):\n",
        "    \"\"\"\n",
        "    CNN_ HOW_LOW\n",
        "    \"\"\"\n",
        "    class emnist_net(nn.Module):\n",
        "        def __init__(self, print_toggle):\n",
        "            super().__init__()\n",
        "\n",
        "            self.print = print_toggle\n",
        "\n",
        "            ######################## FEATURE MAP LAYERS ########################\n",
        "            self.conv1  = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "            self.bnorm1 = nn.BatchNorm2d(num_features=64)\n",
        "                # (28 + 2*1 - 3)/1 + 1 = 28/2 = 14\n",
        "            \n",
        "            self.conv2  = nn.Conv2d(64, 128, kernel_size=3)\n",
        "            self.bnorm2 = nn.BatchNorm2d(num_features=128)\n",
        "                # (14 + 2*0 - 3)/1 + 1 = 12/2 = 6\n",
        "            \n",
        "            self.conv3  = nn.Conv2d(128, 256, kernel_size=3)\n",
        "            self.bnorm3 = nn.BatchNorm2d(num_features=256)\n",
        "                # (6 + 2*0 - 3)/1 + 1 = 4/2 = 2\n",
        "\n",
        "            ######################## LINEAR DECISION LAYERS ####################\n",
        "            self.fc1 = nn.Linear(2 * 2 * 256, 256)\n",
        "            self.fc2 = nn.Linear(256, 64)\n",
        "            self.fc3 = nn.Linear(64, 26)\n",
        "\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # Convolution -> Maxpool -> Batchnorm -> Relu\n",
        "            if self.print:    print(f'Input:            {list(x.shape)}')\n",
        "\n",
        "            x = F.max_pool2d(self.conv1(x), 2)\n",
        "            x = F.leaky_relu(self.bnorm1(x))\n",
        "            x = F.dropout(input=x, p=0.25, training=self.training)\n",
        "            if self.print:    print(f'First CPR Block:  {list(x.shape)}')\n",
        "\n",
        "            x = F.max_pool2d(self.conv2(x), 2)\n",
        "            x = F.leaky_relu(self.bnorm2(x))\n",
        "            x = F.dropout(input=x, p=0.25, training=self.training)\n",
        "            if self.print:    print(f'Second CPR Block: {list(x.shape)}')\n",
        "\n",
        "            x = F.max_pool2d(self.conv3(x), 2)\n",
        "            x = F.leaky_relu(self.bnorm3(x))\n",
        "            x = F.dropout(input=x, p=0.25, training=self.training)\n",
        "            if self.print:    print(f'Third CPR Block:  {list(x.shape)}')\n",
        "\n",
        "            # Reshape for linear layer\n",
        "            n_units = x.shape.numel() / x.shape[0]\n",
        "            x       = x.view(-1, int(n_units))\n",
        "            x = F.dropout(input=x, p=0.25, training=self.training)\n",
        "            if self.print:    print(f'Vectorized:       {list(x.shape)}')\n",
        "\n",
        "            # Linear layer\n",
        "            x = F.leaky_relu(self.fc1(x))\n",
        "            x = F.dropout(input=x, p=0.5, training=self.training)\n",
        "            x = self.fc2(x)\n",
        "            x = F.dropout(input=x, p=0.5, training=self.training)\n",
        "            x = self.fc3(x)\n",
        "            if self.print:    print(f'Final output:     {list(x.shape)}')\n",
        "\n",
        "            return x\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = emnist_net(print_toggle)\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ],
      "metadata": {
        "id": "aThhO2ep38Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN_ NUM_CHANS"
      ],
      "metadata": {
        "id": "cb-lgRRL_ThX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_the_net(num_chans=(6, 6)):\n",
        "    \"\"\"\n",
        "    CNN_EMNIST\n",
        "    \"\"\"\n",
        "    class emnist_net(nn.Module):\n",
        "        def __init__(self, num_chans):\n",
        "            super().__init__()\n",
        "\n",
        "\n",
        "            ######################## FEATURE MAP LAYERS ########################\n",
        "            self.conv1  = nn.Conv2d(1, num_chans[0], kernel_size=3, padding=1)\n",
        "            self.bnorm1 = nn.BatchNorm2d(num_features=num_chans[0])\n",
        "                # (28 + 2*1 - 3)/1 + 1 = 28/2 = 14\n",
        "            \n",
        "            self.conv2  = nn.Conv2d(num_chans[0], num_chans[1], kernel_size=3, padding=1)\n",
        "            self.bnorm2 = nn.BatchNorm2d(num_features=num_chans[1])\n",
        "                # (14 + 2*1 - 3)/1 + 1 = 14/2 = 7\n",
        "\n",
        "            ######################## LINEAR DECISION LAYERS ####################\n",
        "            self.fc1 = nn.Linear(7 * 7 * num_chans[1], 50)\n",
        "            self.fc2 = nn.Linear(50, 26)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # Convolution -> Maxpool -> Batchnorm -> Relu\n",
        "\n",
        "            x = F.max_pool2d(self.conv1(x), 2)\n",
        "            x = F.leaky_relu(self.bnorm1(x))\n",
        "\n",
        "            x = F.max_pool2d(self.conv2(x), 2)\n",
        "            x = F.leaky_relu(self.bnorm2(x))\n",
        "\n",
        "            # Reshape for linear layer\n",
        "            n_units = x.shape.numel() / x.shape[0]\n",
        "            x       = x.view(-1, int(n_units))\n",
        "\n",
        "            # Linear layer\n",
        "            x = F.leaky_relu(self.fc1(x))\n",
        "            x = self.fc2(x)\n",
        "\n",
        "            return x\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = emnist_net(num_chans)\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ],
      "metadata": {
        "id": "N_NmXsFP_fnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER_FMNIST"
      ],
      "metadata": {
        "id": "WYDKGjAPpbx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_the_MNIST_net(print_toggle=False):\n",
        "    \"\"\"\n",
        "    TRANSFER_ FMNIST\n",
        "    \"\"\"\n",
        "\n",
        "    class mnist_net(nn.Module):\n",
        "        def __init__(self, print_toggle):\n",
        "            super().__init__()\n",
        "            # Output Image size(Conv + Pool) = (Floor((Input + 2 * Padding - Filter) / Stride) + 1) / Pool\n",
        "\n",
        "            # Convolution layers. Not put Pooling layer here, do not have parameter.\n",
        "            self.conv1 = nn.Conv2d(1,  10, kernel_size=5, stride=1, padding=1) # Out(Max + Pool) = 13\n",
        "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=1) # Out(Max + Pool) = 5\n",
        "\n",
        "            # Compute the number of units in FClayer\n",
        "            expect_size = np.floor((5 + 2 * 0 - 1) / 1) + 1 # FC1 layer has no padding or kernel, so set to 0/1\n",
        "            expect_size = 20 * int(expect_size ** 2)        # 20 feature maps comes from conv2\n",
        "\n",
        "            # Fully-connected layer\n",
        "            self.fc1 = nn.Linear(expect_size, 50)\n",
        "\n",
        "            # Output layer\n",
        "            self.out = nn.Linear(50, 10)\n",
        "\n",
        "            # Toggle for printing out tensor sizes during forward prop\n",
        "            self.print = print_toggle\n",
        "        \n",
        "        # Forward pass\n",
        "        def forward(self, x):\n",
        "            \n",
        "            print(f'Input: {x.shape}') if self.print else None\n",
        "\n",
        "            # Convolution -> Maxpool -> Relu\n",
        "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "            print(f'Layer conv1/pool1: {x.shape}') if self.print else None\n",
        "\n",
        "            # Convolution -> Maxpool -> Relu\n",
        "            x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "            print(f'Layer conv2/pool2: {x.shape}') if self.print else None\n",
        "\n",
        "            # Reshape for Linear layer\n",
        "            n_units = x.shape.numel() / x.shape[0] # Numbers in X / Number of data samples\n",
        "            x       = x.view(-1, int(n_units))     # Number of images / Number of units\n",
        "            if self.print: print(f'Vectorized: {x.shape}')\n",
        "\n",
        "            # Linear layer\n",
        "            x = F.relu(self.fc1(x))\n",
        "            if self.print: print(f'Layer fc1: {x.shape}')\n",
        "            x = self.out(x)\n",
        "            if self.print: print(f'Layer out: {x.shape}')\n",
        "\n",
        "            return x\n",
        "        \n",
        "    # Create the model instance\n",
        "    net = mnist_net(print_toggle)\n",
        "\n",
        "    # Loss function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.005)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ],
      "metadata": {
        "id": "O-qEypAncWec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER_ PRETRAIN_FMNIST"
      ],
      "metadata": {
        "id": "cEJbyB2Fkum2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_the_AE_net(print_toggle=False):\n",
        "    \"\"\"\n",
        "    TRANSFER_ PRETRAIN_FMNIST\n",
        "    \"\"\"\n",
        "    class AE_net(nn.Module):\n",
        "        def __init__(self, print_toggle):\n",
        "            super().__init__()\n",
        "\n",
        "            self.print = print_toggle\n",
        "\n",
        "            ########################## ENCODER LAYERS ##########################\n",
        "            self.enc_conv1 = nn.Conv2d(1, 16,  kernel_size=3, padding=1, stride=2)\n",
        "                # (28 + 2 * 1 - 3)/2 + 1 = 14\n",
        "            self.enc_conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=2)\n",
        "                # (14 + 2 * 1 - 3)/2 + 1 = 7\n",
        "\n",
        "            ############################ DECODER LAYERS ########################\n",
        "            self.dec_conv1 = nn.ConvTranspose2d(32, 16, kernel_size=4, padding=1, stride=2)\n",
        "                # (28 + 2 * 1 - 3)/2 + 1 = 14\n",
        "            self.dec_conv2 = nn.ConvTranspose2d(16, 1,  kernel_size=4, padding=1, stride=2)\n",
        "                # (14 + 2 * 1 - 3)/2 + 1 = 7\n",
        "\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # Convolution -> Maxpool -> Batchnorm -> Relu\n",
        "            if self.print:  print(f'Input:                {list(x.shape)}')\n",
        "\n",
        "            # First encoder layer\n",
        "            x = F.leaky_relu(self.enc_conv1(x))\n",
        "            if self.print:  print(f'First Encoder layer:  {list(x.shape)}')\n",
        "\n",
        "            # Second encoder layer\n",
        "            x = F.leaky_relu(self.enc_conv2(x))\n",
        "            if self.print:  print(f'Second Encoder layer: {list(x.shape)}')\n",
        "\n",
        "            # First Decoder layer\n",
        "            x = F.leaky_relu(self.dec_conv1(x))\n",
        "            if self.print:  print(f'First Decoder layer:  {list(x.shape)}')\n",
        "\n",
        "            # Second Decoder layer\n",
        "            x = F.leaky_relu(self.dec_conv2(x))\n",
        "            if self.print:  print(f'Second Decoder layer: {list(x.shape)}')       \n",
        "\n",
        "            return x\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = AE_net(print_toggle)\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ],
      "metadata": {
        "id": "Da3PFZ1gkvYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_the_class_net(print_toggle=False):\n",
        "    \"\"\"\n",
        "    TRANSFER_ PRETRAIN_FMNIST\n",
        "    \"\"\"\n",
        "    class cnn_net(nn.Module):\n",
        "        def __init__(self, print_toggle):\n",
        "            super().__init__()\n",
        "\n",
        "            self.print = print_toggle\n",
        "\n",
        "            ########################## ENCODER LAYERS ##########################\n",
        "            self.enc_conv1 = nn.Conv2d(1, 16,  kernel_size=3, padding=1, stride=2)\n",
        "                # (28 + 2 * 1 - 3)/2 + 1 = 14\n",
        "            self.enc_conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=2)\n",
        "                # (14 + 2 * 1 - 3)/2 + 1 = 7\n",
        "\n",
        "            ######################## LINEAR DECISION LAYERS ####################\n",
        "            self.fc1 = nn.Linear(7 * 7 * 32, 50)\n",
        "            self.fc2 = nn.Linear(50, 10)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            # Convolution -> Maxpool -> Batchnorm -> Relu\n",
        "            if self.print:    print(f'Input:              {list(x.shape)}')\n",
        "\n",
        "            # First encoder layer\n",
        "            x = F.leaky_relu(self.enc_conv1(x))\n",
        "            if self.print:  print(f'First Encoder layer:  {list(x.shape)}')\n",
        "\n",
        "            # Second encoder layer\n",
        "            x = F.leaky_relu(self.enc_conv2(x))\n",
        "            if self.print:  print(f'Second Encoder layer: {list(x.shape)}')\n",
        "\n",
        "            # Reshape for Linear layer\n",
        "            n_units = x.shape.numel() / x.shape[0] # Numbers in X / Number of data samples\n",
        "            x       = x.view(-1, int(n_units))     # Number of images / Number of units\n",
        "            if self.print: print(f'Vectorized: {list(x.shape)}')\n",
        "\n",
        "            # Linear layer\n",
        "            x = F.leaky_relu(self.fc1(x))\n",
        "            if self.print: print(f'First Linear layer:    {list(x.shape)}')\n",
        "            x = F.leaky_relu(self.fc2(x))\n",
        "            if self.print: print(f'Second Linear layer:   {list(x.shape)}')\n",
        "\n",
        "            return x\n",
        "    \n",
        "    # Create the model instance\n",
        "    net = cnn_net(print_toggle)\n",
        "\n",
        "    # Loss Function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    return net, loss_func, optimizer"
      ],
      "metadata": {
        "id": "mBDB1vkRqHak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czgev3p8mHNf"
      },
      "source": [
        "## TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlrvkL0cmRrE"
      },
      "source": [
        "### FFN_NONMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVfy8ahYmQYt"
      },
      "outputs": [],
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "X, y = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "# Values are log-prob of each number (0 - 9)\n",
        "print(torch.exp(y_hat))\n",
        "\n",
        "# Compute the loss\n",
        "loss = loss_func(y_hat, y)\n",
        "print(f'Loss: {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMCKudHX4Z7u"
      },
      "source": [
        "### FFN_BINARIZED MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN_OGB_04bN1"
      },
      "outputs": [],
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "X, y = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "# Confirm really binary\n",
        "torch.unique(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tiy_k-S54ZTV"
      },
      "source": [
        "### FFN_ BREADTH VS. DEPTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESnGDCps4bhc"
      },
      "outputs": [],
      "source": [
        "# Generate an instance of the model and confirm that it returns the expected network.\n",
        "n_units_per_layer = 12\n",
        "n_layers          = 4\n",
        "net = create_the_MNIST_net(n_units_per_layer, n_layers)\n",
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BM-N0Iz6iJK"
      },
      "source": [
        "### CNN_MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0DHnpMT6j_L"
      },
      "outputs": [],
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = create_the_MNIST_net(print_toggle=True)\n",
        "X, y  = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "# Check sizes of model outputs and target variable\n",
        "print('')\n",
        "print(y_hat.shape)\n",
        "print(y    .shape)\n",
        "\n",
        "# Now let's compute the loss\n",
        "loss = loss_func(y_hat, y)\n",
        "print('')\n",
        "print(f'Loss: {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFq_9Xq7fJws"
      },
      "source": [
        "### CNN_CLASSIFY_GAUSSIAN_BLURS|FIND_GAUSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1oyiCRafJws"
      },
      "outputs": [],
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "X, y = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "print(''), print(y_hat.shape)\n",
        "\n",
        "loss = loss_func(y_hat, y)\n",
        "print(''), print(f'Loss: {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziHqdFUFfJws"
      },
      "source": [
        "### CNN_GAUSS_FEATURE_MAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TSe5RWxfJws"
      },
      "outputs": [],
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "# Test that the model runs and can compute a loss\n",
        "X, y                          = iter(train_loader).next()\n",
        "y_hat, feat_map_1, feat_map_2 = net(X)\n",
        "loss                          = loss_func(y_hat, y)\n",
        "\n",
        "# Check sizes of outputs\n",
        "print(f'Predicted category:      {y_hat.shape}')\n",
        "print(f'Feature map after conv1: {feat_map_1.shape}')\n",
        "print(f'Feature map after conv2: {feat_map_2.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZvCitCkhvar"
      },
      "source": [
        "### CNN_GAUSS_AUTOENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJfxXVaQhvar"
      },
      "outputs": [],
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "y_hat = net(images[:10, :, :, :])\n",
        "\n",
        "# Check size of output\n",
        "print('')\n",
        "print(y_hat.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
        "ax[0].imshow(torch.squeeze(images[0, 0, :, :]).detach(), cmap='jet')\n",
        "ax[1].imshow(torch.squeeze(y_hat [0, 0, :, :]).detach(), cmap='jet')\n",
        "ax[0].set_title('Model input')\n",
        "ax[1].set_title('Model output')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UY1Cnophvar"
      },
      "source": [
        "### CNN_GAUSS_AE_OCCLUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8EwMHhahvas"
      },
      "outputs": [],
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "y_hat = net(images_no_occ[:10, :, :, :])\n",
        "\n",
        "# Check size of output\n",
        "print('')\n",
        "print(y_hat.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
        "ax[0].imshow(torch.squeeze(images_no_occ[0, 0, :, :]).detach(), cmap='jet')\n",
        "ax[1].imshow(torch.squeeze(y_hat        [0, 0, :, :]).detach(), cmap='jet')\n",
        "ax[0].set_title('Model input')\n",
        "ax[1].set_title('Model output')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN_EMNIST"
      ],
      "metadata": {
        "id": "a5ek-wy1sUfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_net(print_toggle=True)\n",
        "\n",
        "X, y = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "print(''), print(f'Output size: {y_hat.shape}')\n",
        "\n",
        "loss = loss_func(y_hat, torch.squeeze(y))\n",
        "print(''), print(f'Loss: {loss}')"
      ],
      "metadata": {
        "id": "pRsifWZ1sWyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN_ NUM_CHANS"
      ],
      "metadata": {
        "id": "sKFoVULMAIGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_net(num_chans=(6, 12))\n",
        "\n",
        "X, y = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "print(''), print(f'Output size: {y_hat.shape}')\n",
        "\n",
        "loss = loss_func(y_hat, torch.squeeze(y))\n",
        "print(''), print(f'Loss: {loss}')"
      ],
      "metadata": {
        "id": "ZSyB1uXQAKyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER_ PRETRAIN_FMNIST"
      ],
      "metadata": {
        "id": "17m-aSHQmlCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_AE_net(print_toggle=True)\n",
        "\n",
        "X, y = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "print(''), print(f'Output size: {y_hat.shape}')\n",
        "\n",
        "loss = loss_func(y_hat, X)\n",
        "print(''), print(f'Loss: {loss}')"
      ],
      "metadata": {
        "id": "QlhFhg1emlw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with one batch\n",
        "net, loss_func, optimizer = make_the_class_net(print_toggle=True)\n",
        "\n",
        "X, y = iter(train_loader).next()\n",
        "y_hat = net(X)\n",
        "\n",
        "print(''), print(f'Output size: {y_hat.shape}')\n",
        "\n",
        "loss = loss_func(y_hat, y)\n",
        "print(''), print(f'Loss: {loss}')"
      ],
      "metadata": {
        "id": "Zl_M4ddkreXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4uvLTnm3zb"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh_KVQY2jj5B"
      },
      "source": [
        "### META_PARAMS_MULTIOUTPUTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krox-_iZjj5C"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "  \"\"\"\n",
        "  META_PARAMS_MULTIOUTPUTS\n",
        "  \"\"\"\n",
        "  num_epochs = 100\n",
        "  \n",
        "  net, loss_func, optimizer = create_the_qwety_net()\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "    # Activate training mode\n",
        "    net.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    net.eval()                 # Activate testing mode\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      # pred_labels = torch.argmax(net(X), axis=1)\n",
        "      pred_labels = net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((torch.argmax(pred_labels, axis=1)) == y).float()))\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs7KUq_XGhE3"
      },
      "source": [
        "### META_PARAMS_OPTIMIZERS_QWERTY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iadPi0wsGh6p"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "\n",
        "def train_the_model(optimizer_type, learning_rate):\n",
        "  \"\"\"\n",
        "  META_PARAMS_OPTIMIZERS_QWERTY\n",
        "  \"\"\"\n",
        "  net, loss_func, optimizer = create_the_qwety_net(optimizer_type, learning_rate)\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "    # Activate training mode\n",
        "    net.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    net.eval()                 # Activate testing mode\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      y_hat = net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKVxc0xLhP-Q"
      },
      "source": [
        "### META_PARAM_RELUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caJKZse4hP-Q"
      },
      "outputs": [],
      "source": [
        "# Global parameter\n",
        "num_epochs = 600\n",
        "\n",
        "def train_the_model():\n",
        "  \"\"\"\n",
        "  META_PARAM_RELUS\n",
        "  \"\"\"\n",
        "\n",
        "  loss_func = nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.SGD(wine_net.parameters(), lr=0.01)\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "    # Activate training mode\n",
        "    wine_net.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = wine_net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    wine_net.eval()                 # Activate testing mode\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      # pred_labels = torch.argmax(wine_net(X), axis=1)\n",
        "      pred_labels = wine_net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((pred_labels > 0) == y).float()).item())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8hY7IRRrc3j"
      },
      "source": [
        "### FFN_NONMNIST, FFN_BINARIZED MNIST, CNN_MNIST, CNN_MNIST_SHIFTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsShD6pfnCNh"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    FFN_NONMNIST, FFN_BINARIZED MNIST, CNN_MNIST, CNN_MNIST_SHIFTED\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 60\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        net.train()\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        y_hat = net(X)\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuGWZ5I-nApG"
      },
      "source": [
        "### FFN_WEIGHTHISTOGRAMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvR27oFYroOs"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    FFN_WEIGHTHISTOGRAMS\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 100\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Initialize histogram variables\n",
        "    hist_x = np.zeros((num_epochs, 100))\n",
        "    hist_y = np.zeros((num_epochs, 100))\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "        # Get the weights distribution at the start of this epoch\n",
        "        hist_x, hist_y[epoch_i, :] = weights_histogram(net)\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad(): # Deactivates autograd\n",
        "          y_hat = net(X)\n",
        "\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net, hist_x, hist_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSf6JiI04qRt"
      },
      "source": [
        "### FFN_ BREADTH VS. DEPTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F24STX84qoL"
      },
      "outputs": [],
      "source": [
        "def train_the_model(n_units, n_layers):\n",
        "    \"\"\"\n",
        "    FFN_ BREADTH VS. DEPTH\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 60\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net(n_units, n_layers)\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad(): # Deactivates autograd\n",
        "          y_hat = net(X)\n",
        "          \n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AixifpPCHtpR"
      },
      "source": [
        "### FFN_OPTIMIZERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7h6aQXIHuhQ"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "\n",
        "def train_the_model(optimizer_type, learning_rate):\n",
        "  \"\"\"\n",
        "  FFN_OPTIMIZERS\n",
        "  \"\"\"\n",
        "  net, loss_func, optimizer = create_the_mnist_net(optimizer_type, learning_rate)\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    \n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      y_hat = net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcTi3bcoO_og"
      },
      "source": [
        "### FFN_SCRAMBLEDMNIST | DATA_ DATA_OVERSAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogmCl5GdO_ol"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    FFN_SCRAMBLEDMNIST | DATA_ DATA_OVERSAMPLING\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 50\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "          y_hat = net(X)\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_HitW73WTWS"
      },
      "source": [
        "### FFN_SHIFTEDMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddelaTQ5WTWS"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    FFN_SHIFTEDMNIST\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 50\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "          y_hat = net(X)\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S35Vzm6xcEN7"
      },
      "source": [
        "### FFN_NO7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vYpWI0ecEN-"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    FFN_NO7\n",
        "    NOTE: anything test-related can be deleted!\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 100\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq7n-453hP-U"
      },
      "source": [
        "### DATA_DATA_VS_DEPTH_QWERTY2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQLqJKiYhP-V"
      },
      "outputs": [],
      "source": [
        "def train_the_model(n_units, n_layers):\n",
        "    \"\"\"\n",
        "    DATA_DATA_VS_DEPTH_QWERTY2\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 50\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_qwerty_net(n_units, n_layers)\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_data:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_data)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "          y_hat = net(X)\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ByBIgLfhP-V"
      },
      "source": [
        "### DATA_ UNBALANCED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_frGQeshP-V"
      },
      "outputs": [],
      "source": [
        "# Global parameter\n",
        "num_epochs = 500\n",
        "\n",
        "def train_the_model():\n",
        "  \"\"\"\n",
        "  DATA_ UNBALANCED DATA\n",
        "  \"\"\"\n",
        "\n",
        "  loss_func = nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(wine_net.parameters(), lr=0.001)\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = wine_net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      pred_labels = wine_net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((pred_labels > 0) == y).float()).item())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oigen1iC4f1J"
      },
      "source": [
        "### DATA_NOISE_AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL8l52tc4f1J"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    DATA_NOISE_AUGMENTATION\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 50\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    dev_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(dev_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "          y_hat = net(X)\n",
        "        dev_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, dev_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPuEq-dwjj5N"
      },
      "source": [
        "### DATA_DATA_FEATURE_AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV3FUEzHjj5N"
      },
      "outputs": [],
      "source": [
        "def train_the_model(use_extra_feature=False):\n",
        "  \"\"\"\n",
        "  DATA_DATA_FEATURE_AUGMENTATION\n",
        "  \"\"\"\n",
        "  num_epochs = 200\n",
        "\n",
        "  net, loss_func, optimizer = create_the_qwety_net(use_extra_feature)\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      y_hat = net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2HvbCJljj5O"
      },
      "source": [
        "### DATA_SAVE_BEST_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88vYsMg4jj5O"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "  \"\"\"\n",
        "  DATA_SAVE_BEST_MODEL\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize a Dictionary for the best model\n",
        "  the_best_model = {'Accuracy': 0, 'net': None}\n",
        "\n",
        "  num_epochs = 100\n",
        "  \n",
        "  net, loss_func, optimizer = create_the_qwety_net()\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  dev_acc   = []\n",
        "  losses    = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "    # Activate training mode\n",
        "    net.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc  = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(dev_loader))   # Extract x,y from test dataloader\n",
        "    net.eval()                      # Activate testing mode\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      pred_labels = net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    dev_acc.append(100 * torch.mean(((torch.argmax(pred_labels, axis=1)) == y).float()))\n",
        "\n",
        "    # Store this model if it's the best so far\n",
        "    if (dev_acc[-1] > the_best_model['Accuracy']):\n",
        "      # New best accuracy\n",
        "      the_best_model['Accuracy'] = dev_acc[-1].item()\n",
        "\n",
        "      # Model's internal state\n",
        "      the_best_model['net']      = copy.deepcopy(net.state_dict())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, dev_acc, losses, the_best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8PFvpGOjj5O"
      },
      "source": [
        "### APRF_WINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by7uCVxrjj5Q"
      },
      "outputs": [],
      "source": [
        "# Global parameter\n",
        "num_epochs = 1000\n",
        "\n",
        "def train_the_model():\n",
        "  \"\"\"\n",
        "  APRF_WINE\n",
        "  \"\"\"\n",
        "  # Continuous value\n",
        "  loss_func = nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.SGD(wine_net.parameters(), lr=0.01)\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "\n",
        "  # initialize losses\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "    # Activate training mode\n",
        "    wine_net.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = wine_net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      pred_labels = wine_net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((pred_labels > 0) == y).float()).item())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjlwbhI-jj5R"
      },
      "source": [
        "### MODEL_PERFORM_APRF_MNIST, MODEL_PERFORM_MNIST_NO7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laerYJUojj5c"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    MODEL_PERFORM_APRF_MNIST | MODEL_PERFORM_MNIST_NO7\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqemNtzVhXSR"
      },
      "source": [
        "### MODEL_PERFORM_TIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_n-UJ-qhXSR"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    MODEL_PERFORM_TIME\n",
        "    \"\"\"\n",
        "    \n",
        "    # Start the timer!\n",
        "    time_in_function = time.process_time()\n",
        "\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "        # Report the epoch number, computation time, accuracy\n",
        "        comp_time = time.process_time() - time_in_function\n",
        "        print(f'Epoch {epoch_i + 1}/{num_epochs}, elapsed time: {comp_time:.2f} sec, test accuracy: {test_acc[-1]:.0f}')\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srGU6mOihXSS"
      },
      "source": [
        "### WEIGHTS_DEMO_INITS | WEIGHTS_VARIANCE_INITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESMkaTOmhXSS"
      },
      "outputs": [],
      "source": [
        "def train_the_model(net, loss_func, optimizer):\n",
        "  \"\"\"\n",
        "  WEIGHTS_DEMO_INITS | WEIGHTS_VARIANCE_INITS\n",
        "  \"\"\"\n",
        "  num_epochs = 10\n",
        "  \n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "    # Activate training mode\n",
        "    net.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    net.eval()                 # Activate testing mode\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      # pred_labels = torch.argmax(net(X), axis=1)\n",
        "      pred_labels = net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((torch.argmax(pred_labels, axis=1)) == y).float()))\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hq3yRVeGsCN"
      },
      "source": [
        "### WEIGHTS_ XAVIER_VS._KAIMING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps6oLKfPGsCN"
      },
      "outputs": [],
      "source": [
        "# Global parameter\n",
        "num_epochs = 600\n",
        "\n",
        "def train_the_model(wine_net):\n",
        "  \"\"\"\n",
        "  WEIGHTS_ XAVIER_VS._KAIMING\n",
        "  \"\"\"\n",
        "\n",
        "  loss_func = nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.SGD(wine_net.parameters(), lr=0.01)\n",
        "\n",
        "  # Initialize accuracies as empties\n",
        "  train_acc = []\n",
        "  test_acc  = []\n",
        "  losses = torch.zeros(num_epochs)\n",
        "\n",
        "  # Loop over epochs\n",
        "  for epoch_i in range(num_epochs):\n",
        "    # Activate training mode\n",
        "    wine_net.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in train_loader:\n",
        "\n",
        "      # Forward pass and loss\n",
        "      y_hat = wine_net(X)\n",
        "      loss  = loss_func(y_hat, y)\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute training accuracy just for this batch\n",
        "      batch_acc.append(100 * torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "      # Loss from this batch\n",
        "      batch_loss.append(loss.item())\n",
        "\n",
        "    # End of batch loop...\n",
        "\n",
        "    # Now that we've trained through the batches, get their average training accuracy \n",
        "    train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "    # Get average losses across the batches\n",
        "    losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "    # Test accuracy (NOTE: testing in batches!)    \n",
        "    X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "    wine_net.eval()                 # Activate testing mode\n",
        "    with torch.no_grad():           # Deactivates autograd\n",
        "      # pred_labels = torch.argmax(wine_net(X), axis=1)\n",
        "      pred_labels = wine_net(X)\n",
        "\n",
        "    # Compute accuracy\n",
        "    test_acc.append(100 * torch.mean(((pred_labels > 0) == y).float()).item())\n",
        "\n",
        "  # Function output\n",
        "  return train_acc, test_acc, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqykubnsGsCN"
      },
      "source": [
        "### WEIGHTS_FREEZE_WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK241EYCGsCO"
      },
      "outputs": [],
      "source": [
        "def train_the_model(net, loss_func, optimizer):\n",
        "    \"\"\"\n",
        "    WEIGHTS_FREEZE_WEIGHTS\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 100\n",
        "\n",
        "    # Initialize accuracies as empties\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "    losses = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Switch off learning in all-but-output layers during first 1/2 of training\n",
        "        if (epoch_i < (num_epochs / 2)):\n",
        "            for p in net.named_parameters():\n",
        "                if ('output' not in p[0]):\n",
        "                    p[1].requires_grad = False\n",
        "        else:\n",
        "            for p in net.named_parameters():\n",
        "                p[1].requires_grad = True\n",
        "        \n",
        "        # Activate training mode\n",
        "        net.train()\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute training accuracy just for this batch\n",
        "            batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "        # End of batch loop...\n",
        "\n",
        "        # Now that we've trained through the batches, get their average training accuracy \n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # Get average losses across the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy (NOTE: testing in batches!)    \n",
        "        X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "        net.eval()                 # Activate testing mode\n",
        "        with torch.no_grad():           # Deactivates autograd\n",
        "            # pred_labels = torch.argmax(net(X), axis=1)\n",
        "            pred_labels = net(X)\n",
        "\n",
        "        # Compute accuracy\n",
        "        test_acc.append(100 * torch.mean(((torch.argmax(pred_labels, axis=1)) == y).float()))\n",
        "\n",
        "    # Function output\n",
        "    return train_acc, test_acc, losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IZ3-gLXRQYn"
      },
      "source": [
        "### WEIGHTS_WEIGHTS_CHANGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nth82RfRQYn"
      },
      "outputs": [],
      "source": [
        "def train_the_model(net, loss_func, optimizer):\n",
        "    \"\"\"\n",
        "    WEIGHTS_WEIGHTS_CHANGES\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 60\n",
        "\n",
        "    # Initialize accuracies as empties\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "    losses = torch.zeros(num_epochs)\n",
        "\n",
        "    # Initialize Weight change matrices\n",
        "    # 4: 4 layers\n",
        "    weight_change = np.zeros((num_epochs, 4))\n",
        "    weight_conds  = np.zeros((num_epochs, 4))\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Store the weights for each layer\n",
        "        pre_W = []\n",
        "        for p in net.named_parameters():\n",
        "            if ('weight' in p[0]):\n",
        "                pre_W.append(copy.deepcopy(p[1].data.numpy()))\n",
        "        \n",
        "        # Activate training mode\n",
        "        net.train()\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc = []\n",
        "        batch_loss = []\n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute training accuracy just for this batch\n",
        "            batch_acc.append(100 * torch.mean(((torch.argmax(y_hat, axis=1)) == y).float()).item())\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "        # End of batch loop...\n",
        "\n",
        "        # Now that we've trained through the batches, get their average training accuracy \n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # Get average losses across the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy (NOTE: testing in batches!)    \n",
        "        X, y = next(iter(test_loader))  # Extract x,y from test dataloader\n",
        "        net.eval()                 # Activate testing mode\n",
        "        with torch.no_grad():           # Deactivates autograd\n",
        "            # pred_labels = torch.argmax(net(X), axis=1)\n",
        "            pred_labels = net(X)\n",
        "\n",
        "        # Compute accuracy\n",
        "        test_acc.append(100 * torch.mean(((torch.argmax(pred_labels, axis=1)) == y).float()))\n",
        "\n",
        "        # Finally, get the post-learning state of the weights\n",
        "        for (i, p) in enumerate(net.named_parameters()):\n",
        "          # int(i / 2): Only look for the weight\n",
        "            if ('weight' in p[0]):\n",
        "                # Condition number\n",
        "                weight_conds[epoch_i, int(i / 2)] = np.linalg.cond(p[1].data)\n",
        "\n",
        "                # Frobenius Norm of the weight change from pre-learning\n",
        "                weight_change[epoch_i, int(i / 2)] = np.linalg.norm(pre_W[int(i / 2)] - p[1].data.numpy(), ord='fro')\n",
        "    # End epochs\n",
        "    # Function output\n",
        "    return train_acc, test_acc, losses, net, weight_change, weight_conds, pre_W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHQEChkSio0V"
      },
      "source": [
        "### AUTOENCODER_DENOISING_MNIST|LATENT_CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvMIDlDVio0V"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    AUTOENCODER_DENOISING_MNIST|LATENT_CODE\n",
        "    \"\"\"\n",
        "\n",
        "    num_epochs = 10000\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_AE()\n",
        "\n",
        "    # Initialize losses\n",
        "    losses = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Select a random set of images\n",
        "        random_idx = np.random.choice(data_tensor.shape[0], size=32)\n",
        "        X          = data_tensor[random_idx, :]\n",
        "\n",
        "        # Forward pass and loss\n",
        "        y_hat = net(X)\n",
        "        loss  = loss_func(y_hat, X)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Losses in this epoch\n",
        "        losses[epoch_i] = loss.item()\n",
        "    \n",
        "    # End epochs\n",
        "    # Function output\n",
        "    return losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuQ6nng3J1wH"
      },
      "source": [
        "### AUTOENCODER_HOW_MANY_UNIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "holS64_wJ1wI"
      },
      "outputs": [],
      "source": [
        "def train_the_model(n_enc, n_bottle):\n",
        "    \"\"\"\n",
        "    AUTOENCODER_HOW_MANY_UNIT\n",
        "    \"\"\"\n",
        "\n",
        "    num_epochs = 3\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_AE(n_enc, n_bottle)\n",
        "\n",
        "    # Initialize losses\n",
        "    losses = []\n",
        "\n",
        "    # Batch size and number of batches\n",
        "    batch_size = 32\n",
        "    num_batch  = int(data_tensor.shape[0] / batch_size)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Get a permuted index vector\n",
        "        rand_idx = np.random.permutation(data_tensor.shape[0]).astype(int)\n",
        "\n",
        "        # Lossed during batches\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch_i in range(num_batch):\n",
        "\n",
        "            # Samples to use in this batch\n",
        "            samps_2_use = range((batch_i - 1) * batch_size, batch_i * batch_size)\n",
        "\n",
        "            # Select those images\n",
        "            X = data_tensor[rand_idx[samps_2_use], :]\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, X)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Losses in this batch\n",
        "            batch_losses.append(loss.item())\n",
        "        # End minibatch loop\n",
        "\n",
        "        losses.append(np.mean(batch_losses[-3:]))\n",
        "    \n",
        "    # End epochs\n",
        "    # Function output\n",
        "    return losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV3B_KzlVBYB"
      },
      "source": [
        "### AUTO_ENCODER_OCCLUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5YDPOGeVBYB"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    AUTO_ENCODER_OCCLUSION\n",
        "    \"\"\"\n",
        "\n",
        "    num_epochs = 5\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Initialize losses\n",
        "    losses = []\n",
        "\n",
        "    # Batch size and number of batches\n",
        "    batch_size = 32\n",
        "    num_batch  = int(data_tensor.shape[0] / batch_size)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Get a permuted index vector\n",
        "        rand_idx = np.random.permutation(data_tensor.shape[0]).astype(int)\n",
        "\n",
        "        # Lossed during batches\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch_i in range(num_batch):\n",
        "\n",
        "            # Samples to use in this batch\n",
        "            samps_2_use = range((batch_i - 1) * batch_size, batch_i * batch_size)\n",
        "\n",
        "            # Select those images\n",
        "            X = data_tensor[rand_idx[samps_2_use], :]\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, X)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Losses in this batch\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # End minibatch loop\n",
        "    \n",
        "    # End epochs\n",
        "    \n",
        "    # Function output\n",
        "    return losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_DQMeB0fJw5"
      },
      "source": [
        "### CNN_CLASSIFY_GAUSSIAN_BLURS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L98FnO3HfJw5"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    CNN_CLASSIFY_GAUSSIAN_BLURS\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "    train_acc  = torch.zeros(num_epochs)\n",
        "    test_acc   = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc .append(torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_acc [epoch_i] = 100 * np.mean(batch_acc)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "        test_acc [epoch_i] = 100 * torch.mean(((y_hat > 0) == y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, train_acc, test_acc, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a13JTQofJw6"
      },
      "source": [
        "### CNN_GAUSS_FEATURE_MAPS|SOFTCODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aac4x0QVfJw6"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    CNN_GAUSS_FEATURE_MAPS|SOFTCODE\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "    train_acc  = torch.zeros(num_epochs)\n",
        "    test_acc   = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)[0] # Only need the first output\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc .append(torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_acc [epoch_i] = 100 * np.mean(batch_acc)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)[0]\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "        test_acc [epoch_i] = 100 * torch.mean(((y_hat > 0) == y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, train_acc, test_acc, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbvxZ6ofmX4c"
      },
      "source": [
        "### CNN_LINEAR_UNITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWjmQ-ANmX4c"
      },
      "outputs": [],
      "source": [
        "def train_the_model(fc_units):\n",
        "    \"\"\"\n",
        "    CNN_LINEAR_UNITS\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net(fc_units)\n",
        "\n",
        "    # Send the model to the GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "    train_acc  = torch.zeros(num_epochs)\n",
        "    test_acc   = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Push data to GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc .append(torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_acc [epoch_i] = 100 * np.mean(batch_acc)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        \n",
        "        # Push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "        test_acc [epoch_i] = 100 * torch.mean(((y_hat > 0) == y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, train_acc, test_acc, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DITvYIoQhva1"
      },
      "source": [
        "### CNN_CLASSIFY_GAUSSIAN_BLURS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OltKoMIhva2"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    CNN_CLASSIFY_GAUSSIAN_BLURS\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "    train_acc  = torch.zeros(num_epochs)\n",
        "    test_acc   = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc .append(torch.mean(((y_hat > 0) == y).float()).item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_acc [epoch_i] = 100 * np.mean(batch_acc)\n",
        "\n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "        test_acc [epoch_i] = 100 * torch.mean(((y_hat > 0) == y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, train_acc, test_acc, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "088iXGJuhva2"
      },
      "source": [
        "### CNN_GAUSS_AE_OCCLUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3a0e7sVhva2"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    CNN_GAUSS_AE_OCCLUSION\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 1000\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses = torch.zeros(num_epochs)\n",
        "    \n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Pick a set of images at random\n",
        "        pics_2_use = np.random.choice(n_gauss, size=32, replace=False)\n",
        "        \n",
        "        # Get the input (has occlusions) and the target (no occlusions)\n",
        "        X = images_occ   [pics_2_use, :, :, :]\n",
        "        Y = images_no_occ[pics_2_use, :, :, :]\n",
        "        \n",
        "        # Forward pass and loss\n",
        "        y_hat           = net(X)\n",
        "        loss            = loss_func(y_hat, Y)\n",
        "        losses[epoch_i] = loss.item()\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # End epochs\n",
        "    return losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSBF3Seehva2"
      },
      "source": [
        "### CNN_GAUSS_AUTOENCODER|CUSTOM_LOSS_FUNC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf9PGRq6hva2"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    CNN_GAUSS_AUTOENCODER|CUSTOM_LOSS_FUNC\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 1000\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    losses = torch.zeros(num_epochs)\n",
        "    \n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Pick a set of images at random\n",
        "        pics_2_use = np.random.choice(n_gauss, size=32, replace=False)\n",
        "        X          = images[pics_2_use, :, :, :]\n",
        "        \n",
        "        # Forward pass and loss\n",
        "        y_hat           = net(X)\n",
        "        loss            = loss_func(y_hat, X)\n",
        "        losses[epoch_i] = loss.item()\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # End epochs\n",
        "    return losses, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZOlOpnVhva3"
      },
      "source": [
        "### CNN_FIND_GAUSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ooGLmgthva3"
      },
      "outputs": [],
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    CNN_FIND_GAUSS\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 30\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        \n",
        "        # Test accuracy\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, net"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN_EMNIST|HOW_LOW"
      ],
      "metadata": {
        "id": "Ck7leN27s3i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_the_model():\n",
        "    \"\"\"\n",
        "    CNN_EMNIST|HOW_LOW\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "    train_err  = torch.zeros(num_epochs)\n",
        "    test_err   = torch.zeros(num_epochs)\n",
        "\n",
        "    # Send the model to the GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "        net.train()\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_err  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            # Push data to GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and Error from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_err .append(torch.mean((torch.argmax(y_hat, axis=1) != y).float()).item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_err [epoch_i] = 100 * np.mean(batch_err)\n",
        "\n",
        "        # Test accuracy\n",
        "        net.eval()\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        \n",
        "        # Push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "        test_err     [epoch_i] = 100 * torch.mean((torch.argmax(y_hat, axis=1) != y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, train_err, test_err, net"
      ],
      "metadata": {
        "id": "-IQmx0ots6_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN_ NUM_CHANS"
      ],
      "metadata": {
        "id": "xXblGhNHAwLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_the_model(num_chans):\n",
        "    \"\"\"\n",
        "    CNN_ NUM_CHANS\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 5\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net(num_chans)\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "    train_err  = torch.zeros(num_epochs)\n",
        "    test_err   = torch.zeros(num_epochs)\n",
        "\n",
        "    # Send the model to the GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "        net.train()\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_err  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            # Push data to GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and Error from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_err .append(torch.mean((torch.argmax(y_hat, axis=1) != y).float()).item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_err [epoch_i] = 100 * np.mean(batch_err)\n",
        "\n",
        "        # Test accuracy\n",
        "        net.eval()\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        \n",
        "        # Push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "        test_err     [epoch_i] = 100 * torch.mean((torch.argmax(y_hat, axis=1) != y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, train_err, test_err, net"
      ],
      "metadata": {
        "id": "PTO8esTtAwl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER_ FMNIST"
      ],
      "metadata": {
        "id": "qFk89rmqqDql"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IblJo1NCp0kl"
      },
      "source": [
        "def train_the_model(net, train_loader, test_loader, num_epochs=10):\n",
        "    \"\"\"\n",
        "    TRANSFER_ FMNIST\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize\n",
        "    losses    = torch.zeros(num_epochs)\n",
        "    train_acc = []\n",
        "    test_acc  = []\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "\n",
        "        # Loop over training data batches\n",
        "        net.train()\n",
        "        batch_acc  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat,y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # Compute accuracy\n",
        "            matches = torch.argmax(y_hat, axis=1) == y       # Booleans (True/False)\n",
        "            matches_numeric = matches.float()                # Convert to numbers (1/0)\n",
        "            accuracy_pct = 100 * torch.mean(matches_numeric) # Average and *100\n",
        "            batch_acc.append(accuracy_pct)                   # Add to list of accuracies\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_acc.append(np.mean(batch_acc))\n",
        "\n",
        "        # The average losses accross the batches\n",
        "        losses[epoch_i] = np.mean(batch_loss)\n",
        "\n",
        "        # Test accuracy\n",
        "        net.eval()\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "        test_acc.append(100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()))\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_acc, test_acc, losses, net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER_LETTER2NUMBER"
      ],
      "metadata": {
        "id": "PMULjK6xzswP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRYhuhdtzzdi"
      },
      "source": [
        "def train_the_model(net, optimizer, train_loader, test_loader, num_epochs=10):\n",
        "    \"\"\"\n",
        "    TRANSFER_LETTER2NUMBER\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Create a new model\n",
        "    net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    test_loss  = torch.zeros(num_epochs)\n",
        "    train_err  = torch.zeros(num_epochs)\n",
        "    test_err   = torch.zeros(num_epochs)\n",
        "\n",
        "    # Send the model to the GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "        net.train()\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_err  = []\n",
        "        batch_loss = [] \n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            # Push data to GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and Error from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_err .append(torch.mean((torch.argmax(y_hat, axis=1) != y).float()).item())\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_err [epoch_i] = 100 * np.mean(batch_err)\n",
        "\n",
        "        # Test accuracy\n",
        "        net.eval()\n",
        "        X, y = next(iter(test_loader)) # Extract X, y from dataloader\n",
        "        \n",
        "        # Push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        test_loss[epoch_i] = loss.item()\n",
        "        test_err     [epoch_i] = 100 * torch.mean((torch.argmax(y_hat, axis=1) != y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, test_loss, train_err, test_err, net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER_ RESNET"
      ],
      "metadata": {
        "id": "Z3YpwAEQauQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# Initialize\n",
        "train_loss = torch.zeros(num_epochs)\n",
        "test_loss  = torch.zeros(num_epochs)\n",
        "train_acc  = torch.zeros(num_epochs)\n",
        "test_acc   = torch.zeros(num_epochs)\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch_i in range(num_epochs):\n",
        "    \n",
        "    resnet.train()\n",
        "\n",
        "    # Loop over training data batches\n",
        "    batch_acc  = []\n",
        "    batch_loss = [] \n",
        "\n",
        "    for X, y in train_loader:\n",
        "        # Push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Forward pass and loss\n",
        "        y_hat = resnet(X)\n",
        "        loss  = loss_func(y_hat,y)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss from this batch\n",
        "        batch_loss.append(loss.item())\n",
        "        batch_acc .append(torch.mean((torch.argmax(y_hat,axis=1) == y).float()).item())\n",
        "\n",
        "    # End of batch loop.\n",
        "\n",
        "    # Get the average training accuracy of the batches\n",
        "    train_loss[epoch_i] = np.mean(batch_loss)\n",
        "    train_acc [epoch_i] = 100 * np.mean(batch_acc)\n",
        "\n",
        "    # Test accuracy\n",
        "    resnet.eval()\n",
        "    batch_acc  = []\n",
        "    batch_loss = []\n",
        "\n",
        "    for X, y in test_loader:\n",
        "        \n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = resnet(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "        \n",
        "        batch_loss.append(loss.item())\n",
        "        batch_acc .append(torch.mean((torch.argmax(y_hat,axis=1) == y).float()).item())\n",
        "\n",
        "    test_loss[epoch_i] = np.mean(batch_loss)\n",
        "    test_acc [epoch_i] = 100 * np.mean(torch.mean((torch.argmax(y_hat,axis=1) == y).float()).item())\n",
        "\n",
        "    print(f'Finished epoch {epoch_i + 1} / {num_epochs}. Test accuracy = {test_acc[epoch_i]:.2f}%')"
      ],
      "metadata": {
        "id": "8vzHPBt_auyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER_ PRETRAIN_FMNIST"
      ],
      "metadata": {
        "id": "qrVKwDj9nDls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_the_class_model(net, loss_func, optimizer):\n",
        "    \"\"\"\n",
        "    TRANSFER_ PRETRAIN_FMNIST\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    dev_loss   = torch.zeros(num_epochs)\n",
        "    train_acc  = torch.zeros(num_epochs)\n",
        "    dev_acc    = torch.zeros(num_epochs)\n",
        "\n",
        "    # Send the model to the GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "        net.train()\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_loss = [] \n",
        "        batch_acc  = []\n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            # Push data to GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and Error from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc.append(torch.mean((torch.argmax(y_hat, axis=1) == y).float()).item())\n",
        "\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_acc [epoch_i] = 100 * np.mean(batch_acc)\n",
        "\n",
        "        # Test accuracy\n",
        "        net.eval()\n",
        "        X, y = next(iter(dev_loader)) # Extract X, y from dataloader\n",
        "        \n",
        "        # Push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        dev_loss[epoch_i] = loss.item()\n",
        "        dev_acc [epoch_i] = 100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float().item())\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, dev_loss, train_acc, dev_acc, net"
      ],
      "metadata": {
        "id": "nJyTVOJNnEjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_the_class_model(net, loss_func, optimizer):\n",
        "    \"\"\"\n",
        "    TRANSFER_ PRETRAIN_FMNIST\n",
        "    \"\"\"\n",
        "    \n",
        "    num_epochs = 10\n",
        "\n",
        "    # Initialize\n",
        "    train_loss = torch.zeros(num_epochs)\n",
        "    dev_loss   = torch.zeros(num_epochs)\n",
        "    train_acc  = torch.zeros(num_epochs)\n",
        "    dev_acc    = torch.zeros(num_epochs)\n",
        "\n",
        "    # Send the model to the GPU\n",
        "    net.to(device)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch_i in range(num_epochs):\n",
        "        net.train()\n",
        "\n",
        "        # Loop over training data batches\n",
        "        batch_loss = [] \n",
        "        batch_acc  = []\n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            # Push data to GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass and loss\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Loss and Error from this batch\n",
        "            batch_loss.append(loss.item())\n",
        "            batch_acc.append(torch.mean((torch.argmax(y_hat, axis=1) == y).float()).item())\n",
        "\n",
        "\n",
        "        # End of batch loop.\n",
        "\n",
        "        # Get the average training accuracy of the batches\n",
        "        train_loss[epoch_i] = np.mean(batch_loss)\n",
        "        train_acc [epoch_i] = 100 * np.mean(batch_acc)\n",
        "\n",
        "        # Test accuracy\n",
        "        net.eval()\n",
        "        X, y = next(iter(dev_loader)) # Extract X, y from dataloader\n",
        "        \n",
        "        # Push data to GPU\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = net(X)\n",
        "            loss  = loss_func(y_hat, y)\n",
        "            \n",
        "        dev_loss[epoch_i] = loss.item()\n",
        "        dev_acc [epoch_i] = 100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()).item()\n",
        "\n",
        "    # End epochs\n",
        "\n",
        "    return train_loss, dev_loss, train_acc, dev_acc, net"
      ],
      "metadata": {
        "id": "0GakP36WsDgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct6XKOhtg6wM"
      },
      "source": [
        "## BUILD AND TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhonNlIEg6wM"
      },
      "source": [
        "### REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81lFh6ymg6wM"
      },
      "outputs": [],
      "source": [
        "def build_and_train_the_model(x, y):\n",
        "  \"\"\"\n",
        "  Regression slopes\n",
        "  \"\"\"\n",
        "  # Build the model\n",
        "  ANN_reg = nn.Sequential(\n",
        "    nn.Linear(in_features=1, out_features=1), # Input layer\n",
        "    nn.ReLU(),                                # Activation Function\n",
        "    nn.Linear(in_features=1, out_features=1), # Output layer\n",
        "  )\n",
        "\n",
        "  # Loss and optimizer functions\n",
        "  loss_func = nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(params=ANN_reg.parameters(), lr=.05)\n",
        "\n",
        "  ## Train the model\n",
        "  num_epochs  = 500\n",
        "  losses      = torch.zeros(num_epochs)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_hat = ANN_reg(x)\n",
        "    # Compute loss\n",
        "    loss          = loss_func(y_hat, y)\n",
        "    losses[epoch] = loss\n",
        "    # Backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  ## End training loop\n",
        "\n",
        "  ## Compute model predictions\n",
        "  predictions = ANN_reg(x)\n",
        "\n",
        "  # output: \n",
        "  return predictions, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1BK_Wo0jj5i"
      },
      "source": [
        "## SAVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmP0_Ovjj5i"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "# `net`: The trained model\n",
        "# `state_dict()`: Dictionary of the current state of the model, every thing about the model\n",
        "# `'trained_model.pt'`: File name\n",
        "\n",
        "torch.save(net.state_dict(),'trained_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSOrN_3Kg6wN"
      },
      "source": [
        "# EXPERIMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew2ma2nUg6wN"
      },
      "source": [
        "## REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThsLAGjXg6wO"
      },
      "outputs": [],
      "source": [
        "num_epochs  = 500\n",
        "losses      = torch.zeros(num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Forward pass -> Output (prediction) of the model\n",
        "  y_hat = ANN_reg(x)\n",
        "  # Compute loss\n",
        "  loss          = loss_func(y_hat, y)\n",
        "  losses[epoch] = loss\n",
        "  # Backprop\n",
        "  optimizer.zero_grad() # Set all the derivative in the model to be zero\n",
        "  loss.backward()       # Implement back prop based on the loss computed\n",
        "  optimizer.step() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL6xMQVcg6wO"
      },
      "outputs": [],
      "source": [
        "# Show the losses\n",
        "\n",
        "# Manually compute losses\n",
        "# Final forward pass\n",
        "predictions = ANN_reg(x)\n",
        "\n",
        "# Final loss (MSE)\n",
        "test_loss = (predictions - y).pow(2).mean()\n",
        "\n",
        "# `detach()`: detach the number from all other information\n",
        "plt.plot(losses.detach(), 'o', markerfacecolor='w', linewidth=0.1)\n",
        "plt.plot(num_epochs, test_loss.detach(), 'ro')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Final Loss = {np.round(test_loss.item(), 3)}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvH1l0ZEg6wP"
      },
      "source": [
        "## REGRESSION SLOPES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mnSFsxfg6wP"
      },
      "outputs": [],
      "source": [
        "# m: Slope\n",
        "def create_the_data(m):\n",
        "  N = 50\n",
        "  x = torch.randn(N, 1)\n",
        "  y = m * x + torch.randn(N, 1) / 2\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw92_D9Pg6wP"
      },
      "outputs": [],
      "source": [
        "# Create a dataset\n",
        "x, y = create_the_data(m=.8)\n",
        "\n",
        "# Run the model\n",
        "y_hat, losses = build_and_train_the_model(x, y)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
        "\n",
        "ax[0].plot(losses.detach(), 'o', markerfacecolor='w', linewidth=.1)\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_title('Loss')\n",
        "ax[0].grid()\n",
        "\n",
        "ax[1].plot(x, y, 'bo', label='Real Data')\n",
        "ax[1].plot(x, y_hat.detach(), 'rs', label='Predictions')\n",
        "ax[1].set_xlabel('x')\n",
        "ax[1].set_ylabel('y')\n",
        "ax[1].set_title(f'Prediction-data corrr = {np.corrcoef(y.T, y_hat.detach().T)[0, 1]:.2f}')\n",
        "ax[1].legend()\n",
        "ax[1].grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBoGmsishP-Y"
      },
      "source": [
        "## META_PARAMS_RELUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL7EqhYjGp51"
      },
      "outputs": [],
      "source": [
        "# List activation functions to test\n",
        "activation_funcs = ['ReLU', 'ReLU6', 'LeakyReLU']\n",
        "\n",
        "train_acc_by_act = np.zeros(shape=(num_epochs, len(activation_funcs)))\n",
        "test_acc_by_act  = np.zeros(shape=(num_epochs, len(activation_funcs)))\n",
        "\n",
        "for i, act_func_i in enumerate(activation_funcs):\n",
        "  # Create a model and train it\n",
        "  wine_net = ANN_wine(act_func=act_func_i)\n",
        "  train_acc_by_act[:, i], test_acc_by_act[:, i], losses = train_the_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJcsae-3hP-Y"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1 ,2, figsize=(20,7))\n",
        "\n",
        "ax[0].plot(train_acc_by_act)\n",
        "ax[0].set_title('Train accuracy')\n",
        "ax[1].plot(test_acc_by_act)\n",
        "ax[1].set_title('Test accuracy')\n",
        "\n",
        "# Common features\n",
        "for i in range(2):\n",
        "  ax[i].legend(activation_funcs)\n",
        "  ax[i].set_xlabel('Epoch')\n",
        "  ax[i].set_ylabel('Accuracy (%)')\n",
        "  ax[i].set_ylim([50, 100])\n",
        "  ax[i].grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4nZlIL-jj5o"
      },
      "source": [
        "### META_PARAMS_MULTIOUTPUTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFRGEWg8jj5o"
      },
      "outputs": [],
      "source": [
        "# Run the data through the model to get the categorical predictions\n",
        "y_hat       = net(data)\n",
        "predictions = torch.argmax(y_hat, axis=1)\n",
        "\n",
        "# And plot those against the real data\n",
        "plt.figure(figsize=(18, 7))\n",
        "plt.plot(predictions, 'o', label='Predicted values', alpha=0.4)\n",
        "plt.plot(labels + 0.2, 's', label='True value', alpha=0.4)\n",
        "plt.xlabel('Qwerty number')\n",
        "plt.ylabel('Category')\n",
        "plt.yticks([0, 1, 2])\n",
        "plt.ylabel([-1, 3])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2bZ32Tgjj5o"
      },
      "outputs": [],
      "source": [
        "# Recode each prediction by accuracy\n",
        "accuracy = (predictions == labels).float()\n",
        "\n",
        "# Compute overall accuracy\n",
        "total_acc = torch.mean(100 * accuracy).item()\n",
        "\n",
        "# And average by group\n",
        "accuracy_by_group = np.zeros(3)\n",
        "\n",
        "for i in range(3):\n",
        "  accuracy_by_group[i] = 100 * torch.mean(accuracy[labels == i])\n",
        "\n",
        "plt.bar(range(3), accuracy_by_group)\n",
        "plt.ylim([80, 100])\n",
        "plt.xticks([0, 1, 2])\n",
        "plt.xlabel('Group')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title(f'Final Accuracy = {total_acc:.2f}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDPkCnOnjj5p"
      },
      "outputs": [],
      "source": [
        "# Finally, show the data in their native space, labeled by accuracy\n",
        "number_classes = 3\n",
        "# Data markers\n",
        "colors_shapes = ['bs', 'ko', 'g^']\n",
        "\n",
        "# Show the data\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "\n",
        "# Plot correct and incorrect labeled data\n",
        "for i in range(number_classes):\n",
        "    # Plot all data points for this label\n",
        "    plt.plot(data[labels == i, 0], data[labels == i, 1], colors_shapes[i], alpha=0.3, label=f'Group {i}')\n",
        "    \n",
        "    # Cross out the incorrect ones\n",
        "    idx_error = (accuracy == 0) & (labels == i)\n",
        "    plt.plot(data[idx_error, 0], data[idx_error, 1], 'rx')\n",
        "\n",
        "plt.title(f'The qwerties! ({total_acc:.2f}% accurately labeled)')\n",
        "plt.xlabel('qwerty dimension 1')\n",
        "plt.ylabel('qwerty dimension 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elYl4Ks5Golg"
      },
      "source": [
        "## META_PARAMS_OPTIMIZERS_QWERTY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUiMogQlGo_Q"
      },
      "outputs": [],
      "source": [
        "# Variables to loop over\n",
        "learning_rates  = np.logspace(start=np.log10(0.0001), stop=np.log10(0.1), num=20)\n",
        "optimizer_types = ['SGD', 'RMSprop', 'Adam']\n",
        "\n",
        "# Initialize performance matrix\n",
        "final_performance = np.zeros(shape=(len(learning_rates), len(optimizer_types)))\n",
        "\n",
        "# Now for the experiment!\n",
        "for idx_optimizer, optimizer_i in enumerate(optimizer_types):\n",
        "  for idx_l_rate, l_rate_i in enumerate(learning_rates):\n",
        "    train_acc, test_acc, losses, net = train_the_model(optimizer_type=optimizer_i, learning_rate=l_rate_i)\n",
        "    final_performance[idx_l_rate, idx_optimizer] = np.mean(test_acc[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cml0ZHTnGqaS"
      },
      "outputs": [],
      "source": [
        "# Plot the results! \n",
        "plt.plot(learning_rates, final_performance, 'o-', linewidth=2)\n",
        "plt.legend(optimizer_types)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rates')\n",
        "plt.ylabel('Test accuracy (ave. last 10 epochs)')\n",
        "plt.title('Comparison of optimizers by learning rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NFS56B2o-a7"
      },
      "source": [
        "## FFN_DISTRIBUTIONS OF WEIGHTS PRE- AND POST-LEARNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR0rrRUKpBOU"
      },
      "outputs": [],
      "source": [
        "# EXPLORING THE \"INNARDS\" OF THE MODEL\n",
        "# Create a temp model to explore\n",
        "net = create_the_MNIST_net()[0]\n",
        "\n",
        "# Summary of the entire model\n",
        "print('Summary of model: ')\n",
        "print(net, '\\n')\n",
        "\n",
        "# Explore one of the layers\n",
        "print('Summary of input layer: ')\n",
        "print(vars(net.input), '\\n')\n",
        "\n",
        "# Check out the matrix of weights\n",
        "print('Input layer weights: ')\n",
        "print(net.input.weight.shape)\n",
        "print(net.input.weight, '\\n')\n",
        "\n",
        "# Finally, extract the weights and make a histogram\n",
        "w = net.input.weight.detach().flatten()\n",
        "plt.hist(w, 40)\n",
        "plt.xlabel('Weight value')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of initialized input-layer weights')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScEv8kGEqQbC"
      },
      "outputs": [],
      "source": [
        "# FUNCTION RETURNING A HISTOGRAM OF ALL WEIGHTS (ACROSS ALL LAYERS)\n",
        "def weights_histogram(net):\n",
        "\n",
        "  # Initialize weight vector\n",
        "  W = np.array([])\n",
        "\n",
        "  # Concatenate each set of weights into 1 vector\n",
        "  # Get weights of all the layers\n",
        "  for layer in net.parameters():\n",
        "    W = np.concatenate((W, layer.detach().flatten().numpy()))\n",
        "\n",
        "  # Compute theri histogram (Range is hard-coded)\n",
        "  # 100 bin between -0.8 & 0.8\n",
        "  hist_y, hist_x = np.histogram(W, bins=np.linspace(-0.8, 0.8, 101), density=True)\n",
        "  hist_x = (hist_x[1:] + hist_x[:-1]) / 2\n",
        "\n",
        "  return hist_x, hist_y\n",
        "\n",
        "# Test\n",
        "hist_x, hist_y = weights_histogram(net)\n",
        "plt.plot(hist_x, hist_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHSni3uosnkJ"
      },
      "outputs": [],
      "source": [
        "# SHOW THE HISTOGRAM OF THE WEIGHTS\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "for i in range(hist_y.shape[0]):\n",
        "  ax[0].plot(hist_x, hist_y[i, :], color=[1 - i / 100, 0.3, i / 100])\n",
        "\n",
        "ax[0].set_title('Histogram of Weights')\n",
        "ax[0].set_xlabel('Weight value')\n",
        "ax[0].set_ylabel('Density')\n",
        "\n",
        "ax[1].imshow(hist_y, vmin=0, vmax=3, extent=[hist_x[0], hist_x[-1], 0, 99],\n",
        "             aspect='auto', origin='lower', cmap='hot')\n",
        "ax[1].set_xlabel('Weight value')\n",
        "ax[1].set_xlabel('Training epoch')\n",
        "ax[1].set_title('Image of weight histogram')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-hbI3JJsfHO"
      },
      "source": [
        "## FFN_NONMNIST, FFN_BINARIZED MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTgDhIx-q_ip"
      },
      "outputs": [],
      "source": [
        "# Run the model through for the test data\n",
        "X, y        = next(iter(test_loader))\n",
        "predictions = net(X).detach()\n",
        "print(torch.exp(predictions))\n",
        "\n",
        "# Evidence for all numbers from one sample\n",
        "sample_2_show = 120\n",
        "\n",
        "plt.bar(range(10), predictions[sample_2_show])\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel('Number')\n",
        "plt.ylabel('Evidence for that number')\n",
        "plt.title(f'True number was {y[sample_2_show].item()}')\n",
        "plt.show()\n",
        "\n",
        "plt.bar(range(10), torch.exp(predictions[sample_2_show]))\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel('Number')\n",
        "plt.ylabel('Evidence for that number')\n",
        "plt.title(f'True number was {y[sample_2_show].item()}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__x8pExRruLU"
      },
      "outputs": [],
      "source": [
        "# Find the errors\n",
        "errors = np.where(torch.max(predictions, axis=1)[1] != y)[0]\n",
        "print(errors)\n",
        "\n",
        "# Evidence for all numbers from one sample\n",
        "sample_2_show = 14\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax[0].bar(range(10), np.exp(predictions[errors[sample_2_show]]))\n",
        "ax[0].set_xticks(range(10))\n",
        "ax[0].set_xlabel('Number')\n",
        "ax[0].set_ylabel('Evidence for that number')\n",
        "ax[0].set_title(f'True number: {y[errors[sample_2_show]].item()}, model guesses: \\\n",
        "                {torch.argmax(predictions[errors[sample_2_show]]).item()}')\n",
        "\n",
        "ax[1].imshow( np.reshape(X[errors[sample_2_show],:],(28,28)), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Yn62qB5m97"
      },
      "source": [
        "## FFN_ BREADTH VS. DEPTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePvNqWmz5n8P"
      },
      "outputs": [],
      "source": [
        "# Define the model parameters\n",
        "num_layers = range(1, 4)            # Number of hidden layers\n",
        "num_units  = np.arange(50, 251, 50) # Units per hidden layer\n",
        "\n",
        "# Initialize output matrices\n",
        "accuracies = np.zeros((2, len(num_units), len(num_layers)))\n",
        "\n",
        "for unit_i in range(len(num_units)):\n",
        "  for layer_i in range(len(num_layers)):\n",
        "\n",
        "    # Create and train a fresh model\n",
        "    train_acc, test_acc, losses, net = train_the_model(num_units[unit_i], num_layers[layer_i])\n",
        "\n",
        "    # Store the results (Average of final 5 epochs)\n",
        "    accuracies[0, unit_i, layer_i] = np.mean(train_acc[-5:])\n",
        "    accuracies[1, unit_i, layer_i] = np.mean(test_acc[-5:])\n",
        "\n",
        "    # Print a friendly status message\n",
        "    print(f'Finished {unit_i + 1} / {len(num_units)} unit and layers {layer_i + 1} / {len(num_layers)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXwsYwO07H3Q"
      },
      "outputs": [],
      "source": [
        "# SHOW ACCURACY AS A FUNCTION OF MODEL DEPTH\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "ax[0].plot(num_units, accuracies[0, :, :], markerfacecolor='w', markersize=9)\n",
        "ax[1].plot(num_units, accuracies[0, :, :], markerfacecolor='w', markersize=9)\n",
        "\n",
        "for i in range(2):\n",
        "  ax[i].legend(num_layers)\n",
        "  ax[i].set_ylabel('Accuracy')\n",
        "  ax[i].set_xlabel('Number of hidden units')\n",
        "  ax[i].set_title(['Train' if (i == 0) else 'Test'][0])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIDL6H2NIeGA"
      },
      "source": [
        "## FFN_OPTIMIZERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM0YLGOEIiBI"
      },
      "outputs": [],
      "source": [
        "# Variables to loop over\n",
        "learning_rates  = np.logspace(start=np.log10(0.0001), stop=np.log10(0.1), num=6)\n",
        "optimizer_types = ['SGD', 'RMSprop', 'Adam']\n",
        "\n",
        "# Initialize performance matrix\n",
        "final_performance = np.zeros(shape=(len(learning_rates), len(optimizer_types)))\n",
        "\n",
        "# Now for the experiment!\n",
        "for idx_optimizer, optimizer_i in enumerate(optimizer_types):\n",
        "  for idx_l_rate, l_rate_i in enumerate(learning_rates):\n",
        "    train_acc, test_acc, losses, net = train_the_model(optimizer_type=optimizer_i, learning_rate=l_rate_i)\n",
        "    final_performance[idx_l_rate, idx_optimizer] = np.mean(test_acc[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBOSJtx1IiBM"
      },
      "outputs": [],
      "source": [
        "# Plot the results! \n",
        "plt.plot(learning_rates, final_performance, 'o-', linewidth=2)\n",
        "plt.legend(optimizer_types)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rates')\n",
        "plt.ylabel('Test accuracy (ave. last 10 epochs)')\n",
        "plt.title('Comparison of optimizers by learning rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnHyYgwcdeH3"
      },
      "source": [
        "## FFN_MNIST_NO7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYDZqbLgdhZF"
      },
      "outputs": [],
      "source": [
        "# Run the model through for the test data\n",
        "# note: only need one output, hence the [0]\n",
        "X           = next(iter(test_loader))[0]\n",
        "predictions = net(X).detach()\n",
        "\n",
        "# Save the model guesses\n",
        "guesses = torch.argmax(predictions, axis=1).detach()\n",
        "\n",
        "# Show some 7's and their labels\n",
        "fig, axs = plt.subplots(3, 4, figsize=(10, 6))\n",
        "some_random_7s = np.random.choice(len(X), size=12)\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "  this_img = X[some_random_7s[i]].view(28, 28)\n",
        "  ax.imshow(this_img, cmap='gray')\n",
        "  ax.set_title(f'The number {guesses[some_random_7s[i]]}')\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nz0pOFqetW5"
      },
      "outputs": [],
      "source": [
        "# Which numbers are most likely to be confused with 7?\n",
        "print(np.unique(guesses))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "prob_mistaken = np.zeros(10)\n",
        "for i in range(10):\n",
        "  prob_mistaken[i] = torch.mean((guesses == i).float())\n",
        "\n",
        "ax[0].bar(range(10), prob_mistaken)\n",
        "ax[0].xticks(range(10))\n",
        "ax[0].xlabel('Number')\n",
        "ax[0].ylabel('Proportion of times \"7\" was labeled')\n",
        "ax[0].show()\n",
        "\n",
        "# Evidence for all numbers from one sample\n",
        "sample_2_show = 30\n",
        "\n",
        "plt.bar(range(10), torch.exp(predictions[sample_2_show]))\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel('Number')\n",
        "plt.ylabel('Evidence for that number')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyQKSoZ7hP-f"
      },
      "source": [
        "## DATA_DATA_VS_DEPTH_QWERTY2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sx-mzKJhP-f"
      },
      "outputs": [],
      "source": [
        "# Before the experiment, configure and confirm the metaparameters\n",
        "# Specify the parameters for the model\n",
        "n_nodes_in_model = 80\n",
        "layers_range     = [1, 5, 10, 20]\n",
        "n_data_points    = np.arange(start=50, stop=551, step=50)\n",
        "\n",
        "# Create a legend for later plotting\n",
        "legend = []\n",
        "\n",
        "# Print out the model architectures\n",
        "for layer_idx, layers in enumerate(layers_range):\n",
        "    # Create a model\n",
        "    units_per_layer = int(n_nodes_in_model / layers_range[layer_idx])\n",
        "    net             = create_the_qwerty_net(n_units=units_per_layer, n_layers=layers)[0]\n",
        "\n",
        "    # Count its parameters\n",
        "    n_params = np.sum([p.numel() for p in net.parameters() if p.requires_grad])\n",
        "\n",
        "    legend.append(f'{layers} layers, {units_per_layer} units, {n_params} params')\n",
        "    print(f'This model will have {layers} layers, each with {units_per_layer} units, totalling {n_params} parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpEO89yIhP-f"
      },
      "outputs": [],
      "source": [
        "# Initialize results matrix\n",
        "results = np.zeros((len(n_data_points), len(layers_range), 2))\n",
        "\n",
        "for data_point_idx, data_point_i in enumerate(n_data_points):\n",
        "    # Create data (note: same data for each layer manipulation!)\n",
        "    the_data   = create_some_data(n_per_clust=data_point_i)\n",
        "    train_data = the_data['train_data']\n",
        "    test_data  = the_data['test_data'] \n",
        "\n",
        "    # Now loop over layers\n",
        "    for layers_range_idx, layers_range_i in enumerate(layers_range):\n",
        "        units_per_layer = int(n_nodes_in_model / layers_range[layers_range_idx])\n",
        "        train_acc, test_acc, losses, net = train_the_model(n_units=units_per_layer, n_layers=layers_range_i)\n",
        "\n",
        "        # Average of last 5 accuracies and losses\n",
        "        results[data_point_idx, layers_range_idx, 0] = np.mean(test_acc[-5:])\n",
        "        results[data_point_idx, layers_range_idx, 1] = torch.mean(losses[-5:]).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLsohGmThP-f"
      },
      "outputs": [],
      "source": [
        "# Show the results\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax[0].plot(n_data_points, results[:, :, 1], 's-')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_xlabel('Number of data points')\n",
        "ax[0].legend(legend)\n",
        "ax[0].set_title('Losses')\n",
        "\n",
        "ax[1].plot(n_data_points, results[:, :, 0], 'o-')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_xlabel('Number of data points')\n",
        "ax[1].set_title('Accuracy')\n",
        "ax[1].legend(legend)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79C-1pLLhP-g"
      },
      "source": [
        "## DATA_ UNBALANCED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-PAp0WhhP-h"
      },
      "outputs": [],
      "source": [
        "# Create the data\n",
        "train_loader, test_loader = create_a_dataset(qual_threshold=5)\n",
        "\n",
        "# Create a model and train it\n",
        "wine_net = ANN_wine()\n",
        "train_acc, test_acc, losses = train_the_model()\n",
        "\n",
        "# Compute accuracy per quality type\n",
        "X, y = next(iter(test_loader))\n",
        "y_hat = wine_net(X)\n",
        "item_accuracy = ((y_hat > 0) == y).float()\n",
        "\n",
        "per_qual_acc = [100 * torch.mean(item_accuracy[y == 0]),\n",
        "                100 * torch.mean(item_accuracy[y == 1])]\n",
        "\n",
        "per_qual_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14l6EmSChP-h"
      },
      "outputs": [],
      "source": [
        "# Setup the figure\n",
        "fig, ax = plt.subplots(3, 3, figsize=(18, 12))\n",
        "\n",
        "# The quality thresholds\n",
        "quality_thresholds = [4, 5, 6]\n",
        "\n",
        "# Loop over quality thresholds\n",
        "for qual_thres_idx in range(len(quality_thresholds)):\n",
        "\n",
        "    # Create the data and model, and train it\n",
        "    train_loader, test_loader = create_a_dataset(qual_threshold=quality_thresholds[qual_thres_idx])\n",
        "    wine_net                  = ANN_wine()\n",
        "    train_acc, test_acc, loss = train_the_model()\n",
        "\n",
        "    # Compute accuracy per quality type\n",
        "    X, y          = next(iter(test_loader))\n",
        "    item_accuracy = ((wine_net(X) > 0) == y).float()\n",
        "    per_qual_acc  = [100 * torch.mean(item_accuracy[y == 0]),\n",
        "                     100 * torch.mean(item_accuracy[y == 1])]\n",
        "\n",
        "    # Plot losses\n",
        "    ax[qual_thres_idx, 0].plot(losses)\n",
        "    ax[qual_thres_idx, 0].set_title(f'Losses with Threshold = {quality_thresholds[qual_thres_idx] + 0.5}')\n",
        "    ax[qual_thres_idx, 0].set_xlabel('Epoch')\n",
        "    ax[qual_thres_idx, 0].grid()\n",
        "\n",
        "    # Plot overall accuracy\n",
        "    ax[qual_thres_idx, 1].plot(train_acc, label='Train')\n",
        "    ax[qual_thres_idx, 1].plot(test_acc,  label='Test')\n",
        "    ax[qual_thres_idx, 1].set_title(f'Accuracy with Threshold = {quality_thresholds[qual_thres_idx] + 0.5}')\n",
        "    ax[qual_thres_idx, 1].legend()\n",
        "    ax[qual_thres_idx, 1].set_xlabel('Epoch')\n",
        "    ax[qual_thres_idx, 1].set_ylim([0, 100])\n",
        "    ax[qual_thres_idx, 1].grid()\n",
        "\n",
        "    # Plot the per-quality accuracy\n",
        "    bh = ax[qual_thres_idx, 2].bar(['Bad', 'Good'], per_qual_acc)\n",
        "    ax[qual_thres_idx, 2].set_ylim([0, 100])\n",
        "    ax[qual_thres_idx, 2].set_xlabel('Wine quality')\n",
        "    ax[qual_thres_idx, 2].set_ylabel('Test Accuracy')\n",
        "    ax[qual_thres_idx, 2].set_title(f'Per-Qual acc. with Threshold = {quality_thresholds[qual_thres_idx] + 0.5}')\n",
        "\n",
        "    # Print the counts on top of each bar\n",
        "    for i, r in enumerate(bh):\n",
        "        N = torch.sum(train_loader.dataset.tensors[1] == i).item()\n",
        "        ax[qual_thres_idx, 2].text(r.get_x() + r.get_width()/2, r.get_height() + 1,\n",
        "                                    f'N = {N}', ha='center', va='bottom', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omYAV3jFhP-h"
      },
      "source": [
        "## DATA_ DATA_OVERSAMPLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b26JivdhP-i"
      },
      "source": [
        "### FUNC - RETURNS A DATASET WITH A SPECIFIED SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gokPzvb5hP-i"
      },
      "outputs": [],
      "source": [
        "data_full = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Now for the function\n",
        "def make_the_dataset(N, double_the_data=False):\n",
        "\n",
        "    # Extract labels (number IDs) and remove from data\n",
        "    labels = data_full[:N, 0]\n",
        "    data   = data_full[:N, 1:]\n",
        "\n",
        "    # Normalize the data to a range of [0 1]\n",
        "    data_norm = data / np.max(data)\n",
        "\n",
        "    # Make an exact copy of ALL the data\n",
        "    if double_the_data:\n",
        "        data_norm = np.concatenate((data_norm, data_norm), axis=0)\n",
        "        labels    = np.concatenate((labels, labels), axis=0)\n",
        "\n",
        "    # Convert to tensor\n",
        "    data_tensor   = torch.tensor(data_norm).float()\n",
        "    labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "    # Use scikitlearn to split the data\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(data_tensor, labels_tensor, train_size=0.9)\n",
        "\n",
        "    # # Make an exact copy of the TRAIN data\n",
        "    # if doubleTheData:\n",
        "    #   train_data   = torch.cat((train_data,train_data),axis=0)\n",
        "    #   train_labels = torch.cat((train_labels,train_labels),axis=0)\n",
        "    \n",
        "    # Convert into PyTorch Datasets\n",
        "    train_data_set = TensorDataset(train_data, train_labels)\n",
        "    test_data_set  = TensorDataset(test_data, test_labels)\n",
        "\n",
        "    # Translate into Dataloader objects\n",
        "    batch_size   = 20\n",
        "    train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                            shuffle=True, drop_last=True)\n",
        "    test_loader  = DataLoader(dataset=test_data_set, \n",
        "                            batch_size=test_data_set.tensors[0].shape[0])\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xotby8kPhP-j"
      },
      "outputs": [],
      "source": [
        "# Check the sizes\n",
        "r, t = make_the_dataset(N=200, double_the_data=False)\n",
        "print(r.dataset.tensors[0].shape)\n",
        "print(t.dataset.tensors[0].shape)\n",
        "\n",
        "r, t = make_the_dataset(N=200, double_the_data=True)\n",
        "print(r.dataset.tensors[0].shape)\n",
        "print(t.dataset.tensors[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccvGUJY6hP-j"
      },
      "outputs": [],
      "source": [
        "# List of data sample sizes\n",
        "sample_sizes = np.arange(start=500, stop=4001, step=500)\n",
        "\n",
        "# Initialize results matrix\n",
        "results_single = np.zeros(shape=(len(sample_sizes), 3))\n",
        "results_double = np.zeros(shape=(len(sample_sizes), 3))\n",
        "\n",
        "for sample_size_idx, sample_size_i in enumerate(sample_sizes):\n",
        "\n",
        "    # Without doubling the data!\n",
        "    # Generate a dataset and train the model\n",
        "    train_loader, test_loader        = make_the_dataset(N=sample_size_i, double_the_data=False)\n",
        "    train_acc, test_acc, losses, net = train_the_model()\n",
        "\n",
        "    # Grab the results\n",
        "    results_single[sample_size_idx, 0] = np.mean(train_acc[-5:])\n",
        "    results_single[sample_size_idx, 1] = np.mean(test_acc[-5:])\n",
        "    results_single[sample_size_idx, 2] = torch.mean(losses[-5:]).item()\n",
        "\n",
        "    # With doubling the data!\n",
        "    # Generate a dataset and train the model\n",
        "    train_loader, test_loader        = make_the_dataset(N=sample_size_i, double_the_data=True)\n",
        "    train_acc, test_acc, losses, net = train_the_model()\n",
        "\n",
        "    # Grab the results\n",
        "    results_double[sample_size_idx, 0] = np.mean(train_acc[-5:])\n",
        "    results_double[sample_size_idx, 1] = np.mean(test_acc[-5:])\n",
        "    results_double[sample_size_idx, 2] = torch.mean(losses[-5:]).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-DJbBpMhP-k"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Axis and title labels\n",
        "titles      = ['Train', 'Devset', 'Losses']\n",
        "y_ax_labels = ['Accuracy', 'Accuracy', 'Losses']\n",
        "\n",
        "# Common features\n",
        "for i in range(3):\n",
        "\n",
        "    # Plot the lines\n",
        "    ax[i].plot(sample_sizes, results_single[:, i], 's-', label='Original')\n",
        "    ax[i].plot(sample_sizes, results_double[:, i], 's-', label='Doubled')\n",
        "\n",
        "    # Make it look nicer\n",
        "    ax[i].set_ylabel(y_ax_labels[i])\n",
        "    ax[i].set_title(titles[i])\n",
        "    ax[i].legend()\n",
        "    ax[i].set_xlabel('Unique sample size')\n",
        "    ax[i].grid('on')\n",
        "\n",
        "    if (i < 2):\n",
        "        ax[i].set_ylim([20, 102])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H-Ry0Pv4f1T"
      },
      "source": [
        "## DATA_NOISE_AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDmeaTRn4f1U"
      },
      "outputs": [],
      "source": [
        "data_full = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "\n",
        "# Normalize the data to a range of [0 1]\n",
        "data_full[:, 1:] = data_full[:, 1:] / np.max(data_full)\n",
        "\n",
        "# Now for the function\n",
        "def make_the_dataset(N, double_the_data=False):\n",
        "\n",
        "    # Extract labels (number IDs) and remove from data\n",
        "    labels = data_full[:N, 0]\n",
        "    data   = data_full[:N, 1:]\n",
        "\n",
        "    # Make a noisy copy of ALL the data\n",
        "    if double_the_data:\n",
        "        data_noise = data + np.random.random_sample(size=data.shape) / 2\n",
        "        data       = np.concatenate((data, data_noise), axis=0)\n",
        "        labels     = np.concatenate((labels, labels), axis=0)\n",
        "\n",
        "    # Convert to tensor\n",
        "    data_tensor   = torch.tensor(data).float()\n",
        "    labels_tensor = torch.tensor(labels).long()\n",
        "\n",
        "    # Use scikitlearn to split the data\n",
        "    train_data, dev_data, train_labels, dev_labels = train_test_split(data_tensor, labels_tensor, train_size=0.9)\n",
        "\n",
        "    # # Make an exact copy of the TRAIN data\n",
        "    # if doubleTheData:\n",
        "    #   train_data   = torch.cat((train_data,train_data),axis=0)\n",
        "    #   train_labels = torch.cat((train_labels,train_labels),axis=0)\n",
        "    \n",
        "    # Convert into PyTorch Datasets\n",
        "    train_data_set = TensorDataset(train_data, train_labels)\n",
        "    dev_data_set   = TensorDataset(dev_data, dev_labels)\n",
        "\n",
        "    # Translate into Dataloader objects\n",
        "    batch_size   = 20\n",
        "    train_loader = DataLoader(dataset=train_data_set, batch_size=batch_size,\n",
        "                            shuffle=True, drop_last=True)\n",
        "    dev_loader  = DataLoader(dataset=dev_data_set, \n",
        "                            batch_size=dev_data_set.tensors[0].shape[0])\n",
        "    \n",
        "    # Create a test set (don't need a dataloader)\n",
        "    test_data   = torch.tensor(data_full[N:, 1:]).float()\n",
        "    test_labels = torch.tensor(data_full[N:, 0]).long() \n",
        "\n",
        "    return train_loader, dev_loader, (test_data, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA8dR_7U4f1U"
      },
      "outputs": [],
      "source": [
        "# VISUALIZE THE IMAGES\n",
        "# Get some sample data\n",
        "train_loader, dev_loader, test_data_set = make_the_dataset(N=12, double_the_data=True)\n",
        "\n",
        "# Pop out the data matrices\n",
        "img = train_loader.dataset.tensors[0].detach()\n",
        "\n",
        "# Show the numbers\n",
        "fig, ax = plt.subplots(3, 4, figsize=(12, 8))\n",
        "for i, ax in enumerate(ax.flatten()):\n",
        "    ax.imshow(np.reshape(img[i, :], (28, 28)), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p85LNE1V4f1X"
      },
      "outputs": [],
      "source": [
        "# List of data sample sizes\n",
        "sample_sizes = np.arange(start=500, stop=4001, step=500)\n",
        "\n",
        "# Initialize results matrix\n",
        "results_single = np.zeros(shape=(len(sample_sizes), 3))\n",
        "results_double = np.zeros(shape=(len(sample_sizes), 3))\n",
        "\n",
        "for sample_size_idx, sample_size_i in enumerate(sample_sizes):\n",
        "\n",
        "    # Without doubling the data!\n",
        "    # Generate a dataset and train the model\n",
        "    train_loader, dev_loader, test_data_set        = make_the_dataset(N=sample_size_i, double_the_data=False)\n",
        "    train_acc, dev_acc, losses, net                = train_the_model()\n",
        "\n",
        "    # Grab the results\n",
        "    results_single[sample_size_idx, 0] = np.mean(train_acc[-5:])\n",
        "    results_single[sample_size_idx, 1] = np.mean(dev_acc[-5:])\n",
        "    results_single[sample_size_idx, 2] = torch.mean(losses[-5:]).item()\n",
        "\n",
        "    # With doubling the data!\n",
        "    # Generate a dataset and train the model\n",
        "    train_loader, dev_loader, test_data_set        = make_the_dataset(N=sample_size_i, double_the_data=True)\n",
        "    train_acc, dev_acc, losses, net                = train_the_model()\n",
        "\n",
        "    # Grab the results\n",
        "    results_double[sample_size_idx, 0] = np.mean(train_acc[-5:])\n",
        "    results_double[sample_size_idx, 1] = np.mean(dev_acc[-5:])\n",
        "    results_double[sample_size_idx, 2] = torch.mean(losses[-5:]).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-78qzkpG4f1X"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Axis and title labels\n",
        "titles      = ['Train', 'Devset', 'Losses']\n",
        "y_ax_labels = ['Accuracy', 'Accuracy', 'Losses']\n",
        "\n",
        "# Common features\n",
        "for i in range(3):\n",
        "\n",
        "    # Plot the lines\n",
        "    ax[i].plot(sample_sizes, results_single[:, i], 's-', label='Original')\n",
        "    ax[i].plot(sample_sizes, results_double[:, i], 's-', label='Augmented')\n",
        "\n",
        "    # Make it look nicer\n",
        "    ax[i].set_ylabel(y_ax_labels[i])\n",
        "    ax[i].set_title(titles[i])\n",
        "    ax[i].legend()\n",
        "    ax[i].set_xlabel('Unique sample size')\n",
        "    ax[i].grid('on')\n",
        "\n",
        "    if (i < 2):\n",
        "        ax[i].set_ylim([20, 102])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlPsv7xQ4f1Y"
      },
      "outputs": [],
      "source": [
        "# TEST ACCURACY\n",
        "sample_size = 500\n",
        "\n",
        "train_loader, dev_loader, test_data_set = make_the_dataset(N=sample_size, double_the_data=False)\n",
        "train_acc_O, dev_acc_O, losses_O, net_O = train_the_model() # O = original\n",
        "\n",
        "train_loader, dev_loader, test_data_set = make_the_dataset(N=sample_size, double_the_data=True)\n",
        "train_acc_A, dev_acc_A, losses_A, net_A = train_the_model() # A = augmented\n",
        "\n",
        "# Extract the test data\n",
        "X, y = test_data_set\n",
        "\n",
        "# Run the original model\n",
        "y_hat  = net_O(X)\n",
        "test_O = torch.mean((torch.argmax(y_hat, axis=1) == y).float())\n",
        "\n",
        "\n",
        "# And the augmented model\n",
        "y_hat  = net_A(X)\n",
        "test_A = torch.mean((torch.argmax(y_hat, axis=1) == y).float())\n",
        "\n",
        "# Print the results!\n",
        "print(f'ORIGINAL MODEL  (N = {sample_size})\\n  Train: {train_acc_O[-1]:.2f}%, Devset: {dev_acc_O[-1]:.2f}%, Test: {test_O:.2f}%\\n\\n')\n",
        "print(f'AUGMENTED MODEL (N = {sample_size})\\n  Train: {train_acc_A[-1]:.2f}%, Devset: {dev_acc_A[-1]:.2f}%, Test: {test_A:.2f}%\\n\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zzj_HITjj59"
      },
      "source": [
        "## DATA_DATA_FEATURE_AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5eZXPj_jj5-"
      },
      "outputs": [],
      "source": [
        "def plot_the_results():\n",
        "    # Compute accuracy over entire dataset (train + test)\n",
        "    y_hat       = net(data_aug)\n",
        "    predictions = torch.argmax(y_hat, axis=1)\n",
        "    accuracy    = (predictions == labels).float()\n",
        "\n",
        "    # Accuracy by group\n",
        "    accuracy_by_group = np.zeros(3)\n",
        "    for i in range(3):\n",
        "        accuracy_by_group[i] = 100 * torch.mean(accuracy[labels == i])\n",
        "    \n",
        "    # Create the figure\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 6))\n",
        "\n",
        "    # Plot  the loss function\n",
        "    ax[0, 0].plot(losses.detach())\n",
        "    ax[0, 0].set_ylabel('Loss')\n",
        "    ax[0, 0].set_xlabel('Epoch')\n",
        "    ax[0, 0].set_title('Losses')\n",
        "\n",
        "    # Plot the accuracy function\n",
        "    ax[0, 1].plot(train_acc, label='Train')\n",
        "    ax[0, 1].plot(test_acc,  label='Test')\n",
        "    ax[0, 1].set_ylabel('Accuracy (%)')\n",
        "    ax[0, 1].set_xlabel('Epoch')\n",
        "    ax[0, 1].set_title('Accuracy')\n",
        "    ax[0, 1].legend()\n",
        "\n",
        "    # Plot overall accuracy by group\n",
        "    ax[1, 0].bar(range(3), accuracy_by_group)\n",
        "    ax[1, 0].set_ylim([np.min(accuracy_by_group) - 5, np.max(accuracy_by_group) + 5])\n",
        "    ax[1, 0].set_xticks([0, 1, 2])\n",
        "    ax[1, 0].set_xlabel('Group')\n",
        "    ax[1, 0].set_ylabel('Accuracy (%)')\n",
        "    ax[1, 0].set_title('Accuracy by group')\n",
        "\n",
        "    # Scatter plot of correct and incorrect labeled data\n",
        "    color_shapes = ['bs', 'ko', 'g^'] # Data makers\n",
        "    for i in range(3):\n",
        "        # Plot all data points\n",
        "        ax[1, 1].plot(data_aug[labels == i, 0], data_aug[labels == i, 1], color_shapes[i],\n",
        "                      alpha=0.3, label=f'Group {i}')\n",
        "\n",
        "        # Cross-out the incorrect ones\n",
        "        idx_err = (accuracy == 0) & (labels == i)\n",
        "        ax[1, 1].plot(data_aug[idx_err, 0], data_aug[idx_err, 1], 'rx')\n",
        "    \n",
        "    ax[1, 1].set_title('All groups')\n",
        "    ax[1, 1].set_xlabel('qwerty dimension 1')\n",
        "    ax[1, 1].set_ylabel('qwerty dimension 2')\n",
        "    ax[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdeyAFMujj6A"
      },
      "outputs": [],
      "source": [
        "# Run the model and visualize the results\n",
        "train_acc, test_acc, losses, net = train_the_model(use_extra_feature=False)\n",
        "print(f'Final accuracy: {test_acc[-1]:.2f}')\n",
        "plot_the_results()\n",
        "\n",
        "# Run the model and visualize the results\n",
        "train_acc, test_acc, losses, net = train_the_model(use_extra_feature=True)\n",
        "print(f'Final accuracy: {test_acc[-1]:.2f}')\n",
        "plot_the_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ_hM_6ljj6A"
      },
      "outputs": [],
      "source": [
        "# Run the exper multiple times and do a t-test\n",
        "\n",
        "final_acc_2 = np.zeros(10)\n",
        "final_acc_3 = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "    final_acc_2[i] = train_the_model(use_extra_feature=False)[1][-1]\n",
        "    final_acc_3[i] = train_the_model(use_extra_feature=True)[1][-1]\n",
        "\n",
        "# Show the numbers\n",
        "print(np.round(np.vstack(tup=(final_acc_2, final_acc_3)).T, decimals=2))\n",
        "\n",
        "# Run the t-test and print the results\n",
        "t, p = stats.ttest_ind(final_acc_3, final_acc_2)\n",
        "print(f't = {t:.2f}, p = {p:.2f}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNQbLxZ8jj6B"
      },
      "source": [
        "## DATA_SAVE_BEST_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C380fziyjj6C"
      },
      "outputs": [],
      "source": [
        "# Run the model\n",
        "train_acc, dev_acc, losses, the_best_model = train_the_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcGWRcs9jj6D"
      },
      "outputs": [],
      "source": [
        "# Recreate the best-performing model\n",
        "best_net = create_the_qwerty_net()[0]\n",
        "best_net.load_state_dict(the_best_model['net'])\n",
        "\n",
        "# Run the data through TEST\n",
        "X, y     = next(iter(test_loader))\n",
        "y_hat    = best_net(X)\n",
        "best_acc = 100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float())\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFQBcorWjj6D"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(train_acc, 'o-', label='Train')\n",
        "plt.plot(dev_acc,   'o-', label='Devset')\n",
        "plt.plot([0, len(dev_acc)], [best_acc, best_acc], 'r--', label='Best Dev model on TEST')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Accuracy')\n",
        "plt.ylim([best_acc - 5, best_acc + 5])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ml-PI0Wjj6E"
      },
      "source": [
        "## MODEL_PERFORM_APRF_WINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RqxC88qjj6E"
      },
      "outputs": [],
      "source": [
        "# Create and train a model\n",
        "wine_net = ANN_wine()\n",
        "train_acc, test_acc, losses = train_the_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSPHyUZSjj6F"
      },
      "outputs": [],
      "source": [
        "# Predictions for Training data\n",
        "train_predictions = wine_net(train_loader.dataset.tensors[0])\n",
        "\n",
        "# Predictions for Training data\n",
        "test_predictions  = wine_net(test_loader.dataset.tensors[0])\n",
        "\n",
        "# Using scikitlearn to compute APRF\n",
        "# Initialize vectors\n",
        "train_metrics   = [0, 0, 0, 0]\n",
        "test_metrics    = [0, 0, 0, 0]\n",
        "\n",
        "# Training\n",
        "train_metrics[0] = skm.accuracy_score(y_true=train_loader.dataset.tensors[1], y_pred=(train_predictions > 0))\n",
        "train_metrics[1] = skm.precision_score(y_true=train_loader.dataset.tensors[1], y_pred=(train_predictions > 0))\n",
        "train_metrics[2] = skm.recall_score(y_true=train_loader.dataset.tensors[1], y_pred=(train_predictions > 0))\n",
        "train_metrics[3] = skm.f1_score(y_true=train_loader.dataset.tensors[1], y_pred=(train_predictions > 0))\n",
        "\n",
        "# Test\n",
        "test_metrics[0] = skm.accuracy_score(y_true=test_loader.dataset.tensors[1], y_pred=(test_predictions > 0))\n",
        "test_metrics[1] = skm.precision_score(y_true=test_loader.dataset.tensors[1], y_pred=(test_predictions > 0))\n",
        "test_metrics[2] = skm.recall_score(y_true=test_loader.dataset.tensors[1], y_pred=(test_predictions > 0))\n",
        "test_metrics[3] = skm.f1_score(y_true=test_loader.dataset.tensors[1], y_pred=(test_predictions > 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icde3Yegjj6F"
      },
      "outputs": [],
      "source": [
        "plt.bar(x=np.arange(4) - 0.1, height=train_metrics, width=0.5)\n",
        "plt.bar(x=np.arange(4) + 0.1, height=test_metrics, width=0.5)\n",
        "plt.xticks([0, 1, 2, 3], ['Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
        "plt.ylim([0.6, 1])\n",
        "plt.legend(['Train', 'Test'])\n",
        "plt.title('Performance Metrics')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHobrGlojj6F"
      },
      "outputs": [],
      "source": [
        "# CONFUSION MATRICES\n",
        "train_conf  = skm.confusion_matrix(y_true=train_loader.dataset.tensors[1], y_pred=(train_predictions > 0))\n",
        "test_conf   = skm.confusion_matrix(y_true=test_loader.dataset.tensors[1], y_pred=(test_predictions > 0))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# During TRAIN\n",
        "ax[0].imshow(train_conf, 'Blues', vmax=len(train_predictions) / 2)\n",
        "ax[0].set_xticks([0, 1])\n",
        "ax[0].set_yticks([0, 1])\n",
        "ax[0].set_xticklabels(['bad', 'good'])\n",
        "ax[0].set_yticklabels(['bad', 'good'])\n",
        "ax[0].set_xlabel('Prediction Quality')\n",
        "ax[0].set_ylabel('True Quality')\n",
        "ax[0].set_title('TRAIN Confusion Matrix')\n",
        "\n",
        "# Add text labels\n",
        "ax[0].text(0, 0, f'True Negatives:\\n{train_conf[0, 0]}',   ha='center', va='center')\n",
        "ax[0].text(0, 1, f'False Negatives:\\n{train_conf[1, 0]}',  ha='center', va='center')\n",
        "ax[0].text(1, 1, f'True Positives:\\n{train_conf[1, 1]}',   ha='center', va='center')\n",
        "ax[0].text(1, 0, f'False Positives:\\n{train_conf[0, 1]}',  ha='center', va='center')\n",
        "\n",
        "# During TEST\n",
        "ax[1].imshow(test_conf, 'Blues', vmax=len(train_predictions) / 2)\n",
        "ax[1].set_xticks([0, 1])\n",
        "ax[1].set_yticks([0, 1])\n",
        "ax[1].set_xticklabels(['bad', 'good'])\n",
        "ax[1].set_yticklabels(['bad', 'good'])\n",
        "ax[1].set_xlabel('Prediction Quality')\n",
        "ax[1].set_ylabel('True Quality')\n",
        "ax[1].set_title('TRAIN Confusion Matrix')\n",
        "\n",
        "# Add text labels\n",
        "ax[1].text(0, 0, f'True Negatives:\\n{test_conf[0, 0]}',   ha='center', va='center')\n",
        "ax[1].text(0, 1, f'False Negatives:\\n{test_conf[1, 0]}',  ha='center', va='center')\n",
        "ax[1].text(1, 1, f'True Positives:\\n{test_conf[1, 1]}',   ha='center', va='center')\n",
        "ax[1].text(1, 0, f'False Positives:\\n{test_conf[0, 1]}',  ha='center', va='center')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOU9BGNWjj6G"
      },
      "source": [
        "## MODEL_PERFORM_APRF_MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5M5MHxhjj6G"
      },
      "outputs": [],
      "source": [
        "# Predicted digits for training data\n",
        "y_hat             = net(train_loader.dataset.tensors[0])\n",
        "train_predictions = torch.argmax(y_hat, axis=1)\n",
        "\n",
        "# Predictions for test data\n",
        "y_hat             = net(test_loader.dataset.tensors[0])\n",
        "test_predictions  = torch.argmax(y_hat, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfHG_Pp6jj6H"
      },
      "outputs": [],
      "source": [
        "# Initialize vectors\n",
        "train_metrics = [0, 0, 0, 0]\n",
        "test_metrics  = [0, 0, 0, 0]\n",
        "\n",
        "# Training\n",
        "train_metrics[0] = skm.accuracy_score(y_true=train_loader.dataset.tensors[1],  y_pred=train_predictions)\n",
        "train_metrics[1] = skm.precision_score(y_true=train_loader.dataset.tensors[1], y_pred=train_predictions, \n",
        "                                        average='weighted')\n",
        "train_metrics[2] = skm.recall_score(y_true=train_loader.dataset.tensors[1],    y_pred=train_predictions, \n",
        "                                        average='weighted')\n",
        "train_metrics[3] = skm.f1_score(y_true=train_loader.dataset.tensors[1],        y_pred=train_predictions, \n",
        "                                        average='weighted')\n",
        "\n",
        "# Test\n",
        "test_metrics[0] = skm.accuracy_score(y_true=test_loader.dataset.tensors[1],  y_pred=test_predictions)\n",
        "test_metrics[1] = skm.precision_score(y_true=test_loader.dataset.tensors[1], y_pred=test_predictions, \n",
        "                                        average='weighted')\n",
        "test_metrics[2] = skm.recall_score(y_true=test_loader.dataset.tensors[1],    y_pred=test_predictions,\n",
        "                                        average='weighted')\n",
        "test_metrics[3] = skm.f1_score(y_true=test_loader.dataset.tensors[1],        y_pred=test_predictions, \n",
        "                                        average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFt_qfqmjj6K"
      },
      "outputs": [],
      "source": [
        "plt.bar(x=np.arange(4) - 0.1, height=train_metrics, width=0.5)\n",
        "plt.bar(x=np.arange(4) + 0.1, height=test_metrics,  width=0.5)\n",
        "plt.xticks([0, 1, 2, 3], ['Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
        "plt.ylim([0.9, 1])\n",
        "plt.legend(['Train', 'Test'])\n",
        "plt.title('Performance Metrics')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv-Za5b5jj6K"
      },
      "outputs": [],
      "source": [
        "# Bar graphs of class-specific precision and recall for test data\n",
        "\n",
        "precision = skm.precision_score(y_true=test_loader.dataset.tensors[1], y_pred=test_predictions, average=None)\n",
        "recall    = skm.recall_score   (y_true=test_loader.dataset.tensors[1], y_pred=test_predictions, average=None)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 3))\n",
        "plt.bar(x=np.arange(10) - 0.15, height=precision, width=0.5)\n",
        "plt.bar(x=np.arange(10) + 0.15, height=recall,    width=0.5)\n",
        "plt.xticks(range(10), range(10))\n",
        "plt.ylim([0.5, 1])\n",
        "plt.xlabel('Number')\n",
        "plt.legend(['Precision', 'Recall'])\n",
        "plt.title('Categoty-specific performance metrics')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xvLbJcBjj6L"
      },
      "outputs": [],
      "source": [
        "# CONFUSION MATRICES\n",
        "train_conf  = skm.confusion_matrix(y_true=train_loader.dataset.tensors[1], y_pred=train_predictions, normalize='true')\n",
        "test_conf   = skm.confusion_matrix(y_true=test_loader.dataset.tensors[1],  y_pred=test_predictions,  normalize='true')\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# During TRAIN\n",
        "ax[0].imshow(train_conf, 'Blues', vmax=0.05)\n",
        "ax[0].set_xticks(range(10))\n",
        "ax[0].set_yticks(range(10))\n",
        "ax[0].set_xlabel('Predicted number')\n",
        "ax[0].set_ylabel('True Number')\n",
        "ax[0].set_title('TRAIN Confusion Matrix')\n",
        "\n",
        "# During TEST\n",
        "a = ax[1].imshow(test_conf, 'Blues', vmax=0.05)\n",
        "ax[1].set_xticks(range(10))\n",
        "ax[1].set_yticks(range(10))\n",
        "ax[1].set_xlabel('Predicted number')\n",
        "ax[1].set_ylabel('True Number')\n",
        "ax[1].set_title('TEST Confusion Matrix')\n",
        "\n",
        "fig.colorbar(a)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0tphAOthXs"
      },
      "source": [
        "## MODEL_PERFORM_MNIST_NO7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ele67AxQthXs"
      },
      "outputs": [],
      "source": [
        "# See whether there is a roughtly equal number of elements in each category\n",
        "\n",
        "# Category counts\n",
        "cat_count = np.unique(labels, return_counts=True)\n",
        "\n",
        "# Visualize\n",
        "plt.bar(cat_count[0], cat_count[1])\n",
        "plt.xlabel('Digit')\n",
        "plt.ylabel('N occurrences')\n",
        "plt.xticks(range(10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWMclzcKthXs"
      },
      "outputs": [],
      "source": [
        "# Find all the 7's\n",
        "where7s = np.where(labels == 7)[0]\n",
        "\n",
        "# How many to remove, to leave 500 in the data\n",
        "N_2_remove = where7s.shape[0] - 500\n",
        "\n",
        "# Pick that number of 7's at random\n",
        "remove_7s = np.random.choice(where7s, size=N_2_remove, replace=False)\n",
        "\n",
        "# And remove them from the data!\n",
        "print(f'Sizes before removing: {data_norm.shape} | {labels.shape}')\n",
        "\n",
        "data_norm = np.delete(data_norm, remove_7s, axis=0)\n",
        "labels    = np.delete(labels,    remove_7s)\n",
        "\n",
        "print(f'Sizes after removing: {data_norm.shape} | {labels.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAgLHB_bthXs"
      },
      "outputs": [],
      "source": [
        "# Visual confirmation of our manipulation\n",
        "cat_count = np.unique(labels, return_counts=True)\n",
        "plt.bar(cat_count[0], cat_count[1])\n",
        "plt.xlabel('Digit')\n",
        "plt.ylabel('N occurrences')\n",
        "plt.xticks(range(10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XsoJdaIhXSi"
      },
      "source": [
        "## MODEL_PERFORM_TIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6NbnscshXSi"
      },
      "outputs": [],
      "source": [
        "# Start the timer!\n",
        "timer_outside_function = time.process_time()\n",
        "\n",
        "for i in range(10):\n",
        "    train_the_model()\n",
        "\n",
        "total_experiment_time = time.process_time() - timer_outside_function\n",
        "print(f'\\n\\nTotal elapsed experiment time: {total_experiment_time/60:.2f} minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukL1gevMhXSj"
      },
      "source": [
        "## WEIGHTS_DEMO_INITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgKP0plihXSj"
      },
      "outputs": [],
      "source": [
        "# Run the model without changing the weights; this will be the baseline performance.\n",
        "# Notice the model creation is outside the function2train\n",
        "net_base, loss_func, optimizer                  = create_the_MNIST_net()\n",
        "train_acc_base, test_acc_base, losses, net_base = train_the_model(net_base, loss_func, optimizer)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(range(len(train_acc_base)), train_acc_base, 'o-', range(len(test_acc_base)), test_acc_base, 's-')\n",
        "plt.legend(['Train', 'Test'])\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGp4Yu9MhXSj"
      },
      "source": [
        "### SET ALL THE WEIGHTS OF LAYER 1 TO ZERO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFsQR9aAhXSj"
      },
      "outputs": [],
      "source": [
        "# Change the weights before training\n",
        "net_zero, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "# Set to zeros\n",
        "net_zero.fc1.weight.data = torch.zeros_like(net_zero.fc1.weight)\n",
        "\n",
        "# Run the model and show the results\n",
        "train_acc_zero, test_acc_zero, losses, net_zero = train_the_model(net_zero, loss_func, optimizer)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(range(len(train_acc_base)), train_acc_base, 'b-', range(len(test_acc_base)), test_acc_base, 'b:')\n",
        "plt.plot(range(len(train_acc_zero)), train_acc_zero, 'r-', range(len(test_acc_zero)), test_acc_zero, 'r:')\n",
        "plt.legend(['Train base', 'Test base', 'Train fc1=zero', 'Test fc1=zero'])\n",
        "plt.title('Accuracy comparison with layer FC1 init to zeros')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fAtpdVzhXSj"
      },
      "outputs": [],
      "source": [
        "# Are the weights still zeros?\n",
        "print(net_zero.fc1.weight.data)\n",
        "\n",
        "# Show the distributions in a histogram\n",
        "y, x = np.histogram(a=net_base.fc2.weight.data.flatten(), bins=30)\n",
        "plt.plot((x[1:] + x[:-1]) / 2, y, 'r', label='Baseline')\n",
        "\n",
        "y, x = np.histogram(a=net_zero.fc2.weight.data.flatten(), bins=30)\n",
        "plt.plot((x[1:] + x[:-1]) / 2, y, 'b', label='FC1=zeros')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Weight value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmqGFJJnhXSj"
      },
      "source": [
        "### ALL LEARNABLE PARAMETERS SET TO ZERO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9e92oM1hXSj"
      },
      "outputs": [],
      "source": [
        "# Change the weights before training\n",
        "net_all_zero, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "# Loop over parameters and set them all to zeros\n",
        "for p in net_all_zero.named_parameters():\n",
        "    p[1].data = torch.zeros_like(p[1].data)\n",
        "\n",
        "# And confirm for a few select parameters (y-axis offset for visibility)\n",
        "plt.plot(0 + net_all_zero.fc1.weight.data.flatten(), 'bo')\n",
        "plt.plot(1 + net_all_zero.fc2.weight.data.flatten(), 'rx')\n",
        "plt.plot(2 + net_all_zero.fc1.bias.data.flatten(), 'g^')\n",
        "plt.xlabel('Parameter index')\n",
        "plt.ylim([-1, 3])\n",
        "plt.ylabel('Parameter value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UcupKAYhXSj"
      },
      "outputs": [],
      "source": [
        "# Run the model and show the results\n",
        "train_acc_all_zero, test_acc_all_zero, losses, net_all_zero = train_the_model(net_all_zero, loss_func, optimizer)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(range(len(train_acc_base)), train_acc_base, 'b-', range(len(test_acc_base)), test_acc_base, 'b:')\n",
        "plt.plot(range(len(train_acc_all_zero)), train_acc_all_zero, 'r-', range(len(test_acc_all_zero)), test_acc_all_zero, 'r:')\n",
        "plt.legend(['Train base', 'Test base', 'Train all zero', 'Test all zero'])\n",
        "plt.title('Accuracy comparison with all layer init to zeros')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-6uZPAAhXSk"
      },
      "outputs": [],
      "source": [
        "# Show the distributions in a histogram\n",
        "y, x = np.histogram(a=net_base.fc1.weight.data.flatten(), bins=30)\n",
        "plt.plot((x[1:] + x[:-1]) / 2, y, 'r', label='Baseline')\n",
        "\n",
        "y, x = np.histogram(a=net_all_zero.fc1.weight.data.flatten(), bins=30)\n",
        "plt.plot((x[1:] + x[:-1]) / 2, y, 'b', label='All zeros')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Weight value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcZnj09MhXSk"
      },
      "source": [
        "### INITIALIZING TO 1'S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rL-gCCPhXSk"
      },
      "outputs": [],
      "source": [
        "# Change the weights before training\n",
        "net_all_one, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "# Loop over parameters and set them all to zeros\n",
        "for p in net_all_one.named_parameters():\n",
        "    p[1].data = torch.zeros_like(p[1].data) + 1\n",
        "\n",
        "# Run the model and show the results\n",
        "train_acc_all_one, test_acc_all_one, losses, net_all_one = train_the_model(net_all_one, loss_func, optimizer)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(range(len(train_acc_base)), train_acc_base, 'b-', range(len(test_acc_base)), test_acc_base, 'b:')\n",
        "plt.plot(range(len(train_acc_all_one)), train_acc_all_one, 'r-', range(len(test_acc_all_one)), test_acc_all_one, 'r:')\n",
        "plt.legend(['Train base', 'Test base', 'Train all ones', 'Test all ones'])\n",
        "plt.title('Accuracy comparison with all layer init to ones')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM916Uat9Z1O"
      },
      "source": [
        "## WEIGHTS_VARIANCE_INITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Hx2enu9Z1O"
      },
      "outputs": [],
      "source": [
        "# Range of Standard Deviations to use\n",
        "stdevs = np.logspace(start=np.log10(0.001), stop=np.log10(10), num=25)\n",
        "\n",
        "# Number of histogram bins for plotting distributions\n",
        "n_hist_bins = 80\n",
        "\n",
        "# Initialize results output matrix\n",
        "acc_results = np.zeros(len(stdevs))\n",
        "histo_data  = np.zeros((len(stdevs), 2, n_hist_bins))\n",
        "\n",
        "# Start the timer!\n",
        "start_time = time.process_time()\n",
        "\n",
        "# Start the experiment\n",
        "for stdev_idx, stdev_i in enumerate(stdevs):\n",
        "\n",
        "    # Create the network\n",
        "    net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "    # Set all parameters according to the standard deviation\n",
        "    for p in net.named_parameters():\n",
        "        p[1].data = torch.randn_like(p[1].data) * stdev_i\n",
        "    \n",
        "    # Train the model\n",
        "    train_acc, test_acc, losses, net = train_the_model(net, loss_func, optimizer)\n",
        "\n",
        "    # Get test accuracy on final 3 runs (averaging increases stability)\n",
        "    acc_results[stdev_idx] = np.mean(test_acc[-3:])\n",
        "\n",
        "    # Collect all parameters into one vector and compute its histogram\n",
        "    temp_params = np.array([])\n",
        "    for p in net.named_parameters():\n",
        "        temp_params = np.concatenate((temp_params, p[1].data.numpy().flatten()), axis=0)\n",
        "    \n",
        "    # Compute their histogram\n",
        "    y, x = np.histogram(temp_params, n_hist_bins)\n",
        "    histo_data[stdev_idx, 0, :] = (x[1:] + x[:-1]) / 2\n",
        "    histo_data[stdev_idx, 1, :] = y\n",
        "\n",
        "    # Status report\n",
        "    time_elapsed = time.process_time() - start_time\n",
        "    print(f'Finished {stdev_idx + 1}/{len(stdevs)} after {time_elapsed:3.0f}s. Model accuracy: {acc_results[stdev_idx]:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnQl9F-H9Z1O"
      },
      "outputs": [],
      "source": [
        "# Show the results\n",
        "plt.plot(stdevs, acc_results, 's-')\n",
        "plt.xlabel('Standard deviation for weight initializations')\n",
        "plt.ylabel('Final-3 test accuracy (ave %)')\n",
        "plt.xscale('log')\n",
        "plt.ylim([80, 100])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pifP7evt9Z1O"
      },
      "outputs": [],
      "source": [
        "# Show the weights distributions\n",
        "\n",
        "for i in range(len(stdevs)):\n",
        "    plt.plot(histo_data[i, 0, :], histo_data[i, 1, :], color=[1 - i / len(stdevs), 0.2, 1 - i / len(stdevs)])\n",
        "\n",
        "plt.xlabel('Weight value')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(np.round(stdevs, 4), bbox_to_anchor=(1, 1), loc='upper left')\n",
        "plt.xlim([-1, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbKkvJaIGsCw"
      },
      "source": [
        "## WEIGHTS_ XAVIER_KAIMING_INITS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P3E-bMfGsCw"
      },
      "source": [
        "### EXPLORE THE INITIALIZED WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QFngForGsCx"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "net = the_net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFTwVsVbGsCx"
      },
      "outputs": [],
      "source": [
        "# Collect all weights and biases\n",
        "all_weight = np.array([])\n",
        "all_biases = np.array([])\n",
        "\n",
        "for p in net.named_parameters():\n",
        "    if ('bias' in p[0]):\n",
        "        all_biases = np.concatenate((all_biases, p[1].data.numpy().flatten()), axis=0)\n",
        "    elif ('weight' in p[0]):\n",
        "        all_weight = np.concatenate((all_weight, p[1].data.numpy().flatten()), axis=0)\n",
        "    \n",
        "# How many?\n",
        "print('There are {:<10} bias parameters.'.format(len(all_biases)))\n",
        "print('There are {:<10} weight parameters.'.format(len(all_weight)))\n",
        "\n",
        "# Histograms\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "ax[0].hist(all_biases, 40)\n",
        "ax[0].set_title('Histogram of initial biases')\n",
        "\n",
        "ax[1].hist(all_weight, 40)\n",
        "ax[1].set_title('Histogram of initial weights')\n",
        "\n",
        "# Collect histogram data to show as line plots\n",
        "y_B, x_B = np.histogram(all_biases, bins=30)\n",
        "y_W, x_W = np.histogram(all_weight, bins=30)\n",
        "\n",
        "ax[2].plot(x_B[1:] + x_B[:-1] / 2, y_B / np.sum(y_B), label='Bias')\n",
        "ax[2].plot(x_W[1:] + x_W[:-1] / 2, y_W / np.sum(y_W), label='Weights')\n",
        "ax[2].set_title('Density estimate for both')\n",
        "ax[2].legend()\n",
        "ax[2].set_ylabel('Probability')\n",
        "\n",
        "# Plot adjustments common to all subplots\n",
        "for i in range(3):\n",
        "    ax[i].set_xlabel('Initial value')\n",
        "    ax[i].set_ylabel('Count')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkqf_1BGsCx"
      },
      "source": [
        "### LAYER-SPECIFIC DISTRIBUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC1Jod5_GsCx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
        "\n",
        "for p in net.named_parameters():\n",
        "\n",
        "    # Get the data and compute their histogram\n",
        "    params_data = p[1].data.numpy().flatten()\n",
        "    y, x = np.histogram(params_data, 10)\n",
        "\n",
        "    # For the bias\n",
        "    if ('bias' in p[0]):\n",
        "        ax[0].plot((x[1:] + x[:-1]) / 2, y / np.sum(y), label=f'{p[0][:-5]} bias ')\n",
        "    \n",
        "    # For the weight\n",
        "    elif ('weight' in p[0]):\n",
        "        ax[1].plot((x[1:] + x[:-1]) / 2, y / np.sum(y), label=f'{p[0][:-7]} weight ')\n",
        "    \n",
        "ax[0].set_title('Biases per layer')\n",
        "ax[0].legend()\n",
        "ax[1].set_title('Weights per layer')\n",
        "ax[1].legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1FRbbxtGsCy"
      },
      "outputs": [],
      "source": [
        "# Let's test whether the numbers match our prediction from the formula\n",
        "# Empirical bias range\n",
        "bias_range = [torch.min(net.fc1.bias.data).item(), torch.max(net.fc1.bias.data).item()]\n",
        "bias_count = len(net.fc1.bias.data)\n",
        "\n",
        "# Theoretical expected value\n",
        "sigma = np.sqrt(1 / bias_count)\n",
        "\n",
        "# Drum rolllllll.....\n",
        "print(f'Theoretical sigma = {sigma}')\n",
        "print(f'Empirical range   = {bias_range}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPcdUyWCGsCy"
      },
      "source": [
        "### INITIALIZE THE WEIGHTS USING THE XAVIER METHOD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4txyGx6GsCy"
      },
      "outputs": [],
      "source": [
        "# Create a new instance of the model\n",
        "net = the_net()\n",
        "\n",
        "# Change the weights (leave biases as Kaiming (default))\n",
        "for p in net.named_parameters():\n",
        "    if ('weight' in p[0]):\n",
        "        nn.init.xavier_normal_(tensor=p[1].data)\n",
        "\n",
        "# Test whether the numbers match our predictions from the formula\n",
        "# Empirical weight standard deviation\n",
        "weight_var   = torch.var(net.fc1.weight.data.flatten()).item()\n",
        "weight_count = len(net.fc1.weight.data)\n",
        "\n",
        "# Theoretical expected value\n",
        "sigma_2 = 2 / (weight_count + weight_count)\n",
        "\n",
        "print(f'Theoretical sigma = {sigma_2}')\n",
        "print(f'Empirical range   = {weight_var}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPwF4kVZGsCy"
      },
      "source": [
        "## WEIGHTS_ XAVIER_VS._KAIMING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6nxMtVMGsCy"
      },
      "outputs": [],
      "source": [
        "# Create a model\n",
        "wine_net_xavier = ANN_wine()\n",
        "\n",
        "# Change the weights (leave biases as Kaiming [default])\n",
        "for p in wine_net_xavier.named_parameters():\n",
        "    if ('weight' in p[0]):\n",
        "        nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "# Train the model and record its output\n",
        "train_acc_X, test_acc_X, losses_X = train_the_model(wine_net=wine_net_xavier)\n",
        "\n",
        "#==========================================\n",
        "\n",
        "# Create a model\n",
        "wine_net_kaiming = ANN_wine()\n",
        "\n",
        "# Change the weights (leave biases as Kaiming [default])\n",
        "for p in wine_net_kaiming.named_parameters():\n",
        "    if ('weight' in p[0]):\n",
        "        nn.init.kaiming_uniform_(p[1].data, nonlinearity='relu')\n",
        "\n",
        "# Train the model and record its output\n",
        "train_acc_K, test_acc_K, losses_K = train_the_model(wine_net=wine_net_kaiming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbUnrV7qGsCz"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "# Losses\n",
        "ax[0].plot(losses_X, label='Xavier')\n",
        "ax[0].plot(losses_K, label='Kaiming')\n",
        "ax[0].set_title('Loss')\n",
        "\n",
        "# Train accuracy\n",
        "ax[1].plot(train_acc_X, label='Xavier')\n",
        "ax[1].plot(train_acc_K, label='Kaiming')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title('TRAIN')\n",
        "\n",
        "# Test accuracy\n",
        "ax[2].plot(test_acc_X, label='Xavier')\n",
        "ax[2].plot(test_acc_K, label='Kaiming')\n",
        "ax[2].set_ylabel('Accuracy (%)')\n",
        "ax[2].set_title('TEST')\n",
        "\n",
        "for i in range(3):\n",
        "    ax[i].legend()\n",
        "    ax[i].grid('on')\n",
        "    ax[i].set_xlabel('Epochs')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf8mhQS7GsCz"
      },
      "source": [
        "### REPEAT THE EXPERIMENT TO GET MORE STABLE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCIsAZO1GsCz"
      },
      "outputs": [],
      "source": [
        "# Number of experiment iterations\n",
        "num_exps = 10\n",
        "\n",
        "\"\"\" Dimensions of results\n",
        "1 - Experiment run\n",
        "2 - Metric (loss/train/test)\n",
        "3 - Weight init (X/K)\n",
        "\"\"\"\n",
        "results = np.zeros((num_exps, 3, 2))\n",
        "\n",
        "for exp_idx in range(num_exps):\n",
        "    ## XAVIER\n",
        "    # Create a model\n",
        "    wine_net_xavier = ANN_wine()\n",
        "\n",
        "    # Change the weights (leave biases as Kaiming [default])\n",
        "    for p in wine_net_xavier.named_parameters():\n",
        "        if ('weight' in p[0]):\n",
        "            nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "    # Train the model and record its output\n",
        "    train_acc_X, test_acc_X, losses_X = train_the_model(wine_net=wine_net_xavier)\n",
        "\n",
        "    ## KAIMING\n",
        "    # Create a model\n",
        "    wine_net_kaiming = ANN_wine()\n",
        "\n",
        "    # Change the weights (leave biases as Kaiming [default])\n",
        "    for p in wine_net_kaiming.named_parameters():\n",
        "        if ('weight' in p[0]):\n",
        "            nn.init.kaiming_uniform_(p[1].data, nonlinearity='relu')\n",
        "\n",
        "    # Train the model and record its output\n",
        "    train_acc_K, test_acc_K, losses_K = train_the_model(wine_net=wine_net_kaiming)\n",
        "\n",
        "    ## Collect the results\n",
        "    results[exp_idx, 0, 0] = torch.mean(losses_X[-5:]).item()\n",
        "    results[exp_idx, 0, 1] = torch.mean(losses_K[-5:]).item()\n",
        "\n",
        "    results[exp_idx, 1, 0] = np.mean(train_acc_X[-5:])\n",
        "    results[exp_idx, 1, 1] = np.mean(train_acc_K[-5:])\n",
        "\n",
        "    results[exp_idx, 2, 0] = np.mean(test_acc_X[-5:])\n",
        "    results[exp_idx, 2, 1] = np.mean(test_acc_K[-5:])\n",
        "\n",
        "    print(f'Finished run {exp_idx} / {num_exps}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CUzfDXuGsC0"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot titles\n",
        "metric = ['Loss', 'Train acc.', 'Test acc.']\n",
        "\n",
        "for i in range(3):\n",
        "\n",
        "    # Plot the results\n",
        "    ax[i].plot(np.zeros(num_exps), results[:, i, 0], 'bo')\n",
        "    ax[i].plot(np.ones(num_exps),  results[:, i, 1], 'ro')\n",
        "\n",
        "    # Run a t-test to formalize the comparison\n",
        "    t, p = stats.ttest_ind(results[:, i, 0], results[:, i, 1])\n",
        "    title = f'{metric[i]}, (t = {t:.2f}, p = {p:.3f})'\n",
        "\n",
        "    # Make the plot nicer\n",
        "    ax[i].set_xlim([-1, 2])\n",
        "    ax[i].set_xticks([0, 1])\n",
        "    ax[i].set_xticklabels(['Xavier', 'Kaiming'])\n",
        "    ax[i].set_title(title)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2C6v7qjGsC0"
      },
      "source": [
        "## WEIGHTS_ IDENTICALLY_RANDOM_WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUQaanWPGsC0"
      },
      "outputs": [],
      "source": [
        "# Create a model\n",
        "net = nn.Sequential(\n",
        "    nn.Linear(2, 8),\n",
        "    nn.Linear(8, 1),\n",
        "    nn.Linear(1, 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRIjL1UbGsC0"
      },
      "source": [
        "### FOUR NETWORKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlW26ZNLGsC1"
      },
      "outputs": [],
      "source": [
        "## NO RANDOMSEED\n",
        "# Create a \"template\" network\n",
        "net_no_seed = copy.deepcopy(net)\n",
        "\n",
        "# Update the weights\n",
        "for p in net_no_seed.named_parameters():\n",
        "    if ('weight' in p[0]):\n",
        "        nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "## RANDOM SEED 1a\n",
        "# Create a \"template\" network\n",
        "net_rs1a = copy.deepcopy(net)\n",
        "\n",
        "# Set the seed to 1\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Update the weights\n",
        "for p in net_rs1a.named_parameters():\n",
        "    if ('weight' in p[0]):\n",
        "        nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "## RANDOM SEED 2\n",
        "# Create a \"template\" network\n",
        "net_rs2 = copy.deepcopy(net)\n",
        "\n",
        "# Set the seed to 1\n",
        "torch.manual_seed(2)\n",
        "\n",
        "# Update the weights\n",
        "for p in net_rs2.named_parameters():\n",
        "    if ('weight' in p[0]):\n",
        "        nn.init.xavier_normal_(p[1].data)\n",
        "\n",
        "## RANDOM SEED 1b\n",
        "# Create a \"template\" network\n",
        "net_rs1b = copy.deepcopy(net)\n",
        "\n",
        "# Set the seed to 1\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Update the weights\n",
        "for p in net_rs1b.named_parameters():\n",
        "    if ('weight' in p[0]):\n",
        "        nn.init.xavier_normal_(p[1].data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpMDN9AAGsC1"
      },
      "source": [
        "### EXTRACT ALL WEIGHTS FROM ALL NETWORKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnTkGOjTGsC2"
      },
      "outputs": [],
      "source": [
        "# Initialize empty arrays\n",
        "w_ns = np.array([])\n",
        "w_1a = np.array([])\n",
        "w_2  = np.array([])\n",
        "w_1b = np.array([])\n",
        "\n",
        "# Loop over layers in the models\n",
        "for i in range(3):\n",
        "    \n",
        "    # Extract the vectorized weights matrices\n",
        "    # .view(-1): Reshape a Pytorch matrix to vector\n",
        "    w_ns = np.append(arr=w_ns, values=net_no_seed[i].weight.view(-1).detach().numpy())\n",
        "    w_1a = np.append(arr=w_1a, values=net_rs1a[i].   weight.view(-1).detach().numpy())\n",
        "    w_2  = np.append(arr=w_2 , values=net_rs2[i].    weight.view(-1).detach().numpy())\n",
        "    w_1b = np.append(arr=w_1b, values=net_rs1b[i].   weight.view(-1).detach().numpy())\n",
        "\n",
        "# Plotting\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.plot(w_ns, 'ro', markersize=12, label='no seed')\n",
        "plt.plot(w_1a, 'ks', markersize=12, label='rs1a')\n",
        "plt.plot(w_2, ' m^', markersize=12, label='rs2')\n",
        "plt.plot(w_1b, 'g+', markersize=12, label='rs1b')\n",
        "plt.legend()\n",
        "plt.xlabel('Vectorized weight index')\n",
        "plt.ylabel('Weight value')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8QR0ub-GsC2"
      },
      "source": [
        "## WEIGHTS_FREEZE_WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvtE5EiYGsC2"
      },
      "outputs": [],
      "source": [
        "# Create the network\n",
        "net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "# Train the model\n",
        "train_acc, test_acc, losses, net = train_the_model(net, loss_func, optimizer)\n",
        "\n",
        "plt.plot(train_acc, label='Train')\n",
        "plt.plot(test_acc,  label='Test')\n",
        "plt.plot([len(train_acc) / 2, len(train_acc) / 2], [10, 80], 'k--', label='Learning switched on')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae-QBelIRQZY"
      },
      "source": [
        "## WEIGHTS_WEIGHTS_CHANGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YUqTg6nRQZY"
      },
      "outputs": [],
      "source": [
        "# Create the network\n",
        "net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "# Train the model\n",
        "train_acc, test_acc, losses, net, weight_change, weight_conds, pre_W = train_the_model(net, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jooeUh2zRQZY"
      },
      "outputs": [],
      "source": [
        "# Get a list of layer names\n",
        "layer_name = []\n",
        "for (i, p) in enumerate(net.named_parameters()):\n",
        "    if ('weight' in p[0]):\n",
        "        layer_name.append(p[0][:-7])\n",
        "\n",
        "# Set up the plot\n",
        "fig, ax = plt.subplots(1, 3, figsize=(16, 3))\n",
        "\n",
        "# Accuracy\n",
        "ax[0].plot(train_acc)\n",
        "ax[0].plot(test_acc)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy (%)')\n",
        "ax[0].set_title('Accuracy')\n",
        "ax[0].legend(['Train', 'Test'])\n",
        "\n",
        "# Weight changes\n",
        "ax[1].plot(weight_change)\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_title('Weight change from previous epoch')\n",
        "ax[1].legend(layer_name)\n",
        "\n",
        "# Weight condition numbers\n",
        "ax[2].plot(weight_conds)\n",
        "ax[2].set_xlabel('Epochs')\n",
        "ax[2].set_title('Condition number')\n",
        "ax[2].legend(layer_name)\n",
        "ax[2].set_ylim([0, 20])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdzaISXgRQZY"
      },
      "outputs": [],
      "source": [
        "# Final inspection: check the derivative of accuracy against the weight change\n",
        "# Normalize for scaling offsets\n",
        "from scipy.stats import zscore\n",
        "\n",
        "plt.plot(zscore(np.diff(train_acc)),             label='d(train_acc)')\n",
        "plt.plot(zscore(np.mean(weight_change, axis=1)), label='Weight change')\n",
        "plt.legend()\n",
        "plt.title('Change in weights by change in accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh2Leohnio0q"
      },
      "outputs": [],
      "source": [
        "# See what the model did\n",
        "fig, axs = plt.subplots(2, 5, figsize=(10, 3))\n",
        "\n",
        "for i in range(5):\n",
        "    axs[0, i].imshow(X    [i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[1, i].imshow(y_hat[i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[0, i].set_xticks([]), axs[0, i].set_yticks([])\n",
        "    axs[1, i].set_xticks([]), axs[1, i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Yikes!')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncp2qP4yio0q"
      },
      "source": [
        "## AUTOENCODER_DENOISING_MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcO7F_Vkio0q"
      },
      "outputs": [],
      "source": [
        "# Test the model with a bit of data\n",
        "net, loss_func, optimizer = create_the_MNIST_AE()\n",
        "\n",
        "X     = data_tensor[:5, :]\n",
        "y_hat = net(X)\n",
        "\n",
        "print(X.shape)\n",
        "print(y_hat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1EBuolcio0q"
      },
      "outputs": [],
      "source": [
        "# See what the model did\n",
        "fig, axs = plt.subplots(2, 5, figsize=(10, 3))\n",
        "\n",
        "for i in range(5):\n",
        "    axs[0, i].imshow(X    [i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[1, i].imshow(y_hat[i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[0, i].set_xticks([]), axs[0, i].set_yticks([])\n",
        "    axs[1, i].set_xticks([]), axs[1, i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Yikes!')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DROXkKu2io0r"
      },
      "source": [
        "### RUN THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km7JoNkbio0r"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "losses, net = train_the_model()\n",
        "print(f'Final loss: {losses[-1]:.4f}')\n",
        "\n",
        "# Visualize the losses\n",
        "plt.plot(losses, '.-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Model loss')\n",
        "plt.title('OK, but what dis it actually learn?')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRu5sqzXio0r"
      },
      "outputs": [],
      "source": [
        "# Repeat the visualization when testing the model\n",
        "X     = data_tensor[:5, :]\n",
        "y_hat = net(X)\n",
        "\n",
        "# See what the model did\n",
        "fig, axs = plt.subplots(2, 5, figsize=(10, 3))\n",
        "\n",
        "for i in range(5):\n",
        "    axs[0, i].imshow(X    [i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[1, i].imshow(y_hat[i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[0, i].set_xticks([]), axs[0, i].set_yticks([])\n",
        "    axs[1, i].set_xticks([]), axs[1, i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Disregard the Yikes!')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sGI9Hugio0r"
      },
      "source": [
        "### ADD NOISE TO SEE A USE CASE OF AN AUTOENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYBUN2x0io0r"
      },
      "outputs": [],
      "source": [
        "# Grab a small set of images\n",
        "X = data_tensor[:10, :]\n",
        "\n",
        "# Add noise\n",
        "X_noise = X + torch.rand_like(X) / 4\n",
        "\n",
        "# Clip at 1\n",
        "X_noise[X_noise > 1] = 1\n",
        "\n",
        "# Show the noisy image\n",
        "fig, axs = plt.subplots(2, 5, figsize=(10, 3))\n",
        "\n",
        "for i in range(5):\n",
        "    axs[0, i].imshow(X      [i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[1, i].imshow(X_noise[i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[0, i].set_xticks([]), axs[0, i].set_yticks([])\n",
        "    axs[1, i].set_xticks([]), axs[1, i].set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g-PS_dIio0s"
      },
      "outputs": [],
      "source": [
        "# Run through the model\n",
        "Y = net(X_noise)\n",
        "\n",
        "fig, axs = plt.subplots(3, 10, figsize=(12, 5))\n",
        "\n",
        "for i in range(10):\n",
        "    axs[0, i].imshow(X      [i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[1, i].imshow(X_noise[i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[2, i].imshow(Y[i, :]      .view(28, 28).detach(), cmap='gray')\n",
        "    axs[0, i].set_xticks([]), axs[0, i].set_yticks([])\n",
        "    axs[1, i].set_xticks([]), axs[1, i].set_yticks([])\n",
        "    axs[2, i].set_xticks([]), axs[2, i].set_yticks([])\n",
        "\n",
        "plt.suptitle('Neato.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOMXmKUDJ1wm"
      },
      "source": [
        "## AUTOENCODER_HOW_MANY_UNIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ8WS_a_J1wn"
      },
      "outputs": [],
      "source": [
        "# Specific the number of units\n",
        "N_encdec_units = np.linspace(start=10, stop=500, num=12).astype(int)\n",
        "N_bottle_units = np.linspace(start=5,  stop=100, num=8) .astype(int)\n",
        "\n",
        "# Initialize results matrix\n",
        "exp_results = np.zeros((len(N_encdec_units), len(N_bottle_units)))\n",
        "\n",
        "# Start the experiment\n",
        "for N_encdec_idx, N_encdec_i in enumerate(N_encdec_units):\n",
        "    for N_bottle_idx, N_bottle_i in enumerate(N_bottle_units):\n",
        "\n",
        "        # Build/Train the model\n",
        "        losses = train_the_model(N_encdec_i, N_bottle_i)[0] # Only need the first output\n",
        "        exp_results[N_encdec_idx, N_bottle_idx] = np.mean(losses[-1])\n",
        "\n",
        "        # Send update message\n",
        "        current_iter = N_encdec_idx * len(N_bottle_units) + N_bottle_idx + 1\n",
        "        total_iter   = len(N_bottle_units) * len(N_encdec_units)\n",
        "        msg          = 'Finished experiment {:<2}/{:<2}'.format(current_iter, total_iter)\n",
        "        sys.stdout.write('\\r' + msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSIKTKAVJ1wn"
      },
      "outputs": [],
      "source": [
        "# Show the result matrix\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "\n",
        "plt.imshow(exp_results, aspect='auto',              # Data and aspect ratio\n",
        "           vmin=0.01, vmax=0.04, cmap='Purples',    # Color range and palette\n",
        "           extent=[N_bottle_units[0], N_bottle_units[-1], N_encdec_units[-1], N_encdec_units[0]]) # xy axis ticks\n",
        "\n",
        "plt.xlabel('Number of bottleneck units')\n",
        "plt.ylabel('Number of encoder/decoder units')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFOW6VaMJ1wn"
      },
      "outputs": [],
      "source": [
        "plt.plot(N_encdec_units, exp_results)\n",
        "plt.legend(N_bottle_units, loc=(1.01, 0))\n",
        "plt.xlabel('Number of enc/dec units')\n",
        "plt.title('Loss by bottleneck units')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_QvL9bMKG2R"
      },
      "source": [
        "## AUTO_ENCODER_OCCLUSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz_4wdLdKHfv"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "losses, net = train_the_model()\n",
        "print(f'Final loss: {losses[-1]:.4f}')\n",
        "\n",
        "# Visualize the losses\n",
        "plt.plot(losses, '.-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Model loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0piDI0CLVBYj"
      },
      "source": [
        "### ADD OCCLUSION TO SOME IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZAa9zh2VBYk"
      },
      "outputs": [],
      "source": [
        "# Grab a small set of image\n",
        "X = copy.deepcopy(data_tensor[:10, :])\n",
        "\n",
        "# Add noise\n",
        "for i in range(X.shape[0]):\n",
        "\n",
        "    # Reshape the image\n",
        "    img = X[i, :].view(28, 28)\n",
        "\n",
        "    # Occlude random rows or columns\n",
        "    start_loc = np.random.choice(range(10, 21))\n",
        "\n",
        "    # Even -> Horizontal occlusion\n",
        "    if (i % 2 == 0):\n",
        "        img[start_loc:start_loc + 1, :] = 1\n",
        "    # Odd -> Vertical occlusion\n",
        "    else:\n",
        "        img[:, start_loc:start_loc + 1] = 1\n",
        "    \n",
        "# Run the samples through the model\n",
        "de_occluded = net(X)\n",
        "\n",
        "# Show the noisy images\n",
        "fig, axs = plt.subplots(3, 10, figsize=(15, 5))\n",
        "\n",
        "for i in range(10):\n",
        "    axs[0, i].imshow(data_tensor[i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[1, i].imshow(X          [i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[2, i].imshow(de_occluded[i, :].view(28, 28).detach(), cmap='gray')\n",
        "    axs[0, i].set_xticks([]), axs[0, i].set_yticks([])\n",
        "    axs[1, i].set_xticks([]), axs[1, i].set_yticks([])\n",
        "    axs[2, i].set_xticks([]), axs[2, i].set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCJv7JjKVBYk"
      },
      "source": [
        "### SOMETHING MORE QUANTITATIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlDio_P3VBYk"
      },
      "outputs": [],
      "source": [
        "# Quantify the performance of the \"de-occluder\" by correlating the sample with the original\n",
        "\n",
        "in_out_corr = np.corrcoef(data_tensor[9, :].detach(), de_occluded[9, :].detach())\n",
        "\n",
        "plt.plot(data_tensor[9, :].detach(), de_occluded[9, :].detach(), '.')\n",
        "plt.xlabel('Original pixel values')\n",
        "plt.ylabel('Reconstructed pixel values')\n",
        "plt.title(f'Correlation r = {in_out_corr[0, 1]:.3f}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuVPf_kKVBYk"
      },
      "outputs": [],
      "source": [
        "# Try again without the zero-valued pixels\n",
        "# Extract to variables for convenience\n",
        "orig  = data_tensor[9, :].detach()\n",
        "recon = de_occluded[9, :].detach()\n",
        "\n",
        "# Boolean vector that indicates pixels>0 (with some tolerance)\n",
        "tol             = 1e-4\n",
        "non_zero_pixels = (orig > tol) & (recon > tol)\n",
        "\n",
        "# Then re-compute the correlation\n",
        "in_out_corr = np.corrcoef(orig[non_zero_pixels], recon[non_zero_pixels])\n",
        "\n",
        "plt.plot(orig[non_zero_pixels], recon[non_zero_pixels], '.')\n",
        "plt.xlabel('Original pixel values')\n",
        "plt.ylabel('Reconstructed pixel values')\n",
        "plt.title(f'Correlation r = {in_out_corr[0, 1]:.3f}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr4sjallVBYk"
      },
      "outputs": [],
      "source": [
        "# Get data with no occlusion\n",
        "no_occlusion = net(data_tensor[:10, :])\n",
        "\n",
        "# Compare deoccluded-original to noocclusion-original correlation\n",
        "r = np.zeros((10, 2))\n",
        "for i in range(de_occluded.shape[0]):\n",
        "\n",
        "    # Pixel selection (note: tolerance defined in previous cell)\n",
        "    non_zero_pixels = (data_tensor[i, :] > tol) & (no_occlusion[i, :] > tol) & (de_occluded[i, :] > tol)\n",
        "\n",
        "    # Now compute the correlations\n",
        "    r[i, 0] = np.corrcoef(data_tensor[i, non_zero_pixels].detach(), no_occlusion[i, non_zero_pixels].detach())[0, 1]\n",
        "    r[i, 1] = np.corrcoef(data_tensor[i, non_zero_pixels].detach(), de_occluded [i, non_zero_pixels].detach())[0, 1]\n",
        "\n",
        "# Plot the correlation coefficients\n",
        "plt.plot(r, 'o', markersize=10)\n",
        "plt.legend(['No occlusion', 'Occlusion'])\n",
        "plt.xlabel('Sample number')\n",
        "plt.ylabel('Correlation with original')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pfmbSpI3Z8l"
      },
      "source": [
        "## AUTOENCODER_ LATENT_CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBSePMmX3Z8l"
      },
      "outputs": [],
      "source": [
        "# Test the model with a bit of data\n",
        "net, loss_func, optimizer = create_the_MNIST_AE()\n",
        "\n",
        "X     = data_tensor[:5, :]\n",
        "y_hat = net(X)\n",
        "\n",
        "print(f'Input shape: {X.shape}', '\\n')\n",
        "print(type(y_hat), len(y_hat), '\\n')\n",
        "print(f'Shape of model output: {y_hat[0].shape}', '\\n')\n",
        "print(f'Shape of encoding layer output: {y_hat[1].shape}', '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ5R0H3k3Z8l"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "losses, net = train_the_model()\n",
        "print(f'Final loss: {losses[-1]:.4f}')\n",
        "\n",
        "# Visualize the losses\n",
        "plt.plot(losses, '.-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Model loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qqiGKiz3Z8l"
      },
      "source": [
        "### INSPECT THE LATENT \"CODE\" OF THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amz3tzCu3Z8l"
      },
      "outputs": [],
      "source": [
        "# Output the latent layer\n",
        "# Push through the entire dataset\n",
        "y_hat, latent = net(data_tensor)\n",
        "\n",
        "# Print sizes\n",
        "print(f'{y_hat.shape}, {latent.shape}')\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax[0].hist(latent.flatten().detach(), 100)\n",
        "ax[0].set_xlabel('Latent Activation value')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_title('Distribution of latent units activation')\n",
        "\n",
        "ax[1].imshow(latent.detach(), aspect='auto', vmin=0, vmax=10)\n",
        "ax[1].set_xlabel('Latent node')\n",
        "ax[1].set_ylabel('Image number')\n",
        "ax[1].set_title('All latent activations')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YocqFNQ_fJxT"
      },
      "source": [
        "## CNN_CLASSIFY_GAUSSIAN_BLURS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANlplIFOfJxT"
      },
      "outputs": [],
      "source": [
        "# Visualize some images\n",
        "X, y = next(iter(test_loader))\n",
        "y_hat = net(X)\n",
        "\n",
        "fig, axs = plt.subplots(2, 10, figsize=(20, 4))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    G = torch.squeeze(X[i, 0, :, :]).detach()\n",
        "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
        "    t = (int(y[i].item()), int(y_hat[i].item() > 0))\n",
        "    ax.set_title(f'T: {t[0]}, P: {t[1]}')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sck9hxgCfJxT"
      },
      "outputs": [],
      "source": [
        "# Look at the filters\n",
        "print(net)\n",
        "\n",
        "layer_1W = net.enc[0].weight\n",
        "layer_3W = net.enc[3].weight\n",
        "\n",
        "print('')\n",
        "print(layer_1W.shape)\n",
        "print(layer_3W.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQEVNHJ7fJxU"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 6, figsize=(15, 3))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    ax.imshow(torch.squeeze(layer_1W[i, :, :, :]).detach(), cmap='Purples')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('First convolution layer filters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8se0A2CcfJxU"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(4, 6, figsize=(15, 9))\n",
        "\n",
        "for i in range(6 * 4):\n",
        "    idx = np.unravel_index(indices=i, shape=(4, 6))\n",
        "    axs[idx].imshow(torch.squeeze(layer_3W[idx[0], idx[1], :, :]).detach(), cmap='Purples')\n",
        "    axs[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Second convolution layer filters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeeP4CCpfJxU"
      },
      "source": [
        "## CNN_GAUSS_FEATURE_MAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj01QbCFfJxU"
      },
      "outputs": [],
      "source": [
        "# Visualize some images\n",
        "X, y                          = next(iter(test_loader))\n",
        "y_hat, feat_map_1, feat_map_2 = net(X)\n",
        "\n",
        "fig, axs = plt.subplots(2, 10, figsize=(20, 4))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    G = torch.squeeze(X[i, 0, :, :]).detach()\n",
        "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
        "    t = (int(y[i].item()), int(y_hat[i].item() > 0.5))\n",
        "    ax.set_title(f'T: {t[0]}, P: {t[1]}')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDi2fgvofJxU"
      },
      "source": [
        "### DRAW THE FEATURE MAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5QdXNE9fJxU"
      },
      "outputs": [],
      "source": [
        "# From the conv1 layer\n",
        "fig, axs = plt.subplots(7, 10, figsize=(12, 6))\n",
        "\n",
        "# Loop over 10 pictures (First row)\n",
        "for pic_i in range(10):\n",
        "    # Show the original picture. `0`: Only one Gray channel\n",
        "    img = X[pic_i, 0, :, :].detach()\n",
        "    axs[0, pic_i].imshow(img, cmap='jet', vmin=0, vmax=1)\n",
        "    axs[0, pic_i].axis('off')\n",
        "    axs[0, pic_i].text(2, 2, f'T: {int(y[pic_i].item())}', ha='left', va='top', color='w', fontweight='bold')\n",
        "    \n",
        "    # Loop over 06 feature maps (Each column except the first row)\n",
        "    for feat_i in range(6):\n",
        "        # Extract the feature map from this image\n",
        "        img = feat_map_1[pic_i, feat_i, :, :].detach()\n",
        "        axs[feat_i + 1, pic_i].imshow(img, cmap='inferno', vmin=0, vmax=torch.max(img) * 0.9)\n",
        "        axs[feat_i + 1, pic_i].axis('off')\n",
        "        axs[feat_i + 1, pic_i].text(-5, 45, feat_i, ha='right') if (pic_i == 0) else None\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('First set of feature map activations for 10 test images', x=0.5, y=1.01)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT4g17W0fJxU"
      },
      "outputs": [],
      "source": [
        "# From the conv2 layer\n",
        "fig, axs = plt.subplots(5, 10, figsize=(12, 6))\n",
        "\n",
        "# Loop over 10 pictures (First row)\n",
        "for pic_i in range(10):\n",
        "    # Show the original picture. `0`: Only one Gray channel\n",
        "    img = X[pic_i, 0, :, :].detach()\n",
        "    axs[0, pic_i].imshow(img, cmap='jet', vmin=0, vmax=1)\n",
        "    axs[0, pic_i].axis('off')\n",
        "    axs[0, pic_i].text(2, 2, f'T: {int(y[pic_i].item())}', ha='left', va='top', color='w', fontweight='bold')\n",
        "    \n",
        "    # Loop over 04 feature maps (Each column except the first row)\n",
        "    for feat_i in range(4):\n",
        "        # Extract the feature map from this image\n",
        "        img = feat_map_2[pic_i, feat_i, :, :].detach()\n",
        "        axs[feat_i + 1, pic_i].imshow(img, cmap='inferno', vmin=0, vmax=torch.max(img) * 0.9)\n",
        "        axs[feat_i + 1, pic_i].axis('off')\n",
        "        axs[feat_i + 1, pic_i].text(-5, 45, feat_i, ha='right') if (pic_i == 0) else None\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Second set of feature map activations for 10 test images', x=0.5, y=1.01)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASPzQS47fJxU"
      },
      "source": [
        "### SPATIAL CORRELATIONS ACROSS THE FEATURE MAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etlm32SIfJxU"
      },
      "outputs": [],
      "source": [
        "# Correlations across the SECOND convolution layer\n",
        "\n",
        "# Convenient variables\n",
        "n_stim = feat_map_2.shape[0]\n",
        "n_maps = feat_map_2.shape[1]\n",
        "n_cors = (n_maps * (n_maps - 1)) // 2\n",
        "\n",
        "# Initialze the matrix of all correlation values\n",
        "allrs = np.zeros((n_stim, n_cors))\n",
        "Call  = np.zeros((n_maps, n_maps))\n",
        "\n",
        "# Loop over each stimulus/image\n",
        "for i in range(n_stim):\n",
        "  \n",
        "  # Extract the vectorized feature maps from this image\n",
        "  # `.view(n_maps, -1)`: Reshape: Number of maps, total number of pixels with each feature maps\n",
        "  feat_map = feat_map_2[i, :, :, :].view(n_maps, -1).detach()\n",
        "  \n",
        "  # Compute the correlation matrix. Correlation of each feature map pairs of 1 image\n",
        "  C     = np.corrcoef(feat_map)\n",
        "  Call += C\n",
        "  \n",
        "  # Extract the unique correlations from the matrix\n",
        "  idx         = np.nonzero(np.triu(C, 1))\n",
        "  allrs[i, :] = C[idx]\n",
        "\n",
        "# Define the x-axis labels\n",
        "x_lab = [] * n_cors\n",
        "for i in range(n_cors):\n",
        "  x_lab.append(f'{idx[0][i]} - {idx[1][i]}')\n",
        "\n",
        "# Now visualize the correlations\n",
        "fig = plt.figure(figsize=(16, 5))\n",
        "ax0 = fig.add_axes([0.1, 0.1, 0.55, 0.9]) # [left, bottom, width, height]\n",
        "ax1 = fig.add_axes([0.68, 0.1, 0.3, 0.9])\n",
        "cax = fig.add_axes([0.98, 0.1, 0.01, 0.9])\n",
        "\n",
        "for i in range(n_cors):\n",
        "  ax0.plot(i + np.random.randn(n_stim) / 30, allrs[:, i], 'o', markerfacecolor='w', markersize=10)\n",
        "\n",
        "# Make the plot more interpretable\n",
        "ax0.set_xlim([-.5, n_cors - .5])\n",
        "ax0.set_ylim([-1.05, 1.05])\n",
        "ax0.set_xticks(range(n_cors))\n",
        "ax0.set_xticklabels(x_lab)\n",
        "ax0.set_xlabel('Feature map pair')\n",
        "ax0.set_ylabel('Correlation coefficient')\n",
        "ax0.set_title('Correlations for each image')\n",
        "\n",
        "# Now show the average correlation matrix\n",
        "h = ax1.imshow(Call / n_stim, vmin=-1, vmax=1)\n",
        "ax1.set_title('Correlation matrix')\n",
        "ax1.set_xlabel('Feature map')\n",
        "ax1.set_ylabel('Feature map')\n",
        "\n",
        "# Add a colorbar\n",
        "fig.colorbar(h, cax=cax)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz79NRO5m_Pc"
      },
      "source": [
        "## CNN_LINEAR_UNITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJb8fm5qnAAe"
      },
      "outputs": [],
      "source": [
        "# Number of hidden units\n",
        "number_of_linear_units = np.round(np.linspace(5, 500, 20))\n",
        "\n",
        "# Initialize results matrix\n",
        "results = np.zeros((len(number_of_linear_units), 4))\n",
        "\n",
        "for units_idx, units_i in enumerate(number_of_linear_units):\n",
        "    train_loss, test_loss, train_acc, test_acc, net = train_the_model(fc_units=int(units_i))\n",
        "    results[units_idx, :] = [train_loss[-1], test_loss[-1], train_acc[-1], test_acc[-1]] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hcwjPDFpDn9"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(number_of_linear_units, results[:, :2], 's-')\n",
        "ax[0].set_xlabel('Number of units in final linear layer')\n",
        "ax[0].set_ylabel('Loss (MSE)')\n",
        "ax[0].set_title('Final model loss')\n",
        "ax[0].legend(['Train', 'Test'])\n",
        "\n",
        "ax[1].plot(number_of_linear_units, results[:, 2:], 's-')\n",
        "ax[1].set_xlabel('Number of units in final linear layer')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title('Final model test accuracy')\n",
        "ax[1].legend(['Train', 'Test'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiPvZGGhhvbO"
      },
      "source": [
        "## CNN_GAUSS_AUTOENCODER|GAUSS_AE_OCCLUSION|CUSTOM_LOSS_FUNC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spAXZKE7hvbO"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses, 's-', label='Train')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title(f'Model loss (Final loss = {losses[-1]:.3f})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cclMdDKLhvbO"
      },
      "outputs": [],
      "source": [
        "pics_2_use = np.random.choice(n_gauss, size=32, replace=False)\n",
        "X          = images[pics_2_use, :, :, :]\n",
        "# X          = images_occ[pics_2_use, :, :, :]\n",
        "y_hat      = net(X)\n",
        "\n",
        "fig, axs = plt.subplots(2, 10, figsize=(18, 4))\n",
        "for i in range(10):\n",
        "    G = torch.squeeze(X    [i, 0, :, :]).detach()\n",
        "    O = torch.squeeze(y_hat[i, 0, :, :]).detach()\n",
        "    \n",
        "    axs[0, i].imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
        "    axs[0, i].axis('off')\n",
        "    axs[0, i].set_title('Model input')\n",
        "    \n",
        "    axs[1, i].imshow(O, vmin=-1, vmax=1, cmap='jet')\n",
        "    axs[1, i].axis('off')\n",
        "    axs[1, i].set_title('Model output')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llsWq5hxhvbO"
      },
      "source": [
        "## CNN_FIND_GAUSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8-QT3ijhvbO"
      },
      "outputs": [],
      "source": [
        "# Visualize some images\n",
        "X, Y  = next(iter(test_loader))\n",
        "y_hat = net(X)\n",
        "\n",
        "fig, axs = plt.subplots(2, 10, figsize=(16, 4))\n",
        "theta    = np.linspace(start=0, stop=2 * np.pi)\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    # Get the Gaussian and draw it, and draw the white-guide-line\n",
        "    G = torch.squeeze(X[i, 0, :, :]).detach()\n",
        "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet', extent=[-4, 4, -4, 4], origin='lower')\n",
        "    ax.plot([-4, 4], [0, 0], 'w--')\n",
        "    ax.plot([0, 0], [-4, 4], 'w--')\n",
        "    \n",
        "    # Compute the model's prediction\n",
        "    cx = y_hat[i][0].item() # Center X\n",
        "    cy = y_hat[i][1].item() # Center Y\n",
        "    rd = y_hat[i][2].item() # Radius\n",
        "    \n",
        "    # Draw it\n",
        "    x = cx + np.cos(theta) * np.sqrt(rd)\n",
        "    y = cy + np.sin(theta) * np.sqrt(rd)\n",
        "    ax.plot(x,  y,  'b')\n",
        "    ax.plot(cx, cy, 'bo')\n",
        "    \n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlim([-4, 4])\n",
        "    ax.set_ylim([-4, 4])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zC4Un95hvbO"
      },
      "outputs": [],
      "source": [
        "fig         = plt.figure(figsize=(5, 5))\n",
        "param_names = ['Cx', 'Cy', 'rad.']\n",
        "\n",
        "for i in range(3):\n",
        "    # Extract parameters and compute correlation\n",
        "    yy = Y    [:, i].detach()\n",
        "    yh = y_hat[:, i].detach()\n",
        "    cr = np.corrcoef(yy, yh)[0, 1]\n",
        "    \n",
        "    plt.plot(yy, yh, 'o', label=f'{param_names[i]}, r = {cr:.3f}')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('True values')\n",
        "plt.ylabel('Predicted values')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN_EMNIST"
      ],
      "metadata": {
        "id": "U2dWQq6DvPJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some images\n",
        "\n",
        "# Extract X, y from test dataloader\n",
        "X, y = next(iter(test_loader))\n",
        "# Push data to GPU\n",
        "X     = X.to(device)\n",
        "y     = y.to(device)\n",
        "y_hat = net(X)\n",
        "\n",
        "# Pick some examples at random to show\n",
        "rand_idx = np.random.choice(len(y), size=21, replace=False)\n",
        "\n",
        "fig, axs = plt.subplots(3, 7, figsize=(15, 6))\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    # Extract the image and its target letter\n",
        "    I           = np.squeeze(X[rand_idx[i], 0, :, :]).cpu()\n",
        "    true_letter = letter_categories[y[rand_idx[i]]]\n",
        "    pred_letter = letter_categories[torch.argmax(y_hat[rand_idx[i], :])]\n",
        "\n",
        "    # Color-code the accuracy\n",
        "    col = 'gray' if (true_letter == pred_letter) else 'hot'\n",
        "\n",
        "    # Visualize\n",
        "    ax.imshow(I.T, cmap=col)\n",
        "    ax.set_title(f'True: {true_letter}|Predicted: {pred_letter}', fontsize=10)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e-dyly-6vQFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCURACY BY LETTER\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "# Confusion matrix\n",
        "C = skm.confusion_matrix(y_true=y.cpu(), y_pred=torch.argmax(y_hat.cpu(), axis=1), normalize='true')\n",
        "\n",
        "# Visualize\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.imshow(C, 'Blues', vmax=0.05)\n",
        "\n",
        "plt.xticks(range(26), labels=letter_categories)\n",
        "plt.yticks(range(26), labels=letter_categories)\n",
        "plt.title('TEST confusion matrix')\n",
        "plt.ylabel('True number')\n",
        "plt.xlabel('Predicted number')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6e72KMIJxgMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN_ NUM_CHANS"
      ],
      "metadata": {
        "id": "0eL1gBVvBXsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "conv_chans = [2, 5, 8]\n",
        "\n",
        "# Initialize results matrix\n",
        "results     = np.zeros((len(conv_chans), len(conv_chans), 2))\n",
        "conv_params = np.zeros((len(conv_chans), len(conv_chans)))\n",
        "\n",
        "for i, n_chan_i in enumerate(conv_chans):\n",
        "    for j, n_chan_j in enumerate(conv_chans):\n",
        "        train_loss, test_loss, train_err, test_err, net = train_the_model(num_chans=(n_chan_i, n_chan_j))\n",
        "\n",
        "        results    [i, j, :] = train_err[-1], test_err[-1]\n",
        "        conv_params[i, j]    = n_chan_i + n_chan_j # Total number of conv layer channels\n",
        "\n",
        "        print(i, j)      "
      ],
      "metadata": {
        "id": "dQK1tzsbBYkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results matrix\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "for i in range(2):\n",
        "    h = ax[i].imshow(results[:, :, i], vmin=np.min(results), vmax=np.max(results))\n",
        "    ax[i].set_xlabel('Channels in conv1')\n",
        "    ax[i].set_ylabel('Channels in conv2')\n",
        "    ax[i].set_xticks(range(j + 1))\n",
        "    ax[i].set_yticks(range(j + 1))\n",
        "    ax[i].set_xticklabels(conv_chans)\n",
        "    ax[i].set_yticklabels(conv_chans)\n",
        "    title = 'Train' if (i == 0) else 'Test'\n",
        "    ax[i].set_title(f'Error rates {title}', fontweight='bold')\n",
        "\n",
        "# Add a colorbar right of the plot\n",
        "axpos = ax[1].get_position()\n",
        "cax = fig.add_axes([axpos.x1+.01,axpos.y0,.01,.75])\n",
        "hh = fig.colorbar(h,cax=cax)\n",
        "hh.set_label('Error rate (%)',rotation=270,labelpad=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "87tVGXFEC09o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error rate as a function of the total number of conv channels\n",
        "corr_train = np.corrcoef(conv_params.flatten(), results[:, :, 0].flatten())\n",
        "corr_test  = np.corrcoef(conv_params.flatten(), results[:, :, 1].flatten())\n",
        "\n",
        "plt.plot(conv_params.flatten(), results[:, :, 0].flatten(), 'o',\n",
        "         label=f'Train (r = {corr_train[0, 1]:.2f})')\n",
        "plt.plot(conv_params.flatten(), results[:, :, 1].flatten(), 's',\n",
        "         label=f'Test  (r = {corr_test [0, 1]:.2f})')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Total number of convolution channels')\n",
        "plt.ylabel('Error rate (%)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N7F5TaUEFGcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFER_ FMNIST"
      ],
      "metadata": {
        "id": "9_m11LgBqpir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model\n",
        "number_net, loss_func, optimizer = create_the_MNIST_net()\n",
        "\n",
        "# Train it on numbers data\n",
        "train_acc, test_acc, losses, number_net = train_the_model(net=number_net, train_loader=numbers_train_loader, test_loader=numbers_test_loader, num_epochs=5)"
      ],
      "metadata": {
        "id": "WHJcQ9l4qq0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with fashion data\n",
        "# Extract X, y from FASHION test dataloader\n",
        "X, y  = next(iter(fashion_test_loader))\n",
        "y_hat = number_net(X)\n",
        "\n",
        "# The test\n",
        "fashion_acc = 100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float())\n",
        "print(f'NUMBER_NET performance on FASHION data: {fashion_acc:.2f}%')"
      ],
      "metadata": {
        "id": "G_jxmO6lrzk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## NOTE about this cell: I've added print() statements in here to show that the copying works.\n",
        "## The first print function will show a non-zeros matrix because the weights of the two models\n",
        "## differ. The second print function shows the zeros matrix because the two models have identical\n",
        "## weights. See Q&A for this lecture.\n",
        "\n",
        "# Create the target model\n",
        "fashion_net, loss_func, optimizer = create_the_MNIST_net()\n",
        "print(fashion_net.conv1.weight[0] - number_net.conv1.weight[0])\n",
        "\n",
        "# Replace all the weights in TARGET model from SOURCE model\n",
        "for target, source in zip(fashion_net.named_parameters(), number_net.named_parameters()):\n",
        "    target[1].data = copy.deepcopy(source[1].data)\n",
        "\n",
        "print(fashion_net.conv1.weight[0] - number_net.conv1.weight[0])"
      ],
      "metadata": {
        "id": "xx3lfxx1smCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now re-train the network on the numbers data\n",
        "train_acc, test_acc, losses, fashion_net = train_the_model(net=fashion_net, train_loader=fashion_train_loader, test_loader=fashion_test_loader, num_epochs=1)\n"
      ],
      "metadata": {
        "id": "dO4oZdTrt5Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFER_LETTER2NUMBER"
      ],
      "metadata": {
        "id": "GhOIukSB0G38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "letter_net, loss_func, optimizer = make_the_net()\n",
        "train_loss, test_loss, train_err, test_err, net = train_the_model(net=letter_net, optimizer=optimizer, train_loader=letter_train_loader, test_loader=letter_test_loader, num_epochs=5)"
      ],
      "metadata": {
        "id": "J-7R8G9t0GX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TEST THE MODEL ON THE NUMBER DATA\n",
        "# Extract X, y from NUMBER test dataloader\n",
        "X, y = next(iter(number_test_loader))\n",
        "X    = X.to(device)\n",
        "y    = y.to(device)\n",
        "\n",
        "letter_net.eval()\n",
        "y_hat = letter_net(X)\n",
        "\n",
        "# Test\n",
        "number_acc = 100 * torch.mean((torch.argmax(y_hat, axis=1) != y).float())\n",
        "print(f'number_net error rate on NUMBER data: {number_acc:.2f}%')"
      ],
      "metadata": {
        "id": "5HPEj9fG05za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Fine-tune the model with one training batch\n",
        "# Create the target model\n",
        "number_net, loss_func, optimizer = make_the_net()\n",
        "\n",
        "# Replace all the weights in TARGET model from SOURCE model\n",
        "for target, source in zip(number_net.named_parameters(), letter_net.named_parameters()):\n",
        "    target[1].data = copy.deepcopy(source[1].data)\n",
        "\n",
        "# Check out the network\n",
        "print(number_net, '\\n')\n",
        "# The final layer\n",
        "print(number_net.fc2, '\\n')\n",
        "\n",
        "# Replace the final layer to have 10 outputs instead of 26\n",
        "number_net.fc2 = nn.Linear(50, 10)\n",
        "# Check again\n",
        "print(number_net)"
      ],
      "metadata": {
        "id": "e1XWnELe1jPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Re-train the network on the numbers data\n",
        "train_loss, test_loss, train_err, test_err, number_net = train_the_model(net=number_net, optimizer=optimizer, train_loader=number_train_loader, test_loader=number_test_loader, num_epochs=1)"
      ],
      "metadata": {
        "id": "zx1kI28n3GEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Try again, only train the output layer\n",
        "# Create the target model\n",
        "number_net_2, loss_func, optimizer = make_the_net()\n",
        "\n",
        "# Replace all the weights in TARGET model from SOURCE model\n",
        "for target, source in zip(number_net_2.named_parameters(), letter_net.named_parameters()):\n",
        "    target[1].data = copy.deepcopy(source[1].data)\n",
        "\n",
        "# Replace the final layer to have 10 outputs instead of 26\n",
        "number_net_2.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "# Freeze all layers except output\n",
        "for p in number_net_2.named_parameters():\n",
        "    if (not 'fc2' in p[0]):\n",
        "        p[1].requires_grad = False\n",
        "\n",
        "## Re-train the network on the numbers data\n",
        "train_loss, test_loss, train_err, test_err, number_net2 = train_the_model(net=number_net2, optimizer=optimizer, train_loader=number_train_loader, test_loader=number_test_loader, num_epochs=1)\n",
        "\n",
        "## Try again, only train the output layer\n",
        "# Create the target model\n",
        "number_net_2, loss_func, optimizer = make_the_net()\n",
        "\n",
        "# Replace all the weights in TARGET model from SOURCE model\n",
        "for target, source in zip(number_net_2.named_parameters(), letter_net.named_parameters()):\n",
        "    target[1].data = copy.deepcopy(source[1].data)\n",
        "\n",
        "# Replace the final layer to have 10 outputs instead of 26\n",
        "number_net_2.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "# Freeze all layers except output\n",
        "for p in number_net_2.named_parameters():\n",
        "    if (not 'fc2' in p[0]):\n",
        "        p[1].requires_grad = False\n",
        "\n",
        "## Re-train the network on the numbers data\n",
        "train_loss, test_loss, train_err, test_err, number_net2 = train_the_model(net=number_net2, optimizer=optimizer, train_loader=number_train_loader, test_loader=number_test_loader, num_epochs=1)\n",
        "\n",
        "print(f'number_net TRAIN error rate: {train_err[-1]:.2f}%')\n",
        "print(f'number_net TEST  error rate: {test_err [-1]:.2f}%')"
      ],
      "metadata": {
        "id": "Dd7xTXqY4Wix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFER_ RESNET"
      ],
      "metadata": {
        "id": "2F7IGUv7YPQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for p in resnet.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Change the final layer\n",
        "resnet.fc = nn.Linear(512, 10)\n",
        "\n",
        "# Push the model to the GPU\n",
        "resnet.to(device);"
      ],
      "metadata": {
        "id": "RFAweU3uYP_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAIN THE MODEL\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "DV420wx5Z0oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect a few random images\n",
        "X, y = next(iter(test_loader))\n",
        "X = X.to(device)\n",
        "y = y.to(device)\n",
        "resnet.eval()\n",
        "predictions = torch.argmax(resnet(X), axis=1)\n",
        "\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "for (i, ax) in enumerate(axs.flatten()):\n",
        "    # Extract image\n",
        "    pic = X.data[i].cpu().numpy().transpose((1, 2, 0))\n",
        "    pic = pic - np.min(pic)\n",
        "    pic = pic / np.max(pic)\n",
        "\n",
        "    ax.imshow(pic)\n",
        "\n",
        "    label = train_data_set.classes[predictions[i]]\n",
        "    truec = train_data_set.classes[y[i]]\n",
        "    title = f'Pred: {label} - True: {truec}'\n",
        "\n",
        "    # Set the title with color-coded accuracy\n",
        "    title_color = 'g' if (truec == label) else 'r'\n",
        "    ax.text(44, 90, title, ha='center', va='top', fontweight='bold', color='k', backgroundcolor=title_color, fontsize=8)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SIuVNcw2dMp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSFER_ PRETRAIN_FMNIST"
      ],
      "metadata": {
        "id": "A37tDjdIoa1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Show some random examples\n",
        "\n",
        "# Get some data\n",
        "X, y = next(iter(dev_loader))\n",
        "\n",
        "# Forward pass and loss\n",
        "ae_net.cpu()\n",
        "ae_net.eval() # Switch to test mose\n",
        "y_hat = ae_net(X)\n",
        "\n",
        "fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n",
        "for i in range(10):\n",
        "    pic = y_hat[i, 0, :, :].detach()\n",
        "    pic = pic / 2 + 0.5 # Undo normalization\n",
        "    axs[0, i].imshow(pic, cmap='gray')\n",
        "    axs[0, i].axis('off')\n",
        "\n",
        "    pic = X[i, 0, :, :].detach()\n",
        "    pic = pic / 2 + 0.5 # Undo normalization\n",
        "    axs[1, i].imshow(pic, cmap='gray')\n",
        "    axs[1, i].axis('off')\n",
        "\n",
        "    if (i == 0):\n",
        "        axs[0, 0].text(-6, 14, 'Reconstructed', rotation=90, va='center')\n",
        "        axs[1, 0].text(-6, 14, 'Original',      rotation=90, va='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v8LbbFisob3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_train_net, loss_func, optimizer = make_the_class_net()\n",
        "### Note about the code below: Both networks have the same number of layers overall; in other applications\n",
        "#    you may need to modify the code to find the matching layers.\n",
        "# then replace the conv weights in TARGET model from encoder weights in SOURCE model\n",
        "for target, source in zip(pre_train_net.named_parameters(), ae_net.named_parameters()):\n",
        "    print(f'PRETRAIN: {target[0]}, AENET: {source[0]}')\n",
        "    if ('enc' in target[0]):\n",
        "        target[1].data = copy.deepcopy(source[1].data)"
      ],
      "metadata": {
        "id": "w2qADs6RwPT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN THE PRETRAINED MODEL\n",
        "train_loss_pre, dev_loss_pre, train_acc_pre, dev_acc_pre, pre_train_net = train_the_class_model(pre_train_net, loss_func, optimizer)\n",
        "\n",
        "# Evaluate on the test set\n",
        "pre_train_net.eval() # Switch to test mode\n",
        "X, y = next(iter(test_loader))\n",
        "\n",
        "# Push data to GPU\n",
        "X = X.to(device)\n",
        "y = y.to(device)\n",
        "\n",
        "# Forward pass and loss\n",
        "with torch.no_grad():\n",
        "    y_hat = pre_train_net(X)\n",
        "    loss  = loss_func(y_hat, y)\n",
        "\n",
        "test_loss_pre = loss.item()\n",
        "test_acc_pre  = 100 * torch.mean((torch.argmax(y_hat, axis=1) == y).float()).item()\n"
      ],
      "metadata": {
        "id": "V-nWE2Jrx73S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(train_loss_pre, 'rs-',    label='PRE Train')\n",
        "ax[0].plot(dev_loss_pre,   'ro--',   label='PRE Dev')\n",
        "ax[0].plot(len(dev_loss_pre) - 1, test_loss_pre, 'rp', markersize=15,    label='PRE Test')\n",
        "\n",
        "ax[0].plot(train_loss_naive, 'bs-',  label='NAIVE Train')\n",
        "ax[0].plot(dev_loss_naive,   'bo--', label='NAIVE Dev')\n",
        "ax[0].plot(len(dev_loss_naive) - 1, test_loss_naive, 'b*', markersize=15, label='NAIVE Test')\n",
        "\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_acc_pre, 'rs-',    label='PRE Train')\n",
        "ax[1].plot(dev_acc_pre,   'ro--',   label='PRE Dev')\n",
        "ax[1].plot(len(dev_acc_pre) - 1, test_acc_pre, 'rp', markersize=15,    label='PRE Test')\n",
        "\n",
        "ax[1].plot(train_acc_naive, 'bs-',  label='NAIVE Train')\n",
        "ax[1].plot(dev_acc_naive,   'bo--', label='NAIVE Dev')\n",
        "ax[1].plot(len(dev_acc_naive) - 1, test_acc_naive, 'b*', markersize=15, label='NAIVE Test')\n",
        "\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title(f'Final NaiveTest/PreTest accuracy: {test_acc_naive:.2f}%/{test_acc_pre:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LBS7DovkyuQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab one image\n",
        "x = X[10,:,:,:].view(1,1,28,28)\n",
        "\n",
        "# compute the activations of the first layer (excluding the bias b/c this is simply a constant)\n",
        "layer1ActPre = F.relu( F.conv2d(x,pretrainNet.encconv1.weight) )\n",
        "layer1ActNai = F.relu( F.conv2d(x,naivenet.encconv1.weight) )\n",
        "\n",
        "\n",
        "\n",
        "## show the feature map activations for the pretrained model\n",
        "fig,axs = plt.subplots(2,8,figsize=(14,4))\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "  act = torch.squeeze(layer1ActPre[0,i,:,:]).detach().cpu()\n",
        "  ax.imshow(act,cmap='gray')\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.suptitle('Pretrained activations',y=.9)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "## show the feature map activations for the naive model\n",
        "fig,axs = plt.subplots(2,8,figsize=(14,4))\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "  act = torch.squeeze(layer1ActNai[0,i,:,:]).detach().cpu()\n",
        "  ax.imshow(act,cmap='gray')\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.suptitle('Naive activations',y=.9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qt7k_0a20jDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN_MNIST"
      ],
      "metadata": {
        "id": "v3CDb5_OhlV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function (Same for both phases of training)\n",
        "loss_func = nn.BCELoss()\n",
        "\n",
        "# Create instance of the models\n",
        "d_net = discriminator_net().to(device)\n",
        "g_net = generator_net()    .to(device)\n",
        "\n",
        "# Optimizers (Same algo but different variables b/c different parameters)\n",
        "d_optimizer = torch.optim.Adam(d_net.parameters(), lr=0.0003)\n",
        "g_optimizer = torch.optim.Adam(g_net.parameters(), lr=0.0003)"
      ],
      "metadata": {
        "id": "Tk4pMPDLhl6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAINING ##\n",
        "num_epochs = 50000\n",
        "\n",
        "losses                  = np.zeros((num_epochs, 2))\n",
        "discriminator_decisions = np.zeros((num_epochs, 2))\n",
        "\n",
        "for epoch_i in range(num_epochs):\n",
        "\n",
        "    # Create minibatches of REAL and FAKE images\n",
        "    rand_idx    = torch.randint(data_T.shape[0], (batch_size, ))\n",
        "    real_images = data_T[rand_idx, :]               .to(device)\n",
        "    fake_images = g_net(torch.randn(batch_size, 64) .to(device)) # Output of generator\n",
        "\n",
        "    # Labels used for REAL and FAKE images\n",
        "    real_labels = torch.ones (batch_size, 1).to(device)\n",
        "    fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "    \n",
        "    ########################### Train the Discriminator ########################\n",
        "    # Forward pass and loss for REAL images\n",
        "    pred_real   = d_net(real_images)                # REAL images into Discriminator\n",
        "    d_loss_real = loss_func(pred_real, real_labels) # All labels are 1\n",
        "\n",
        "    # Forward pass and loss for FAKE images\n",
        "    pred_fake   = d_net(fake_images)                # FAKE images into Discriminator\n",
        "    d_loss_fake = loss_func(pred_fake, fake_labels) # All labels are 0\n",
        "\n",
        "    # Collect loss (Using combined loss)\n",
        "    d_loss                              = d_loss_real + d_loss_fake\n",
        "    losses                 [epoch_i, 0] = d_loss.item()\n",
        "    discriminator_decisions[epoch_i, 0] = torch.mean((pred_real > 0.5).float()).detach()\n",
        "\n",
        "    # Backprop\n",
        "    d_optimizer.zero_grad()\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    \n",
        "    ############################ Train the Generator ###########################\n",
        "    # Create fake images and compute loss\n",
        "    fake_images = g_net(torch.randn(batch_size, 64).to(device))\n",
        "    pred_fake   = d_net(fake_images)\n",
        "\n",
        "    # Compute and collect loss and accuracy\n",
        "    g_loss = loss_func(pred_fake, real_labels)\n",
        "    losses                 [epoch_i, 1] = g_loss.item()\n",
        "    discriminator_decisions[epoch_i, 1] = torch.mean((pred_fake > 0.5).float()).detach()\n",
        "\n",
        "    # Backprop\n",
        "    g_optimizer.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    ############################################################################\n",
        "    # Print out a status message\n",
        "    if ((epoch_i + 1) % 500 == 0):\n",
        "        msg = 'Finished epoch {:<5}/{:<5}'.format(epoch_i + 1, num_epochs)\n",
        "        sys.stdout.write('\\r' + msg)"
      ],
      "metadata": {
        "id": "fLRxFaABiaCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model Loss')\n",
        "ax[0].legend(['Discrimator', 'Generator'])\n",
        "ax[0].set_xlim([4000, 5000])\n",
        "\n",
        "ax[1].plot(losses[::5, 0], losses[::5, 1], 'k.', alpha=0.1)\n",
        "ax[1].set_xlabel('Discriminator Loss')\n",
        "ax[1].set_ylabel('Generator Loss')\n",
        "\n",
        "ax[2].plot(discriminator_decisions)\n",
        "ax[2].set_xlabel('Epochs')\n",
        "ax[2].set_ylabel('Probability (\"Real\")')\n",
        "ax[2].set_title('Discriminator Output')\n",
        "ax[2].legend(['Real', 'Fake'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dEjEtQ2LoWTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## See some fake digits\n",
        "\n",
        "# Generage the images from the generator network\n",
        "g_net.eval()\n",
        "fake_data = g_net(torch.randn(12, 64).to(device)).cpu()\n",
        "\n",
        "fig, axs = plt.subplots(3, 4, figsize=(8, 6))\n",
        "for (i, ax) in enumerate(axs.flatten()):\n",
        "    ax.imshow(fake_data[i, :].detach().view(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eWJwYc3yq6s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN_CNN_GAUSS"
      ],
      "metadata": {
        "id": "yPV3UM7T0O43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function (Same for both phases of training)\n",
        "loss_func = nn.BCELoss()\n",
        "\n",
        "# Create instance of the models\n",
        "d_net = discriminator_net().to(device)\n",
        "g_net = generator_net()    .to(device)\n",
        "\n",
        "# Optimizers (Same algo but different variables b/c different parameters)\n",
        "d_optimizer = torch.optim.Adam(d_net.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "g_optimizer = torch.optim.Adam(g_net.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "ZyXWiuCF0ecj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAINING ##\n",
        "num_epochs              = 1500\n",
        "batch_size              = 86\n",
        "losses                  = []\n",
        "discriminator_decisions = []\n",
        "\n",
        "for epoch_i in range(num_epochs):\n",
        "\n",
        "    # Create minibatches of REAL and FAKE images\n",
        "    rand_idx = torch.randint(images.shape[0], (batch_size, ))\n",
        "    data     = images[rand_idx, :]               .to(device)\n",
        "\n",
        "    # Labels used for REAL and FAKE images\n",
        "    real_labels = torch.ones (batch_size, 1).to(device)\n",
        "    fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "    \n",
        "    ########################### Train the Discriminator ########################\n",
        "    # Forward pass and loss for REAL images\n",
        "    pred_real   = d_net(data)                       # REAL images into Discriminator\n",
        "    d_loss_real = loss_func(pred_real, real_labels) # All labels are 1\n",
        "\n",
        "    # Forward pass and loss for FAKE images\n",
        "    fake_data   = torch.randn(batch_size, 100, 1, 1).to(device)\n",
        "    fake_images = g_net(fake_data)                  # Output of generator\n",
        "    pred_fake   = d_net(data)                       # FAKE images into Discriminator\n",
        "    d_loss_fake = loss_func(pred_fake, fake_labels) # All labels are 0\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    # Backprop\n",
        "    d_optimizer.zero_grad()\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "    \n",
        "    ############################ Train the Generator ###########################\n",
        "    # Create fake images and compute loss\n",
        "    fake_images = g_net(torch.randn(batch_size, 100, 1, 1).to(device))\n",
        "    pred_fake   = d_net(fake_images)\n",
        "\n",
        "    # Compute and collect loss and accuracy\n",
        "    g_loss = loss_func(pred_fake, real_labels)\n",
        "\n",
        "    # Backprop\n",
        "    g_optimizer.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    losses.append([d_loss.item(), g_loss.item()])\n",
        "    d1 = torch.mean((pred_real > 0.5).float()).detach()\n",
        "    d2 = torch.mean((pred_fake > 0.5).float()).detach()\n",
        "    discriminator_decisions.append([d1, d2])\n",
        "\n",
        "    ############################################################################\n",
        "    # Print out a status message\n",
        "    if ((epoch_i + 1) % 50 == 0):\n",
        "        msg = 'Finished epoch {:<4}/{:<4}'.format(epoch_i + 1, num_epochs)\n",
        "        sys.stdout.write('\\r' + msg)\n",
        "\n",
        "# Convert performance from list to Numpy array\n",
        "losses                  = np.array(losses)\n",
        "discriminator_decisions = np.array(discriminator_decisions)"
      ],
      "metadata": {
        "id": "Xb8njPc60u0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "ax[0].plot(smooth(losses[:, 0]))\n",
        "ax[0].plot(smooth(losses[:, 1]))\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model Loss')\n",
        "ax[0].legend(['Discrimator', 'Generator'])\n",
        "ax[0].set_xlim([4000, 5000])\n",
        "\n",
        "ax[1].plot(losses[200:, 0], losses[200:, 1], 'k.', alpha=0.1)\n",
        "ax[1].set_xlabel('Discriminator Loss')\n",
        "ax[1].set_ylabel('Generator Loss')\n",
        "\n",
        "ax[2].plot(smooth(discriminator_decisions[:, 0]))\n",
        "ax[2].plot(smooth(discriminator_decisions[:, 1]))\n",
        "ax[2].plot(discriminator_decisions)\n",
        "ax[2].set_xlabel('Epochs')\n",
        "ax[2].set_ylabel('Probability (\"Real\")')\n",
        "ax[2].set_title('Discriminator Output')\n",
        "ax[2].legend(['Real', 'Fake'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RZpGMEZt3I-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generage the images from the generator network\n",
        "g_net.eval()\n",
        "fake_data = g_net(torch.randn(batch_size, 100, 1, 1).to(device)).cpu()\n",
        "\n",
        "fig, axs = plt.subplots(3, 6, figsize=(12, 6))\n",
        "for (i, ax) in enumerate(axs.flatten()):\n",
        "    ax.imshow(fake_data[i, :].detach().squeeze(), cmap='jet')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NtX222M4422n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00sOp1Yag6wP"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X-8p0iYg6wQ"
      },
      "source": [
        "## REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6zuPWDEg6wQ"
      },
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "plt.plot(x, y,                    'bo', label='Real Data')\n",
        "plt.plot(x, predictions.detach(), 'rs', label='Predictions')\n",
        "# Correlation coefficient between the observed data and the predicted data\n",
        "plt.title(f'Prediction-data r = {np.corrcoef(y.T, predictions.detach().T)[0, 1]:.3f}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCTo0y2kppD0"
      },
      "source": [
        "## LOSS, ACCURACY - TRAIN/TEST "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TC5-ZZKp550"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(losses, 's-')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_ylim([0, 3])\n",
        "ax[0].set_title('Model Loss')\n",
        "\n",
        "ax[1].plot(train_acc, 's-', label='Train')\n",
        "ax[1].plot(test_acc,  'o-', label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_ylim([10, 100])\n",
        "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjBqO5SuhP-l"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(losses.detach())\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model Loss')\n",
        "\n",
        "ax[1].plot(train_acc, label='Train')\n",
        "ax[1].plot(test_acc,  label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title('Accuracy')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(train_loss, 's-', label='Train')\n",
        "ax[0].plot(test_loss,  'o-', label='Test')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_acc, 's-', label='Train')\n",
        "ax[1].plot(test_acc,  'o-', label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title(f'Final model train/test accuracy: {train_acc[-1]:.2f}%/{test_acc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ob9RbDEacyAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be2Y919mjj6M"
      },
      "source": [
        "## LOSS, ACCURACY - TRAIN/DEV "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x6ec0HAjj6N"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(losses.detach(), 'o-')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Losses')\n",
        "\n",
        "ax[1].plot(train_acc, 'o-', label='Train')\n",
        "ax[1].plot(dev_acc,   'o-', label='Devset')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title(f'Final model Dev accuracy: {dev_acc[-1]:.2f}')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(train_loss, 's-', label='Train')\n",
        "ax[0].plot(dev_loss,   'o-', label='Dev')\n",
        "ax[0].plot(len(dev_loss) -1, test_loss, 'r*', markersize=15, label='Test')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Losses')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_acc, 's-', label='Train')\n",
        "ax[1].plot(dev_acc,   'o-', label='Devset')\n",
        "ax[1].plot(len(dev_acc) -1, test_acc, 'r*', markersize=15, label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title(f'Final model Dev/Test accuracy: {dev_acc[-1]:.2f}%/{test_acc:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tb6uqctwf0s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOSS - TRAIN/DEV"
      ],
      "metadata": {
        "id": "JaBnvliNn1Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss, 's-', label='Train')\n",
        "plt.plot(dev_loss,   'o-', label='Dev')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p0XNl7Jtn4Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNdr8IXihvbP"
      },
      "source": [
        "## LOSS - TRAIN/TEST "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GURti5r0hvbP"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss, 's-', label='Train')\n",
        "plt.plot(test_loss,  'o-', label='Test')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.title(f'Model loss (Final Test loss: {test_loss[-1]:.2f})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh17XD36hXSl"
      },
      "source": [
        "## LOSS, ACCURACY - TRAIN/TEST "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWJgK8iVhXSl"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(train_loss, 's-', label='Train')\n",
        "ax[0].plot(test_loss,  's-', label='Test')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_acc, 's-', label='Train')\n",
        "ax[1].plot(test_acc,  'o-', label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOSS, ERROR - TRAIN/TEST"
      ],
      "metadata": {
        "id": "ctZuT-1qufF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax[0].plot(train_loss, 's-', label='Train')\n",
        "ax[0].plot(test_loss,  'o-', label='Test')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Model Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_err, 's-', label='Train')\n",
        "ax[1].plot(test_err,  'o-', label='Test')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Error rates (%)')\n",
        "ax[1].set_title(f'Final model test error rate: {test_err[-1]:.2f}%')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MCZsO_RYuwb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8dYknwKhXSl"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZffICZ3dI1"
      },
      "source": [
        "# GPU USING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRgpyZRVhXSl"
      },
      "outputs": [],
      "source": [
        "# Use GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Send model and data to the GPU\n",
        "net.to(device)\n",
        "data   = data  .to(device)\n",
        "lables = labels.to(device)\n",
        "\n",
        "# Get result from the model\n",
        "output = net(data)\n",
        "\n",
        "# Transfer output back to the CPU\n",
        "output = output.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShJVa1k24ubp"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "net.to(device)\n",
        "data   = data.to(device)\n",
        "labels = labels.to(device)\n",
        "output = net(data).detach().cpu()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DUDL_total.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}