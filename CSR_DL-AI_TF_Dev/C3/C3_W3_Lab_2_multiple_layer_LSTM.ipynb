{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiGMs-cBiijD"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/doantronghieu/DEEP-LEARNING/main/helper_DL.py\n",
        "!pip install colorama\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size':15})\n",
        "import seaborn           as sns\n",
        "sns.set()\n",
        "import helper_DL as helper"
      ],
      "metadata": {
        "id": "XFIrvgk0in8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFiCyWQ-NC5D"
      },
      "source": [
        "# Ungraded Lab: Multiple LSTMs\n",
        "\n",
        "In this lab, you will look at how to build a model with multiple LSTM layers. Since you know the preceding steps already (e.g. downloading datasets, preparing the data, etc.), we won't expound on it anymore so you can just focus on the model building code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqmDNHeByJqr"
      },
      "source": [
        "## Download and Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW-4Vo4TMUHb"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Download the subword encoded pretokenized dataset\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info = True, as_supervised = True)\n",
        "\n",
        "# Get then tokenizer\n",
        "tokenizer = info.features['text'].encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8bUh_5Ff7y"
      },
      "source": [
        "Like the previous lab, we increased the `BATCH_SIZE` here to make the training faster. If you are doing this on your local machine and have a powerful processor, feel free to use the value used in the lecture (i.e. 64) to get the same results as Laurence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffvRUI0_McDS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE  = 256\n",
        "\n",
        "# Get the train and test splits\n",
        "train_data, test_data = dataset['train'], dataset['test']\n",
        "\n",
        "# Shuffle the training data\n",
        "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Batch and pad the datasets to the maximum length of the sequences\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "test_dataset  = test_data    .padded_batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcZEiG9ayNZr"
      },
      "source": [
        "## Build and Compile the Model\n",
        "\n",
        "You can build multiple layer LSTM models by simply appending another `LSTM` layer in your `Sequential` model and enabling the `return_sequences` flag to `True`. This is because an `LSTM` layer expects a sequence input so if the previous layer is also an LSTM, then it should output a sequence as well. See the code cell below that demonstrates this flag in action. You'll notice that the output dimension is in 3 dimensions `(batch_size, timesteps, features)` when when `return_sequences` is True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18MsI2LU75kH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tfk\n",
        "from tensorflow import nn\n",
        "from tensorflow.keras import layers, losses, optimizers, models, Model\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 1  # Batch size\n",
        "TIMESTEPS  = 20 # Sequence length\n",
        "FEATURES   = 16 # Embedding size\n",
        "LSTM_DIM   = 8  # LSTM output units\n",
        "\n",
        "# Define array input with random values\n",
        "random_input = np.random.rand(BATCH_SIZE, TIMESTEPS, FEATURES)\n",
        "print(f'Shape of input array: {random_input.shape}')\n",
        "\n",
        "# Define LSTM that returns a single output\n",
        "lstm = layers.LSTM(LSTM_DIM)\n",
        "result = lstm(random_input)\n",
        "print(f'Shape of LSTM output (return_sequences = False): {result.shape}')\n",
        "\n",
        "# Define LSTM that returns a sequence\n",
        "lstm_rs = layers.LSTM(LSTM_DIM, return_sequences = True)\n",
        "result = lstm_rs(random_input)\n",
        "print(f'Shape of LSTM output (return_sequences = True):  {result.shape}')"
      ],
      "metadata": {
        "id": "0BcFWEpOjI3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Was3BX6_50C"
      },
      "source": [
        "The next cell implements the stacked LSTM architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPNwU1SVyTjm"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 64\n",
        "LSTM1_DIM     = 64\n",
        "LSTM2_DIM     = 32\n",
        "DENSE_DIM     = 64\n",
        "\n",
        "# Buid the model\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(tokenizer.vocab_size, EMBEDDING_DIM),\n",
        "    layers.Bidirectional(layers.LSTM(LSTM1_DIM, return_sequences = True)),  \n",
        "    layers.Bidirectional(layers.LSTM(LSTM2_DIM, return_sequences = False)),  \n",
        "    layers.Dense(DENSE_DIM, activation = nn.relu),\n",
        "    layers.Dense(1, activation = nn.sigmoid)                         \n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Set the training parameters\n",
        "model.compile(loss = losses.binary_crossentropy,\n",
        "              optimizer = optimizers.Adam(),\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh39GlZP79DY"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "The additional LSTM layer will lengthen the training time compared to the previous lab. Given the default parameters we set, it will take around 2 minutes per epoch with the Colab GPU enabled. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mlgzaRDMtF6"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "history = model.fit(train_dataset, epochs = NUM_EPOCHS, validation_data = test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "helper.plot_history_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txQdN63vBlTK"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "This lab showed how you can build deep networks by stacking LSTM layers. In the next labs, you will continue exploring other architectures you can use to implement your sentiment classification model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C3_W3_Lab_2_multiple_layer_LSTM.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}