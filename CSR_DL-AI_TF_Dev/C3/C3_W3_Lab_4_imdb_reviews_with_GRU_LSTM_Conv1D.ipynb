{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbj20gO3nTt4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W3/ungraded_labs/C3_W3_Lab_4_imdb_reviews_with_GRU_LSTM_Conv1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/doantronghieu/DEEP-LEARNING/main/helper_DL.py\n",
        "!pip install colorama\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size':15})\n",
        "import seaborn           as sns\n",
        "sns.set()\n",
        "import helper_DL as helper"
      ],
      "metadata": {
        "id": "eifOy4fLnYLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnpO3iadYEY2"
      },
      "source": [
        "# Ungraded Lab: Building Models for the IMDB Reviews Dataset\n",
        "\n",
        "In this lab, you will build four models and train it on the [IMDB Reviews dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews) with full word encoding. These use different layers after the embedding namely `Flatten`, `LSTM`, `GRU`, and `Conv1D`. You will compare the performance and see which architecture might be best for this particular dataset. Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6PhPXVCa_1i"
      },
      "source": [
        "## Imports\n",
        "\n",
        "You will first import common libraries that will be used throughout the exercise."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as tfk\n",
        "from tensorflow import nn\n",
        "from tensorflow.keras import layers, losses, optimizers, models, Model\n",
        "import tensorflow.keras.preprocessing as tfkp\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7eSIbsfUnlFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTmnR_9dbBY9"
      },
      "source": [
        "## Download and Prepare the Dataset\n",
        "\n",
        "Next, you will download the `plain_text` version of the `IMDB Reviews` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-AhVYeBWgQ3"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the IMDB Reviews dataset\n",
        "imdb, info = tfds.load('imdb_reviews', with_info = True, as_supervised = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHQ2Ko0zl7M4"
      },
      "outputs": [],
      "source": [
        "# Get the train and test sets\n",
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "\n",
        "# Initialize sentences and labels lists\n",
        "training_sentences = []\n",
        "training_labels    = []\n",
        "testing_sentences  = []\n",
        "testing_labels     = []\n",
        "\n",
        "# Loop over all training examples and save the sentences and labels\n",
        "for sentence, label in train_data:\n",
        "    training_sentences.append(sentence.numpy().decode('utf8'))\n",
        "    training_labels   .append(label   .numpy())\n",
        "\n",
        "# Loop over all test examples and save the sentences and labels\n",
        "for sentence, label in test_data:\n",
        "    testing_sentences.append(sentence.numpy().decode('utf8'))\n",
        "    testing_labels   .append(label   .numpy())\n",
        "\n",
        "# Convert labels lists to numpy array\n",
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final  = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygj9nleMfrAy"
      },
      "source": [
        "Unlike the subword encoded set you've been using in the previous labs, you will need to build the vocabulary from scratch and generate padded sequences. You already know how to do that with the `Tokenizer` class and `pad_sequences()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n15yyMdmoH1"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocab_size     = 10000\n",
        "max_length     = 120\n",
        "trunc_type     = 'post'\n",
        "oov_tok        = '<OOV>'\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = tfkp.text.Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
        "\n",
        "# Generate the word index dictionary for the training sentences\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate and pad the training sequences\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded    = tfkp.sequence.pad_sequences(sequences, maxlen = max_length, truncating = trunc_type)\n",
        "\n",
        "# Generate and pad the test sequences\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded    = tfkp.sequence.pad_sequences(testing_sequences, maxlen = max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUoZJv02bP0m"
      },
      "source": [
        "## Model 1: Flatten\n",
        "\n",
        "First up is simply using a `Flatten` layer after the embedding. Its main advantage is that it is very fast to train. Observe the results below.\n",
        "\n",
        "*Note: You might see a different graph in the lectures. This is because we adjusted the `BATCH_SIZE` for training so subsequent models will train faster.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SRAyulSaWAa"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "EMBEDDING_DIM = 16\n",
        "DENSE_DIM     = 6\n",
        "\n",
        "# Buid the model\n",
        "model_flatten = models.Sequential([\n",
        "    layers.Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(DENSE_DIM, activation = nn.relu),\n",
        "    layers.Dense(1, activation = nn.sigmoid)                         \n",
        "])\n",
        "\n",
        "model_flatten.summary()\n",
        "\n",
        "# Set the training parameters\n",
        "model_flatten.compile(loss = losses.binary_crossentropy,\n",
        "                      optimizer = optimizers.Adam(),\n",
        "                      metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYLZUZ3Ga1ok"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "history_flatten = model_flatten.fit(padded, training_labels_final, batch_size = BATCH_SIZE,\n",
        "                                    epochs = NUM_EPOCHS,\n",
        "                                    validation_data = (testing_padded, testing_labels_final))\n",
        "\n",
        "# Plot the accuracy and loss history\n",
        "helper.plot_history_curves(history_flatten)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w_soBeUbSXu"
      },
      "source": [
        "## LSTM\n",
        "\n",
        "Next, you will use an LSTM. This is slower to train but useful in applications where the order of the tokens is important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSualgGPPK0S"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "EMBEDDING_DIM = 16\n",
        "LSTM_DIM      = 32\n",
        "DENSE_DIM     = 6\n",
        "\n",
        "# Buid the model\n",
        "model_lstm = models.Sequential([\n",
        "    layers.Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length),\n",
        "    layers.Bidirectional(layers.LSTM(LSTM_DIM)),  \n",
        "    layers.Dense(DENSE_DIM, activation = nn.relu),\n",
        "    layers.Dense(1, activation = nn.sigmoid)                         \n",
        "])\n",
        "\n",
        "model_lstm.summary()\n",
        "\n",
        "# Set the training parameters\n",
        "model_lstm.compile(loss = losses.binary_crossentropy,\n",
        "                   optimizer = optimizers.Adam(),\n",
        "                   metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crEvEcQmUQiL"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "history_lstm = model_lstm.fit(padded, training_labels_final, batch_size = BATCH_SIZE,\n",
        "                              epochs = NUM_EPOCHS,\n",
        "                              validation_data = (testing_padded, testing_labels_final))\n",
        "\n",
        "# Plot the accuracy and loss history\n",
        "helper.plot_history_curves(history_lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcBMGJgzcXkl"
      },
      "source": [
        "## GRU\n",
        "\n",
        "The *Gated Recurrent Unit* or [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) is usually referred to as a simpler version of the LSTM. It can be used in applications where the sequence is important but you want faster results and can sacrifice some accuracy. You will notice in the model summary that it is a bit smaller than the LSTM and it also trains faster by a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NEpdhb8AxID"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "EMBEDDING_DIM = 16\n",
        "GRU_DIM       = 32\n",
        "DENSE_DIM     = 6\n",
        "\n",
        "# Buid the model\n",
        "model_gru = models.Sequential([\n",
        "    layers.Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length),\n",
        "    layers.Bidirectional(layers.GRU(GRU_DIM)),  \n",
        "    layers.Dense(DENSE_DIM, activation = nn.relu),\n",
        "    layers.Dense(1, activation = nn.sigmoid)                         \n",
        "])\n",
        "\n",
        "model_gru.summary()\n",
        "\n",
        "# Set the training parameters\n",
        "model_gru.compile(loss = losses.binary_crossentropy,\n",
        "                  optimizer = optimizers.Adam(),\n",
        "                  metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5LLrXC-uNX6"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "history_gru = model_gru.fit(padded, training_labels_final, batch_size = BATCH_SIZE,\n",
        "                            epochs = NUM_EPOCHS,\n",
        "                            validation_data = (testing_padded, testing_labels_final))\n",
        "\n",
        "# Plot the accuracy and loss history\n",
        "helper.plot_history_curves(history_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugToQrB-cfr5"
      },
      "source": [
        "## Convolution\n",
        "\n",
        "Lastly, you will use a convolution layer to extract features from your dataset. You will append a [GlobalAveragePooling1d](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer to reduce the results before passing it on to the dense layers. Like the model with `Flatten`, this also trains much faster than the ones using RNN layers like `LSTM` and `GRU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_Jc7cY3Qxke"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 16\n",
        "FILTERS        = 128\n",
        "KERNEL_SIZE   = 5\n",
        "DENSE_DIM     = 6\n",
        "\n",
        "# Buid the model\n",
        "model_conv = models.Sequential([\n",
        "    layers.Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length),\n",
        "    layers.Conv1D(filters = FILTERS, kernel_size = KERNEL_SIZE, activation = nn.relu),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "    layers.Dense(DENSE_DIM, activation = nn.relu),\n",
        "    layers.Dense(1, activation = nn.sigmoid)                         \n",
        "])\n",
        "\n",
        "model_conv.summary()\n",
        "\n",
        "# Set the training parameters\n",
        "model_conv.compile(loss = losses.binary_crossentropy,\n",
        "                   optimizer = optimizers.Adam(),\n",
        "                   metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUV70isnTiFF"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "history_conv = model_conv.fit(padded, training_labels_final, batch_size = BATCH_SIZE,\n",
        "                              epochs = NUM_EPOCHS,\n",
        "                              validation_data = (testing_padded, testing_labels_final))\n",
        "\n",
        "# Plot the accuracy and loss history\n",
        "helper.plot_history_curves(history_conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgTIZxoUkv0l"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "Now that you've seen the results for each model, can you make a recommendation on what works best for this dataset? Do you still get the same results if you tweak some hyperparameters like the vocabulary size? Try tweaking some of the values some more so you can get more insight on what model performs best."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C3_W3_Lab_4_imdb_reviews_with_GRU_LSTM_Conv1D.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}